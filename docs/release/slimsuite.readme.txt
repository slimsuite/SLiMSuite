# ReadMe documentation for SLiMSuite software

> Copyright (C) 2015 Richard J. Edwards

> Distribution compiled: Mon Dec 18 16:24:26 2017

> Please visit http://www.slimsuite.unsw.edu.au for additional documentation


# GNU License

> This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License
> as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.
> 
> This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied
> warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
> 
> You should have received a copy of the GNU General Public License along with this program; if not, write to
> the Free Software Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA.
> 
> Author contact: <seqsuite@gmail.com> / School of Biotechnology and Biomolecular Sciences, UNSW, Sydney, Australia.
> 
> To incorporate this module into your own programs, please see GNU Lesser General Public License disclaimer in rje.py.


# Module Docstrings

-tools:

### ~~~~~~~~ Module aphid ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/aphid.py] ~~~~~~~~ ###

Program:      APHID
Description:  Automated Processing of High-resolution Intensity Data
Version:      2.2
Last Edit:    10/07/14
Citation:     Raab et al. (2010), Proteomics 10: 2790-2800. [PMID: 20486118]
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This modules takes for input the partially processed results of MS analysis, with intensity data, filters based on
    scores thresholds, removes redundancy (using PINGU) and calculates relative intensity scores. PINGU is then used to
    generate outputs for use with Cytoscape and other visualisation tools.

    Input takes the form of a delimited text file with the following column headers: Expt, Subpop, Identifier,
    logInt, Score. In addition, other columns may be present (and may be used to filter data). The "unique" column headers allow
    individual identifications to be isolated, which is important for data filtering and intensity mapping.

    Intensities are converted into relative intensities with two options: (1) the redundancy level determines which
    results are combined. This may be simply at the identified peptide level, the gene level, or even at the protein
    family level (as determined through BLAST homology). If "fam" is used then GABLAM will be used to generate families
    for a given level of identity.
    
    **Note.** Core functionality has not been checked/developed since 2010.

Commandline:
    ### ~ Input Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    data=FILE       : Delimited file containing input data [None]
    seqin=FILE      : Sequence file containing hits (if pepnorm != count) [None]
    basefile=X      : Text base for output files and resdir [default based on data file name]
    unique=LIST     : Headers that, with Identifier, Treatment & Replicate, constitute unique entries [Slice]
    identifier=X    : Column containing protein indentifications [Identifier]
    treatment=X     : Column heading to identify different experimental treatment samples (e.g. condition/control) [Treatment]
    replicate=X     : Column heading to identify experimntal replicates [Replicate]
    intensity=X     : Column containing intensity values [logInt]
    logint=T/F      : Whether intensity value is a log intensity [True]
    pepcount=X      : Column containing peptide counts [rI]
    statfilter=LIST : List of filters for data [Score>-3]
    force=T/F       : Regenerate intermediate files even if found [False]

    ### ~ Intensity Combination Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    nr=X            : Level for redundancy removal (gene/pep/fam) [gene]
    famcut=X        : Percentage identity to be used by GABLAM for clustering [0.0]
    normalise=X     : Scoring strategy for normalising intensity (ppm/shared/none/mwt/frag/count) [ppm]
    flatout=T/F     : Whether to output reduced GeneMap flatfiles (*.data.tdt & *.aliases.tdt) [False]

    ### ~ Enrichment options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    absence=X       : Min normalised combined score (arbitrarily assigned to proteins totally absent from samples) [0.5]
    combine=X       : Methods for combining different replicates (max,mean,min,geo,full) [mean]
    convert=LIST    : List of X:Y where Treatment X will be renamed Y []
    enrpairs=LIST   : List of X:Y where enrichment will be restricted to X:Y ratios []
    jackknife=T/F   : Whether to perform jack-knifing tests on enrpairs [True]
    enrcut=X        : Enrichment > X for jack-knifing test [1.0]
    blanks=T/F      : Whether to include most blank enrichment/jacknife rows for missing NRID [True]


### ~~~~~~~ Module badasp ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/badasp.py] ~~~~~~~ ###

Program:      BADASP
Description:  Burst After Duplication with Ancestral Sequence Prediction
Version:      1.3.1
Last Edit:    28/03/15
Citation:     Edwards & Shields (2005), Bioinformatics 21(22):4190-1. [PMID: 16159912]
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    BADASP implements the previously published Burst After Duplication (BAD) algorithm, plus two variants that have been used
    successfully in identifying functionally interesting sites in platelet signalling proteins and can identify Type I and
    Type II divergence. In addition, several other measures of functional specificity and conservation are calculated and
    output in plain text format for easy import into other applications. See Manual for details.

Commandline:
    # General Dataset Input/Output Options #
    seqin=FILE  : Loads sequences from FILE
    query=X     : Selects query sequence by name (or part of name, e.g. Accession Number)
    basefile=X  : Basic 'root' for all files X.* [By default will use 'root' of seqin=FILE if given or haq_AccNum if qblast]
    v=X         : Sets verbosity (-1 for silent) [0]
    i=X         : Sets interactivity (-1 for full auto) [0]
    log=FILE    : Redirect log to FILE [Default = calling_program.log or basefile.log]
    newlog=T/F  : Create new log file. [Default = False: append log file]
    rank=T/F    : Whether to output ranks as well as scores [True]
    append=FILE : Append results to FILE instead of standard output to *.badasp
    trimtrunc=T/F   : Whether to trim the leading and trailing gaps (within groups) -> change to X [False]
    winsize=X       : Window size for window scores

    # BADASP Statistics #
    funcspec=X1[,X2,..] : List of functional specificity methods to apply X1,X2,..,XN
        - BAD   = Burst After Duplication (2 Subfamilies)
        - BADX  = Burst After Duplication Extra (Query Subfam versus X subfams)
        - BADN  = Burst After Duplication vs N Subfams (2+ Subfams)
        - SSC   = Livingstone and Barton Score
        - PDAD  = Variant of Livingstone and Barton
        - ETA   = Evolutionary Trace Analysis (Basic)
        - ETAQ  = Evolutionary Trace Analysis (Quantitative)
        - all   = All of the above!
    seqcon=X1[,X2,..] : List of sequence conservation measures to apply X1,X2,..,XN
        - Info  = Information content
        - PCon  = Property Conservation (Absolute)
        - MPCon = Mean Property Conservation
        - QPCon = Mean Property Conservation with Query
        - all   = All of the above

    # Tree and Grouping Options #
    nsfin=FILE  : File name for Newick Standard Format tree
    root=X      : Rooting of tree (rje_tree.py), where X is:
        - mid = midpoint root tree. [Default]
        - ran = random branch.
        - ranwt = random branch, weighted by branch lengths.
        - man = always ask for rooting options (unless i<0).
        - FILE = with seqs in FILE as outgroup. (Any option other than above)
    bootcut=X   : cut-off percentage of tree bootstraps for grouping.
    mfs=X       : minimum family size [3]
    fam=X       : minimum number of families (If 0, no subfam grouping) [0]
    orphan=T/F  : Whether orphans sequences (not in subfam) allowed. [True]
    allowvar=T/F: Allow variants of same species within a group. [False]
    gnspacc=T/F : Convert sequences into gene_SPECIES__AccNum format wherever possible. [True] 
    groupspec=X : Species for duplication grouping [None]
    group=X     : Grouping of tree
        - man = manual grouping (unless i<0).
        - dup = duplication (all species unless groupspec specified).
        - qry = duplication with species of Query sequence (or Sequence 1) of treeseq
        - one = all sequences in one group
        - None = no group (case sensitive)
        - FILE = load groups from file

    # GASP ancestral sequence prediction options #
    useanc=FILE : Gives file of predicted ancestral sequences
    pamfile=FILE: Sets PAM1 input file [jones.pam]
    pammax=X    : Initial maximum PAM matrix to generate [100]
    pamcut=X    : Absolute maximum PAM matrix [1000]
    fixpam=X    : PAM distance fixed to X [100].
    rarecut=X   : Rare aa cut-off [0.05].
    fixup=T/F   : Fix AAs on way up (keep probabilities) [True].
    fixdown=T/F : Fix AAs on initial pass down tree [False].
    ordered=T/F : Order ancestral sequence output by node number [False].
    pamtree=T/F : Calculate and output ancestral tree with PAM distances [True].
    desconly=T/F: Limits ancestral AAs to those found in descendants [True].
    xpass=X     : How many extra passes to make down & up tree after initial GASP [1].

    # System Info Options #
    win32=T/F       : Run in Win32 Mode [False]

Please see help for rje_tree.py and rje_seq.py for additional options not covered here.    

Uses general modules: copy, string, sys, time
Uses RJE modules: rje, rje_aaprop, rje_ancseq, rje_conseq, rje_seq, rje_specificity, rje_tree
Additional modules required: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_tree_group, rje_uniprot

### ~~~~~ Module budapest ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/budapest.py] ~~~~~ ###

Program:      BUDAPEST
Description:  Bioinformatics Utility for Data Analysis of Proteomics on ESTs
Version:      2.3
Last Edit:    31/07/13
Citation:     Jones, Edwards et al. (2011), Marine Biotechnology 13(3): 496-504. [PMID: 20924652]
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    Proteomic analysis of EST data presents a bioinformatics challenge that is absent from standard protein-sequence
    based identification. EST sequences are translated in all six Reading Frames (RF), most of which will not be
    biologically relevant. In addition to increasing the search space for the MS search engines, there is also the added
    challenge of removing redundancy from results (due to the inherent redundancy of the EST database), removing spurious
    identifications (due to the translation of incorrect reading frames), and identifying the true protein hits through
    homology to known proteins.
    
    BUDAPEST (Bioinformatics Utility for Data Analysis of Proteomics on ESTs) aims to overcome some of these problems by
    post-processing results to remove redundancy and assign putative homology-based identifications to translated RFs
    that have been "hit" during a MASCOT search of MS data against an EST database. Peptides assigned to "incorrect" RFs
    are eliminated and EST translations combined in consensus sequences using FIESTA (Fasta Input EST Analysis). These
    consensus hits are optionally filtered on the number of MASCOT peptides they contain before being re-annotated using
    BLAST searches against a reference database. Finally, HAQESAC can be used for automated or semi-automated phylogenetic
    analysis for improved sequence annotation.

Input:
    BUDAPEST takes three main files as input:
    * A MASCOT results file, specified by mascot=FILENAME.
    * The EST sequences used (or, at least, hit by) the MASCOT search, in fasta format, specified by seqin=FILENAME.
    * A protein database for BLAST-based annotation in fasta format, specified by searchdb=FILENAME.

Output:
    BUDAPEST produces the following main output files, where X is set by basefile=X:
    * X.budapest.tdt = main output table of results
    * X.budapest.fas = BLAST-annotated clustered consensus EST translations using FIESTA
    * X.summary.txt = summary of results from BUDAPEST pipeline
    * X.details.txt = full details of processing for each original MASCOT hit.

    Additional information can also be obtained from the additional sequence files:        
    * X.est.fas = subset of EST sequences from EST database that have 1+ hits in MASCOT results. 
    * X.translations.fas = fasta format of translated RF Hits that are retained after BUDAPEST cleanup.
    * X.fiesta.fas = BLAST-annotated consensus EST translations using FIESTA (pre min. peptide filtering)
    * X_HAQESAC/X.* = HAQESAC results files for annotating translated ESTs (haqesac=T only)
    * X_seqfiles/X.cluster*.fas = fasta files of translations and BLAST hits in NR clusters (clusterfas=T only)

    Lastly, reformatted MASCOT files are produced, named after the original input file (Y):
    * Y.mascot.txt = header information from the MASCOT file.
    * Y.mascot.csv = the delimited data portion of the MASCOT file.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    mascot=FILE     : Name of MASCOT csv file [None]
    seqin=FILE      : Name of EST fasta file used for search [None]
    searchdb=FILE   : Fasta file for GABLAM search of EST translations [None]
    partial=T/F     : Whether partial EST data is acceptable (True) or all MASCOT hits must be found (False) [True]
    itraq=T/F       : Whether data is from an iTRAQ experiment [False]
    empai=T/F       : Whether emPAI data is present in MASCOT file [True]
    samples=LIST    : List of X:Y, where X is an iTRAQ isotag and Y is a sample []
    ### ~ PROCESSING OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    minpolyat=X     : Min length of poly-A/T to be counted. (-1 = ignore all) [10]
    fwdonly=T/F     : Whether to treat EST/cDNA sequences as coding strands (False = search all 6RF) [False]
    minorf=X        : Min length of ORFs to be considered [10]
    topblast=X      : Report the top X BLAST results [10]
    minaln=X        : Min length of shared region for FIESTA consensus assembly [20]
    minid=X         : Min identity of shared region for FIESTA consensus assembly [95.0]
    minpep=X        : Minimum number of different peptides mapped to final translation/consensus [2]
    ### ~ SEQUENCE FORMATTING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    newacc=X        : New base for sequence accession numbers ['BUD']
    gnspacc=T/F     : Convert sequences into gene_SPECIES__AccNum format wherever possible. [True]
    spcode=X        : Species code for EST sequences [None]
    ### ~ OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : "Base" name for all results files, e.g. X.budapest.tdt [MASCOT file basename]
    hitdata=LIST    : List of hit data to add to main budapest table [prot_mass,prot_pi]
    seqcluster=T/F  : Perform additional sequence (BLAST/GABLAM) clustering [True]
    clusterfas=T/F  : Generate fasta files of translations and BLAST hits in NR clusters [False]
    clustertree=LIST: List of formats for cluster tree output (3+ seqs only) [text,nsf,png]
    fiestacons=T/F  : Use FIESTA to auto-construct consensi from BUDAPEST RF translations [True]
    haqesac=T/F     : HAQESAC analysis of identified EST translations [True]
    blastcut=X      : Reduced the number of sequences in HAQESAC runs to X (0 = no reduction) [50]
    multihaq=T/F    : Whether to run HAQESAC in two-phases with second, manual phase [False]
    cleanhaq=T/F    : Delete excessive HAQESAC results files [True]
    haqdb=FILE      : Optional additional search database for MultiHAQ analysis [None]


### ~~~ Module comparimotif_V3 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/comparimotif_V3.py] ~~~ ###

Program:      CompariMotif
Description:  Motif vs Motif Comparison Software
Version:      3.13.0
Last Edit:    03/12/15
Citation:     Edwards, Davey & Shields (2008), Bioinformatics 24(10):1307-9. [PMID: 18375965]
Webserver:    http://bioware.ucd.ie/
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    CompariMotif is a piece of software with a single objective: to take two lists of regular expression protein motifs
    (typically SLiMs) and compare them to each other, identifying which motifs have some degree of overlap, and
    identifying the relationships between those motifs. It can be used to compare a list of motifs with themselves, their
    reversed selves, or a list of previously published motifs, for example (e.g. ELM (http://elm.eu.org/)). CompariMotif
    outputs a table of all pairs of matching motifs, along with their degree of similarity (information content) and
    their relationship to each other.

    The best match is used to define the relationship between the two motifs. These relationships are comprised of the
    following keywords:

    Match type keywords identify the type of relationship seen:
    * Exact = all the matches in the two motifs are precise
    * Variant = focal motif contains only exact matches and subvariants of degenerate positions in the other motif
    * Degenerate = the focal motif contains only exact matches and degenerate versions of positions in the other motif
    * Complex = some positions in the focal motif are degenerate versions of positions in the compared motif, while  others are subvariants of degenerate positions.
    * Ugly = the two motifs match at partially (but not wholly) overlapping ambiguous positions (e.g. [AGS] vs [ST]). Such matches can be excluded using overlaps=F. (Version 3.8 onwards only.)

    Match length keywords identify the length relationships of the two motifs:
    * Match = both motifs are the same length and match across their entire length
    * Parent = the focal motif is longer and entirely contains the compared motif
    * Subsequence = the focal motif is shorter and entirely contained within the compared motif
    * Overlap = neither motif is entirely contained within the other

    This gives twenty possible classifications for each motif's relationship to the compared motif.

Input:
    CompariMotif can take input in a number of formats. The preferred format is SLiMSearch format, which is a single line
    motif format: 'Name Sequence #Comments' (Comments are optional and ignored). Alternative inputs include SLiMFinder and 
    SLiMDisc output, ELM downloads, raw lists of motifs, and fasta format. Any delimited file with 'Name' and 'Pattern'
    fields should be recognised. 

    Complex motifs containing either/or (REGEX1|REGEX2) portions will be split into multiple motifs (marked a, b etc.).
    Similarly, variable numbers of non-wildcard positions will be split, e.g. RK{0,1}R would become RR and RKR. "3of5"
    motif patterns, formatted <R:m:n> where at least m of a stretch of n residues must match R, are also split prior to a
    search being perfomed. Currently, wildcard spacers are limited to a maximum length of 9. 

Output:
    The main output for CompariMotif is delimited text file containing the following fields:
    * File1     = Name of motifs file (if outstyle=multi)
    * File2     = Name of searchdb file (if outstyle=multi)
    * Name1     = Name of motif from motif file 1
    * Name2     = Name of motif from motif file 2 
    * Motif1    = Motif (pattern) from motif file 1
    * Motif2    = Motif (pattern) from motif file 2
    * Sim1      = Description of motif1's relationship to motif2
    * Sim2      = Description of motif2's relationship to motif1
    * Match     = Text summary of matched region
    * MatchPos  = Number of matched positions between motif1 and motif2 (>= mishare=X)
    * MatchIC   = Information content of matched positions
    * NormIC    = MatchIC as a proportion of the maximum possible MatchIC (e.g. the lowest IC motif)
    * CoreIC    = MatchIC as a proportion of the maximum possible IC in the matched region only.
    * Score     = Heuristic score (MatchPos x NormIC) for ranking motif matches
    * Info1     = Ambiguity score of motif1 
    * Info2     = Ambiguity score of motif2 
    * Desc1     = Description of motif1 (if motdesc = 1 or 3)
    * Desc2     = Description of motif2 (if motdesc = 2 or 3)

    With the exception of the file names, which are only output if outstyle=multi, the above is the output for the
    default "normal" output style. If outstyle=single then only statistics for motif2 (the searchdb motif) are output
    as this is designed for searches using a single motif against a motif database. If outstyle=normalsplit or
    outstyle=multisplit then motif1 information is grouped together, followed by motif2 information, followed by the
    match statistics. More information can be found in the CompariMotif manual.

Webserver:
    CompariMotif can be run online at http://bioware.ucd.ie.

Commandline:
    ## Basic Input Parameters ##
    motifs=FILE     : File of input motifs/peptides [None]
    searchdb=FILE   : (Optional) second motif file to compare. Will compare to self if none given. [None]
    dna=T/F         : Whether motifs should be considered as DNA motifs [False]

    ## Basic Output Parameters ##
    resfile=FILE    : Name of results file, FILE.compare.tdt. [motifsFILE-searchdbFILE.compare.tdt]
    motinfo=FILE    : Filename for output of motif summary table (if desired) [None]
    motific=T/F     : Output Information Content for motifs [False]
    coreic=T/F      : Whether to output normalised Core IC [True]
    unmatched=T/F   : Whether to output lists of unmatched motifs (not from searchdb) into *.unmatched.txt [False]

    ## Motif Comparison Parameters ##
    minshare=X      : Min. number of non-wildcard positions for motifs to share [2]
    normcut=X       : Min. normalised MatchIC for motif match [0.5]
    matchfix=X      : If >0 must exactly match *all* fixed positions in the motifs from:  [0]
                        - 1: input (motifs=FILE) motifs
                        - 2: searchdb motifs
                        - 3: *both* input and searchdb motifs
    ambcut=X        : Max number of choices in ambiguous position before replaced with wildcard (0=use all) [10]
    overlaps=T/F    : Whether to include overlapping ambiguities (e.g. [KR] vs [HK]) as match [True]
    memsaver=T/F    : Run in more efficient memory saver mode. XGMML output not available. [False]

    ## Advanced Motif Input Parameters ##
    minic=X         : Min information content for a motif (1 fixed position = 1.0) [2.0]
    minfix=X        : Min number of fixed positions for a motif to contain [0]
    minpep=X        : Min number of defined positions in a motif [2]
    trimx=T/F       : Trims Xs from the ends of a motif [False]
    nrmotif=T/F     : Whether to remove redundancy in input motifs [False]
    reverse=T/F     : Reverse the input motifs. [False]
                        - If no searchdb given, these will be searched against the "forward" motifs.
    mismatches=X    : <= X mismatches of positions can be tolerated [0]
    aafreq=FILE     : Use FILE to replace uniform AAFreqs (FILE can be sequences or aafreq) [None]    

    ## Advanced Motif Output Parameters ##
    xgmml=T/F       : Whether to output XGMML format results [True]
    xgformat=T/F    : Whether to use default CompariMotif formatting or leave blank for e.g. Cytoscape [True]
    pickle=T/F      : Whether to load/save pickle following motif loading/filtering [False]
    motdesc=X       : Sets which motifs have description outputs (0-3 as matchfix option) [3]
    outstyle=X      : Sets the output style for the resfile [normal]
                        - normal = all standard stats are output
                        - multi = designed for multiple appended runs. File names are also output
                        - single = designed for searches of a single motif vs a database. Only motif2 stats are output
                        - reduced = motifs do not have names or descriptions
                        - normalsplit/multisplit = as normal/multi but stats are grouped by motif rather than by type

Uses general modules: os, string, sys, time
Uses RJE modules: rje, rje_menu, rje_slim, rje_slimlist, rje_xgmml, rje_zen
Other modules needed: rje_aaprop, rje_dismatrix, rje_seq, rje_blast, rje_pam, rje_sequence, rje_uniprot

### ~~~~~~~ Module fiesta ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/fiesta.py] ~~~~~~~ ###

Program:      FIESTA
Description:  Fasta Input EST Analysis
Version:      1.9.0
Last Edit:    26/11/14
Citation:     Jones, Edwards et al. (2011), Marine Biotechnology 13(3): 496-504. [PMID: 20924652]
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    FIESTA has three primary functions: <1>	Discovery, assembly and evolutionary analysis of candidate genes in an EST
    library; <2> Assembly of an EST library for proteomics analysis; <3> Translation/Annotation of an EST library for
    proteomics analysis. These functions are outlined below.

Candidate Gene Discovery:
    After optional pre-assembly of the EST library using the DNA FIESTA pipeline (see below), the candidate protein
    dataset (the QueryDB) is used for translation and annotation (see below) of those ESTs with BLAST homology to 1+
    candidates. These translations are then assembled into consensus sequences where appropriate and alignments and
    trees made for the hits to each candidate protein. Finally, if desired, HAQESAC is run for each candidate protein.

EST Assembly:
    All EST assembly experiences two trade-offs: one between speed and accuracy, and a second between redundancy and
    accuracy. In particular, distinguishing sequencing errors from sequence variants (alleles) from different gene family
    members is not trivial.
    
    FIESTA is designed to provide straightforward assembly and BLAST-based annotation of EST sequences in Fasta format.
    The rationale behind its design was to try and optimise quality & redundancy versus comprehensive coverage for
    Proteomics identifications from Mass Spec data. FIESTA is designed to function in a relatively standalone capacity,
    with BLAST being the only other tool necessary. Due to this simplicity, FIESTA has some limitations; the main one
    being its inability to identify and deal with frameshift (indel) sequencing errors.
    
    FIESTA has two assembly and annotation pipelines: a protein pipeline based loosely on BUDAPEST and a DNA pipeline for
    "true" EST assembly. Details can be found in the Manual.

EST library assembly/annotation:
    In addition to the main functions, parts of the main FIESTA assembly/annotation pipeline can be run as standalone
    functions.

    ESTs can be converted to Reading Frames (RF) with est2rf=T:
    1. Identify orientation using 5' poly-T or 3' poly-A.
    - 1a. Where poly-AT tail exists, remove, translate in 3 forward RF and truncate at terminal stop codon.
    - 1b. Where no poly-AT tail exists, translate in all six RF.
    2. BLAST translations vs. search database with complexity filter on.
    - 2a. If EST has BLAST hits, retain RFs with desired e-value or better. 
    - 2b. If no BLAST hits, retain all RFs.

    Alternatively, translated RFs or other unannotated protein sequences can be given crude BLAST-based annotations using
    searchdb=FILE sequences with blastann=T. Note that these are simply the top BLAST hit and better annotation would be
    achieved using HAQESAC (or MultiHAQ for many sequences).
    
Commandline:
    ### ~ GENERAL INPUT ~ ###
    seqin=FILE      : EST file to be processed [None]
    fwdonly=T/F     : Whether to treat EST/cDNA sequences as coding strands (False = search all 6RF) [False]
    minpolyat=X     : Min length of poly-AT to be considered a poly AT [10]
    minorf=X        : Min length of ORFs to be considered [20]   
    blastopt=FILE   : File containing additional BLAST options for run, e.g. -B F [None]
    ntrim=X         : Trims of regions >= X proportion N bases [0.5]

    ### ~ SEQUENCE FORMATTING ~ ###
    gnspacc=T/F     : Convert sequences into gene_SPECIES__AccNum format wherever possible. [False]
    spcode=X        : Species code for EST sequences [None]
    species=X       : Species for EST sequences [None]
    newacc=X        : New base for sequence accession numbers ['' or spcode]

    ### ~ EST ASSEMBLY ~ ###
    minaln=X        : Min length of shared region for consensus assembly [40]
    minid=X         : Min identity of shared region for consensus assembly [95.0]
    bestorf=T/F     : Whether to use the "Best" ORF only for ESTs without BLAST Hits [True]
    pickup=T/F      : Whether to read in partial results and skip those sequences [True]
    annotate=T/F    : Annotate consensus sequences using BLAST-based approach [False]
    dna=T/F         : Implement DNA-based GABLAM assembly [True]
    resave=X        : Number of ESTs to remove before each resave of GABLAM searchdb [200]
    gapblast=T/F    : Whether to allow gaps during BLAST identification of GABLAM homologues [False]
    assmode=X       : Mode to use for EST assembly (nogab,gablam,oneqry) [oneqry]
    gabrev=T/F      : Whether to use GABLAM-based reverse complementation [True]

    ### ~ ANNOTATION ~ ###
    est2rf=T/F      : Execute BLAST-based EST to RF translation/annotation only, on seqin [False]
    est2haq=T/F     : Execute BLAST-based EST to RF translation/annotation on seqin followed by HAQESAC analysis [False]
    blastann=T/F    : Execute BLAST-based annotation of conensus translations only, on seqin [False]
    truncnt=T/F     : Whether to truncate N-terminal to Met in final BLAST annotation (if hit) [False]
    searchdb=FILE   : Fasta file for GABLAM search of EST translations [None]

    ### ~ QUERY SEARCH ~ ###
    batch=LIST      : List of EST libraries to search (will use seqin if none given) []
    querydb=FILE    : File of query sequences to search for in EST library [None]
    qtype=X         : Sequence "Type" to be used with NewAcc for annotation of translations [hit]
    assembly=T/F    : Assemble EST sequences prior to search [False]
    consensi=T/F    : Assemble hit ORF into consensus sequences [False]

    ### ~ HAQESAC OPTIONS ~ ###
    haqesac=T/F     : HAQESAC analysis of identified EST translations [True]
    multihaq=T/F    : Whether to run HAQESAC in two-phases [True]
    blastcut=X      : Reduced the number of sequences in HAQESAC runs to X (0 = no reduction) [50]
    cleanhaq=T/F    : Delete excessive HAQESAC results files [True]
    haqdb=FILELIST  : Optional extra databases to search for HAQESAC analysis []
    haqbatch=T/F    : Whether to only generate HAQESAC batch file (True) or perform whole run (False) [False]

### ~~~~~~~ Module gablam ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/gablam.py] ~~~~~~~ ###

Program:      GABLAM
Description:  Global Analysis of BLAST Local AlignMents
Version:      2.28.3
Last Edit:    31/08/17
Citation:     Davey, Shields & Edwards (2006), Nucleic Acids Res. 34(12):3546-54. [PMID: 16855291]
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for taking one or two sequence datasets, peforming an intensive All by All BLAST and then tabulating
    the results as a series of pairwise comparisons (`*.gablam.*`):

    * Qry       = Query Short Name (or AccNum)
    * Hit       = Hit Short Name
    * Rank      = Rank of that Hit vs Query (based on Score)
    * Score     = BLAST Score (one-line)
    * E-Value   = BLAST E-value
    * QryLen    = Length of Query Sequence
    * HitLen    = Length of Hit Sequence
    * Qry_AlnLen        = Total length of local BLAST alignment fragments in Query (Unordered)
    * Qry_AlnID         = Number of Identical residues of Query aligned against Hit in local BLAST alignments (Unordered)
    * Qry_AlnSim        = Number of Similar residues of Query aligned against Hit in local BLAST alignments (Unordered)
    * Qry_OrderedAlnLen = Total length of local BLAST alignment fragments in Query (Ordered)
    * Qry_OrderedAlnID  = Number of Identical residues of Query aligned against Hit in local BLAST alignments (Ordered)
    * Qry_OrderedAlnSim = Number of Similar residues of Query aligned against Hit in local BLAST alignments (Ordered)
    * Hit_AlnLen        = Total length of local BLAST alignment fragments in Hit (Unordered)
    * Hit_AlnID         = Number of Identical residues of Hit aligned against Query in local BLAST alignments (Unordered)
    * Hit_AlnSim        = Number of Similar residues of Hit aligned against Query in local BLAST alignments (Unordered)
    * Hit_OrderedAlnLen = Total length of local BLAST alignment fragments in Hit (Ordered)
    * Hit_OrderedAlnID  = Number of Identical residues of Hit aligned against Query in local BLAST alignments (Ordered)
    * Hit_OrderedAlnSim = Number of Similar residues of Hit aligned against Query in local BLAST alignments (Unordered)
    * ALIGN_ID          = Number of Identical residues as determined by pairwise ALIGN
    * ALIGN_Len         = Length of pairwise ALIGN

    When searchdb=FILE is DNA (blastprog=blastn/tblastn/tblastx), there will be additional `Dirn` fields that indicate
    whether the hits are on the forward strand (`Fwd`), reverse strand (`Bwd`) or a combination (`Both`).

    By default, all BLAST hits will return alignments. (blastv=N blastb=N, where `N` is the size of `searchdb`.) This can
    be over-ridden by the blastv=X and blastb=X options to limit results to the top `X` hits.

    GABLAM will also produce a single table of summary statistics for all non-self hits (`*.hitsum.*`) (self hits included
    if selfhit=T selfsum=T):

    * Qry       = Query Short Name (or AccNum)
    * Hits      = Number of Hits
    * MaxScore  = Max non-self BLAST Score (one-line)
    * E-Value   = BLAST E-value for max score

    If `keepblast=T` then the blast results files themselves will not be deleted and will also be available as output -
    or for re-running GABLAM faster on the same input. This can be performed even when the output basefile has changed
    by giving the BLAST results file with `blastres=FILE`.

Versions/Updates:
    ### ~ V2.8: All-by-all search output ~ ###
    Version 2.8 onwards features explicit extra functionality for all-by-all searches, where the QueryDB (seqin=FILE) and
    SearchDB (searchdb=FILE) are the same. (Failing to give a searchdb will run in this mode.)

    ### ~ V2.16: FullBLAST mode ~ ###
    Version 2.16.x introduces a new "fullblast" mode, which performs a full BLAST search (using forks=X to set the number
    of processors for the BLAST search) followed by the blastres=FILE multiGABLAM processing. This should be faster for
    large datasets but precludes any appending of results files. This is incompatible with the missing=LIST advanced
    update option. (missing=LIST should only be required for aborted fullblast=F runs.) By default, the blast file will
    be named the same as the other output but the blastres=FILE command can be used to read in results from a file with
    a different name. If this file was not created with sequences in QueryDB and SearchDB, it will create errors.

    ### ~ V2.20: SNP Table & LocalUnique output ~ ###
    Version 2.20.x added a `snptable=T/F` output to generate a SNP table (similar to MUMmer NUCmer output) with the
    following fields: `Locus`, `Pos`, `REF`, `ALT`, `AltLocus`, `AltPos`. The `localunique=T/F` controls whether hit
    regions can be covered multiple times (`False`) or (default:`True`) reduced to unique "best" hits. Local hits are
    sorted according to `localsort=X` (default:`Identity`). NOTE: Output is restricted to regions of overlap between
    query and hit sequences. Regions of each that are not covered will be output in `*.nocoverage.tdt`. See `*.local.tdt`
    or `*.unique.tdt` output for the regions covered. Normally, the reference genome will be `seqin=FILE` and the genome
    to compare against the reference will be `searchdb=FILE`. NOTE: `localmin` and `localidmin` are applied to unique
    regions as well as the original local hits table.

    ### ~ V2.26: Fragment fasta output ~ ###
    Version 2.26.x tidied up the `fragfas=T` and `combinedfas=T` output, which are now only available in combination when
    `fullblast=T`. The `gablamfrag=X` parameter controls fragmentation within a given Query-Hit pair, i.e. local hits
    within gablamfrag=X positions of each other in the Hit will be merged. `fragmerge=X` applies to different hit regions
    when they are combined during `combinedfas=T` output, and will merge regions with `fragmerge=X` positions
    irrespective of whether they are hits from the same query or different ones. Fragments are strand-specific and hits
    to the reverse strand will be reverse-complemented by default. If `fragrevcomp=F` then all local hits will be mapped
    onto the forward strand, regardless of their original direction.

    Default values are now `gablamfrag=1 fragmerge=0`. This means that hits from the same query will merge if directly
    adjacent and on the same strand, whereas hits from different queries will only merge if overlapping and on the same
    strand. To restrict merging to overlaps of a specific minimum length, set the relevant parameter to a negative
    number, e.g. `fragmerge=-10` will require a 10 bp/aa overlap before merging. `gablamfrag` can only be set to values
    below 1 when used in combination with `fullblast=T`.

    The defaults for when GABLAM is run from the commandline have been set to fullblast=T keepblast=T. When GABLAM is
    called from within another program, the fullblast=T/F and keepblast=T/F default status is set by that program.

Commandline:
    ### ~ Input/Search Options ~ ###
    seqin=FILE      : Query dataset file [infile.fas]
    searchdb=FILE   : Database to search. [By default, same as seqin]
    blastres=FILE   : BLAST results file for input (over-rides seqin and searchdb) [None]
    fullblast=T/F   : Whether to perform full BLAST followed by blastres analysis [True]
    blastp=X        : Type of BLAST search to perform (blastx for DNA vs prot; tblastn for Prot vs DNA) [blastp]
    gablamcut=X     : Min. percentage value for a GABLAM stat to report hit (GABLAM from FASTA only) [0.0]
    cutstat=X       : Stat for gablamcut (eg. AlnLen or OrderedAlnSim. See Function docs for full list) [OrderedAlnID]
    cutfocus=X      : Focus for gablamcut. Can be Query/Hit/Either/Both. [Either]
    localcut=X      : Cut-off length for local alignments contributing to global GABLAM stats [0]
    localidcut=PERC : Cut-off local %identity for local alignments contributing to global GABLAM stats [0.0]

    ### ~ General Output Options ~ ###
    append=T/F      : Whether to append to output file or not. (Not available for blastres=FILE or fullblast=F) [False]
    fullres=T/F     : Whether to output full GABLAM results table [True]
    hitsum=T/F      : Whether to output the BLAST Hit Summary table [True]
    local=T/F       : Whether to output local alignment summary stats table [True]
    localsAM=T/F    : Save local (and unique) hits data as SAM files in addition to TDT [False]
    reftype=X       : Whether to map SAM/GFF3 hits onto the Qry, Hit, Both or Combined [Hit]
    qassemble=T/F   : Whether to fully assemble query stats from all hits in HitSum [False]
    localmin=X      : Minimum length of local alignment to output to local stats table [0]
    localidmin=PERC : Minimum local %identity of local alignment to output to local stats table [0.0]
    localunique=T/F : Reduce local hits to unique non-overlapping regions (*.unique.tdt) [snptable=T/F]
    localsort=X     : Local hit field used to sort local alignments for localunique reduction [Identity]
    snptable=T/F    : Generate a SNP table (similar to MUMmer NUCmer output) for query/hit overlap (fullblast=T) [False]
    selfhit=T/F     : Whether to include self hits in the fullres output [True] * See also selfsum=T/F *
    selfsum=T/F     : Whether to also include self hits in hitsum output [False] * selfhit must also be T *
    qryacc=T/F      : Whether to use the Accession Number rather than the short name for the Query [True]
    keepblast=T/F   : Whether to keep the blast results files rather than delete them [True]
    blastdir=PATH   : Path for blast results file (best used with keepblast=T) [./]
    percres=T/F     : Whether output is a percentage figures (2d.p.) or absolute numbers [True]
                      - Note that enough data is output to convert one into the other in other packages (for short sequences)
    reduced=LIST    : List of terms that must be included in reduced output headers (e.g. Hit or Qry_Ordered) []

    ### ~ All-by-all Output Options ~ ###
    maxall=X        : Maximum number of sequences for all-by-all outputs [100]
    dismat=T/F      : Whether to output compiled distance matrix [True]
    diskey=X        : GABLAM Output Key to be used for distance matrix ['Qry_AlnID']
    distrees=T/F    : Whether to generate UPGMA tree summaries of all-by-all distances [True]
    treeformats=LIST: List of output formats for generated trees (see rje_tree.py) [nwk,text,png]
    disgraph=T/F    : Whether to output a graph representation of the distance matrix (edges = homology) [False]
    graphtypes=LIST : Formats for graph outputs (svg/xgmml/png/html) [xgmml,png]
    clusters=T/F    : Whether to output a list of clusters based on shared BLAST homology [True]
    bycluster=X     : Generate separate trees and distance matrix for clusters of X+ sequences [0]
    clustersplit=X  : Threshold at which clusters will be split (e.g. must be < distance to cluster) [1.0]
    singletons=T/F  : Whether to include singleton in main tree and distance matrix [False]
    saveupc=T/F     : Whether to output a UPC file for SLiMSuite compatibility [False]

    ### ~ Sequence output options ~ ###
    localalnfas=T/F : Whether to output local alignments to *.local.fas fasta file (if local=T) [False]
    fasout=T/F      : Output a fasta file per input sequence "ACCNUM.DBASE.fas" [False]   (GABLAM from FASTA only)
    fasdir=PATH     : Directory in which to save fasta files [BLASTFAS/]
    fragfas=T/F     : Whether to output fragmented Hits based on local alignments [False]
    fragrevcomp=T/F : Whether to reverse-complement DNA fragments that are on reverse strand to query [True]
    gablamfrag=X    : Length of gaps between mapped residues for fragmenting local hits [0]
    fragmerge=X     : Max Length of gaps between fragmented local hits to merge [0]
    addflanks=X     : Add flanking regions of length X to fragmented hits [0]
    combinedfas=T/F : Whether to generate a combined fasta file [False]

    ### ~ Advanced/Obselete Search/Output Options ~ ###
    dotplots=T/F    : Whether to use gablam.r to output dotplots. (Needs R installed and setup) [False]
    dotlocalmin=X   : Minimum length of local alignment to output to local hit dot plots [1000]
    mysql=T/F       : Whether to output column headers for mysql table build [False]
    missing=LIST    : This will go through and add missing results for AccNums in FILE (or list of AccNums X,Y,..) [None]
    startfrom=X     : Accession number to start from [None]
    alnstats=T/F    : Whether to output GABLAM stats or limit to one-line stats (blastb=0) [True]
    posinfo=T/F     : Output the Start/End limits of the BLAST Hits [True]
    outstats=X      : Whether to output just GABLAM, GABLAMO or All [All]

    ### ~ GABLAM Non-redundancy options. NOTE: These are different to rje_seq NR options. ~ ###
    nrseq=T/F       : Make sequences Non-Redundant following all-by-all. [False]
    nrcut=X         : Cut-off for non-redundancy filter, uses nrstat=X for either query or hit [100.0]
    nrstat=X        : Stat for nrcut (eg. AlnLen or OrderedAlnSim. See above for full list) [OrderedAlnID]
    nrchoice=LIST   : Order of decisions for choosing NR sequence to keep. Otherwise keeps first sequence. (swiss/nonx/length/spec/name/acc/manual) [swiss,nonx,length]
    nrsamespec=T/F  : Non-Redundancy within same species only. [False]
    nrspec=LIST     : List of species codes in order of preference (good to bad) []

    ### ~ BLAST Options ~ ###
    blastpath+=PATH : path for blast+ files [c:/bioware/blast+/] *Use fwd slashes
    blastpath=PATH  : path for blast files [c:/bioware/blast/] *Use fwd slashes
    blaste=X        : E-Value cut-off for BLAST searches (BLAST -e X) [1e-4]
    blastv=X        : Number of one-line hits per query (BLAST -v X) [500]
    blastb=X        : Number of hit alignments per query (BLAST -b X) [500]
    blastf=T/F      : Complexity Filter (BLAST -F X) [False]
    checktype=T/F   : Whether to check sequence types and BLAST program selection [True]

    ### ~ Additional ALIGN Global Identity ~ ###
    globid=T/F  : Whether to output Global %ID using ALIGN [False]
    rankaln=X   : Perform ALIGN pairwise global alignment for top X hits [0]
    evalaln=X   : Perform ALIGN pairwise global alignment for all hits with e <= X [1000]
    alncut=X    : Perform ALIGN pairwise global alignment until < X %ID reached [0]

    ### ~ Forking Options ~ ###
    noforks=T/F     : Whether to avoid forks [False]
    forks=X         : Number of parallel sequences to process at once [0]
    killforks=X     : Number of seconds of no activity before killing all remaining forks. [36000]

### ~~~~~~~~~ Module gasp ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/gasp.py] ~~~~~~~~~ ###

Program:      GASP
Description:  Gapped Ancestral Sequence Prediction
Version:      1.4
Last Edit:    16/07/13
Citation:     Edwards & Shields (2004), BMC Bioinformatics 5(1):123. [PMID: 15350199]
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is purely for running the GASP algorithm as a standalone program. All the main functionality is encoded in
    the modules listed below. The GASP algorithm itself is now encoded in the rje_ancseq module. GASP may also be run
    interactively from the rje_tree module.

Commandline: 
    seqin=FILE  : Fasta/ClustalW/Phylip Format sequence file [infile.fas].
    nsfin=FILE  : Newick Standard Format treefile [infile.nsf].

    help    : Triggers Help = list of commandline arguments.

    indeltree=FILE  : Prints a text tree describing indel patterns to FILE.

Uses general modules: os, re, sys, time
Uses RJE modules: rje, rje_ancseq, rje_pam, rje_seq, rje_tree
Additional modules needed: rje_blast, rje_dismatrix, rje_sequence, rje_tree_group, rje_uniprot

### ~~~~~~~ Module gfessa ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/gfessa.py] ~~~~~~~ ###

Program:      GFESSA
Description:  Genome-Free EST SuperSAGE Analysis
Version:      1.4
Last Edit:    20/08/13
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    This program is for the automated processing, mapping and identification-by-homology for SuperSAGE tag data for
    organisms without genome sequences, relying predominantly on EST libraries etc. Although designed for genome-free
    analysis, there is no reason why transcriptome data from genome projects cannot be used in the pipeline. 

    GFESSA aims to take care of the following main issues:
    1. Removal of unreliable tag identification/quantification based on limited count numbers.
    2. Converting raw count values into enrichment in one condition versus another.
    3. Calculating mean quantification for genes based on all the tags mapping to the same sequence.
    4. The redundancy of EST libraries, by mapping tags to multiple sequences where necessary and clustering sequences
    on shared tags.

    The final output is a list of the sequences identified by the SAGE experiment along with enrichment data and
    clustering based on shared tags.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    tagfile=FILE    : File containing SuperSAGE tags and counts [None]
    tagfield=X      : Field in tagfile containing tag sequence. (All others should be counts) ['Tag sequence']
    expconvert=FILE : File containing 'Header', 'Experiment' conversion data [None]
    experiments=LIST: List of (converted) experiment names to use []
    seqin=FILE      : File containing EST/cDNA data to search for tags within [None]
    tagindex=FILE   : File containing possible tags and sequence names from seqin file [*.tag.index]
    tagmap=FILE     : Tag to sequence mapping file to over-ride auto-generated file based on Seqin and Mismatch [None]
    tagstart=X      : Sequence starting tags ['CATG']
    taglen=X        : Length of sequence tags [26]
    ### ~ PROCESS OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    mintag=X        : Minimum total number of counts for a tag to be included (summed replicates) [0]
    minabstag=X     : Minimum individual number of counts for a tag to be included (ANY one replicate) [5]
    minexptag=X     : Minimum number of experiments for a tag to be included (no. replicates) [3]
    allreptag=X     : Filter out any Tags that are not returned by ALL replicates of X experiments [0]
    minenrtag=X     : Minimum number of counts for a tag to be retained for enrichment etc. (summed replicates) [15]
    enrcut=X        : Minimum mean fold change between experiments [2.5]
    pwenr=X         : Minimum fold change between pairwise experiment comparisons [1.0]
    expand=T/F      : Whether to expand from enriched TAGs to unenriched TAGs through shared sequence hits [True]
    mismatch=X      : No. mismatches to allow. -1 = Exact matching w/o BLAST [-1]
    bestmatch=T/F   : Whether to stop looking for more mismatches once hits of a given stringency found [True]
    normalise=X     : Method for normalising tag counts within replicate (None/ppm) [ppm]
    ### ~ OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : "Base" name for all results files, e.g. X.gfessa.tdt [TAG file basename]
    longtdt=T/F     : Whether to output "Long" format file needed for R analysis [True]

See also rje.py generic commandline options.

### ~~~~~~~ Module gopher ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/gopher.py] ~~~~~~~ ###

Program:      GOPHER
Description:  Generation of Orthologous Proteins from Homology-based Estimation of Relationships
Version:      3.4.2
Last Edit:    17/06/15
Citation:     Davey, Edwards & Shields (2007), Nucleic Acids Res. 35(Web Server issue):W455-9. [PMID: 17576682]
Copyright (C) 2005 Richard J. Edwards - See source code for GNU License Notice

Function:
    This script is designed to take in two sequences files and generate datasets of orthologous sequence alignments.
    The first `seqin` sequence set is the 'queries' around which orthologous datasets are to be assembled. This is now
    optimised for a dataset consisting of one protein per protein-coding gene, although splice variants should be dealt
    with OK and treated as paralogues. This will only cause problems if the postdup=T option is used, which restricts
    orthologues returned to be within the last post-duplication clade for the sequence.

    The second `orthdb` is the list of proteins from which the orthologues will be extracted. The `seqin` sequences are
    then BLASTed against the orthdb and processed (see below) to retain putative orthologues using an estimation of the
    phylogenetic relationships based on pairwise sequences similarities.

    NB. As of version 2.0, gopher=FILE has been replaced with seqin=FILE for greater rje python consistency. The `allqry`
    option has been removed. Please cleanup the input data into a desired non-redundant dataset before running GOPHER.
    (In many ways, GOPHER's strength is it's capacity to be run for a single sequence of interest rather than a whole
    genome, and it is this functionality that has been concentrated on for use with PRESTO and SLiM Pickings etc.) The
    output of statistics for each GOPHER run has also been discontinued for now but may be reintroduced with future
    versions. The `phosalign` command (to produce a table of potential phosphorylation sites (e.g. S,T,Y) across
    orthologues for special conservation of phosphorylation prediction analyses) has also been discontinued for now.

    Version 2.1 has tightened up on the use of rje_seq parameters that were causing trouble otherwise. It is now the
    responsibility of the user to make sure that the orthologue database meets the desired criteria. Duplicate accession
    numbers will not be tolerated by GOPHER and (arbitrary) duplicates will be deleted if the sequences are the same, or
    renamed otherwise. Renaming may cause problems later. It is highly desirable not to have two proteins with the same
    accession number but different amino acid sequences. The following commands are added to the rje_seq object when input
    is read: accnr=T unkspec=F specnr=F gnspacc=T. Note that unknown species are also not permitted.

    Version 3.0 has improved directory organisation for multi-species and multi-orthdb GOPHER runs on the same system, in
    line with the bioware.ucd.ie webserver. Additional data cleanup has been added too. (NB. Experimental Sticky GOPHER
    runs will not work with Organise=T.) The output directory is now set at the highest level by gopherdir=PATH. If
    organise=T then a subdirectory will be created within this directory named after the `orthdb` (path and extension
    stripped) and each species will have its own subdirectory within this in turn. By default, these are named using the
    UniProt species codes, which are read from the input sequences. To use TaxIDs, the unispec=FILE option must be used.
    This should point to a file that has one line per UniProt species code, starting with the species code and ending
    with :TaxID: where TaxID is a number. The file should be sorted by species code.

Processing:
    The process for dataset assembly is as follows for each protein :

    1. BLAST against orthdb [`orthblast`]
    * BLASTs saved in BLAST/AccNum.blast

    2. Work through BLAST hits, indentifying paralogues (query species duplicates) and the closest homologue from each
    other species. This involves a second BLAST of the query versus original BLAST hits (blaste=10, no complexity
    filter). The best sequence from each species is kept, i.e. the one with the best similarity to the query and not part
    of a clade with any paralogue that excludes the query. (If postdup=T, the hit must be in the query's post duplication
    clade.) In addition hits:  [`orthfas`]
    * Must have minimum identity level with Query
    * Must be one of the 'good species' [goodspec=LIST]
    * Save reduced sequences as ORTH/AccNum.orth.fas
    * Save paralogues identified (and meeting minsim settings) in PARA/AccNum.para.fas

    3. Align sequences with MUSCLE  [`orthalign`]
    * ALN/AccNum.orthaln.fas

    4. Generate an unrooted tree with (ClustalW or PHYLIP)  [`orthtree`]
    * TREE/AccNum.orth.nsf

    Optional paralogue/subfamily output:  (These are best not used with Force=T or FullForce=T)
    2a. Alignment of query protein and any paralogues >minsim threshold (paralign=T/F). The parasplice=T/F controls
    whether splice variants are in these paralogue alignments (where identified using AccNum-X notation).
    * PARALN/AccNum.paraln.fas

    2b. Pairwise combinations of paralogues and their orthologues aligned, with "common" orthologues removed from the
    dataset, with a rooted tree and group data for BADASP analysis etc. (parafam=T)
    * PARAFAM/AccNum+ParaAccNum.parafam.fas
    * PARAFAM/AccNum+ParaAccNum.parafam.nsf
    * PARAFAM/AccNum+ParaAccNum.parafam.grp

    2c. Combined protein families consisting of a protein, all the paralogues > minsim and all orthologues for each in a
    single dataset. Unaligned. (gopherfam=T)
    * SUBFAM/AccNum.subfam.fas

    NB. The subfamily outputs involve Gopher calling itself to ensure the paralogues have gone through the Gopher
    process themselves. This could potentially cause conflict if forking is used.

Commandline:
    ### Basic Input/Output ###
    seqin=FILE      : Fasta file of 'query' sequences for orthology discovery (over-rides uniprotid=LIST) []
    uniprotid=LIST  : Extract IDs/AccNums in list from Uniprot into BASEFILE.fas and use as seqin=FILE. []
    orthdb=FILE     : Fasta file with pool of sequences for orthology discovery []. Should contain query sequences.
    startfrom=X     : Accession Number / ID to start from. (Enables restart after crash.) [None]
    dna=T/F         : Whether to analyse DNA sequences (not optimised) [False]

    ### GOPHER run control parameters ###
    orthblast   : Run to blasting versus orthdb (Stage 1).
    orthfas     : Run to output of orthologues (Stage 2). 
    orthalign   : Run to alignment of orthologues (Stage 3).
    orthtree    : Run to tree-generation (Stage 4). [default!]

    ### GOPHER Orthologue identifcation Parameters ###
    postdup=T/F     : Whether to align only post-duplication sequences [False]
    reciprocal=T/F  : Use Reciprocal Best Hit method instead of standard GOPHER approach [False]
    fullrbh=T/F     : Whether RBH method should run BLAST searches for all potential RBH orthologues [False]
    compfilter=T/F  : Whether to use complexity filter and composition statistics for *initial* BLAST [True]
    minsim=X        : Minimum %similarity of Query for each "orthologue" [40.0]
    simfocus=X      : Style of similarity comparison used for MinSim and "Best" sequence identification [query]
        - query = %query must > minsim (Best if query is ultimate focus and maximises closeness of returned orthologues)
        - hit = %hit must > minsim (Best if lots of sequence fragments are in searchdb and should be retained)
        - either = %query > minsim OR %hit > minsim (Best if both above conditions are true)
        - both = %query > minsim AND %hit > minsim (Most stringent setting)
    gablamo=X       : GABLAMO measure to use for similarity measures [Sim]
        - ID = %Identity (from BLAST)
        - Sim = %Similarity (from BLAST)
        - Len = %Coverage (from BLAST)
        - Score = BLAST BitScore (Reciprocal Match only)
    goodX=LIST      : Filters where only sequences meeting the requirement of LIST are kept.
                      LIST may be a list X,Y,..,Z or a FILE which contains a list [None]
                        - goodacc  = list of accession numbers
                        - goodseq  = list of sequence names
                        - goodspec = list of species codes
                        - gooddb   = list of source databases
                        - gooddesc = list of terms that, at least one of which must be in description line
    badX=LIST       : As goodX but excludes rather than retains filtered sequences

    ### Additional run control options ###
    repair=T/F      : Repair mode - replace previous files if date mismatches or files missing.
                      (Skip missing files if False) [True]
    force=T/F       : Whether to force execution at current level even if results are new enough [False]
    fullforce=T/F   : Whether to force current and previous execution even if results are new enough [False]
    dropout=T/F     : Whether to "drop out" at earlier phases, or continue with single sequence [False]
    ignoredate=T/F  : Ignores the age of files and only replaces if missing [False]
    savespace=T/F   : Save space by deleting intermediate blast files during orthfas [True]
    maxpara=X       : Maximum number of paralogues to consider (large gene families can cause problems) [50]
    oldblast=T/F    : Run with old BLAST rather than BLAST+ [False]

    ### Additional Output Options ###
    runpath=PATH    : Directory from which to run GOPHER. (NB. Will look for input here unless full paths given) [./]
    gopherdir=PATH  : Parent directory for output of files [./]
    organise=T/F    : Output files according to orthdb an species (code or TaxaID - need conversion) [True]
    unispec=FILE    : UniProt species file for TaxaID conversion. Will use TaxaID instead of Species Code if given [None]
    alnprog=X       : Choice of alignment program to use (clustalw/clustalo/muscle/mafft/fsa) [clustalo]
    orthid=X        : File identifier (Lower case) for orthology files [orth]
    paralign=T/F    : Whether to produce paralogue alignments (>minsim) in PARALN/ (assuming run to orthfas+) [False]
    parasplice=T/F  : Whether splice variants (where identified) are counted as paralogues [False]
    parafam=T/F     : Whether to paralogue paired subfamily alignments (>minsim) (assuming run to orthfas+) [False]
    gopherfam=T/F   : Whether to combined paralogous gopher orthologues into protein families (>minsim) (assuming run to orthfas+) [False]
    sticky=T/F      : Switch on "Sticky Orthologous Group generation" [False] *** Experimental ***
    stiggid=X       : Base for Stigg ID numbers [STIGG]

### ~~~~~~~~ Module happi ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/happi.py] ~~~~~~~~ ###

Program:      HAPPI
Description:  Hyperlinked Analysis of Protein-Protein Interactions
Version:      1.2
Last Edit:    06/03/13
Citation:     Edwards et al. (2011), Molecular Biosystems DOI: 10.1039/C1MB05212H. [PMID: 21879107]
Copyright (C) 2010  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module makes a set of linked webpages for the quick and dirty analysis of PPI networks around one or more
    sets of genes. A table of genes and their corresponding classes is used to make the initial front page tables.

    Additional data tables can also be loaded and linked via the "Gene" field for individual Gene Pages. These must
    be named in the format X.N.tdt, where N will be used as the tab name.

    Additional classes can be added using the makeclass and classorder lists. Any classes in classorder that are not
    found in indata will be added using the rules set by makeclass:
    - go = GO term description or ID
    - keyword = found in GO term description or protein description
    - desc = found in protein description
    - xref = adds PPI for protein identified from xrefdata table
    - add = will only add a gene to a class if it does not already have one (no multiple-class genes)

Commandline:
    ### ~ INPUT ~ ###
    indata=FILE     : Input data for front pages []
    pagehead=X      : Text for Front Page Header ['HAPPI Analysis of basefile']
    infotext=X      : Text to display under header ['This data has not yet been published and should not be used without permission.']
    genefield=X     : Field to be used for indentifying Genes ['Gene']
    geneclass=X     : Field used to identify class of Genes ['Class']
    classcol=X      : Table of "Class" and "Col" (soton$col indexes) to be used for PPI images [basefile.col.tdt]
    fillcol=T/F     : Fill in colour for missing class combinations [True]
    classorder=LIST : List of Class orders (otherwise alphabetical) []
    multiclass=T/F  : Whether to allow membership of multiple classes (joined by "-" [True]
    makeclass=LIST  : Generate classes from classorder LIST if missing from indata (go/keyword/desc/xref(+ppi)/add*) [keyword]
    genedata=LIST   : List of additional data tables for Gene-centric pages. Must have "Gene" or genefield field. []
    xrefdata=FILE   : File of Database cross-references. Must have "Gene" field. []
    xreftab=T/F     : Add database cross-reference tabs to the front page Class tabs [True]
    pairwise=FILE   : Pairwise protein-protein interation (PPI) file []
    addppi=LIST     : List of additional PPI pairwise files to add []
    addclass=T/F    : Whether to add the Classes themselves to the PPI networks as nodes [False]
    godata=FILE     : Delimited file of GO Data (Gene,GO_ID,GO_Type,GO_Desc) [basefile.go.tdt]
    gablam=FILE     : Delimited GABLAM results file for homology data [basefile.gablam.tdt]
    ppexpand=X      : Expand PPI by X levels for MCODE complex generation [1]
    ppcomplex=LIST  : List of different evidence codes for special MCODE analyses ['Complex']
    ppextra=T/F     : Make additional pages for genes added and returned in MCODE clusters [False]
    combine=LIST    : List of Classes to combine into Interactome for front page ('all' or '*' for all) []
    special=LIST    : Execute special analysis code for specific applications []

    ### ~ BASIC OUTPUT OPTIONS ~ ###
    htmlpath=PATH       : Path of parent html directory [./html]
    stylesheets=LIST    : List of CSS files to use ['../example.css','../redwards.css']
    border=X            : Border setting for tables [0]
    dropfields=LIST     : Fields to exclude from summary tables []
    makepng=T/F         : Whether to (look for and) make PNG files with R [True]
    pngmax=X            : Max number of genes for PNG construction [2000]
    svg=T/F             : Use SVG files instead of PNG files [True]
    xgmml=T/F           : Whether to also output XGMML files in PNG/SVG directories [False]
    makepages=LIST      : Types of pages to make [front,gene,go,mcode,class,interactome,expand]
    titletext=FILE      : File containing (Page,ID,Title) for mouseover text [titletext.tdt]
    gopages=X           : Create Pages of GO terms with X+ representative genes [5]
    maxgo=X             : Go terms above X genes will not have gene data tabs [500]
    nobots=T/F          : Whether to insert no-bot meta tag to pages [True]

    ### ~ FIGURE REMAKING OPTIONS ~ ###
    usepos=T/F          : Whether to use existing Node positions if found [True]
    updatepos=T/F       : Whether to run an additional round of the layout algorithm if npos found [False]

    *** See also RJE_PPI Fragment and MCODE options ***
    *** See also RJE_GO options ***

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_ppi, rje_uniprot, rje_zen
Other modules needed: None

### ~~~~~~ Module haqesac ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/haqesac.py] ~~~~~~ ###

Program:      HAQESAC
Description:  Homologue Alignment Quality, Establishment of Subfamilies and Ancestor Construction
Version:      1.11.0
Last Edit:    31/10/17
Citation:     Edwards et al. (2007), Nature Chem. Biol. 3(2):108-112. [PMID: 17220901]
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    HAQESAC is a tool designed for processing a dataset of potential homologues into a trustworthy gobal alignment and well
    bootstrap-supported phylogeny. By default, an intial dataset consisting of homologues (detected by BLAST, for example)
    is processed into a dataset consisting of the query protein and its orthologues in other species plus paralogous
    subfamilies in the same gene family.

    Individual sequences are therefore screened fulfil two criteria:
       1. They must be homologous to (and alignable with) the query sequence of interest.
       2. They must be a member of a subfamily within the gene family to which the query sequence belongs. 

    HAQ. The first stage of data cleanup is therefore to remove rogue sequences that either do not fit in the gene family at
    all or are too distantly related to the query protein for a decent alignment that can be used for useful further
    analysis. This is achieved firstly by a simple identity cut-off, determined by pairwise alignments of sequences, and
    then by a more complex procedure of removing sequences for whom the overall alignability is poor. During this procedure,
    sequences that have too many gaps are also removed as too many gapped residues can cause problems for downstream
    evolutionary analyses. Further screening is achieved based on phylogenetic information.

    ES. Once the dataset has been 'cleaned up' (and, indeed, during processing), HAQESAC can be used to assign sequences to
    subgroups or subfamilies, if such information is needed for downstream analyses.

    AC. The final step that HAQESAC is able to perform is ancestral sequence prediction using the GASP (Gapped Ancestral
    Sequence Prediction) algorithm (Edwards & Shields 2005)

    By default, HAQESAC will perform all these operations. However, it is possible to turn one or more off and only, for
    example, reject individually badly aligned sequences, if desired. Details can be found in the Manual:
    https://github.com/slimsuite/SLiMSuite/blob/master/docs/manuals/HAQESAC%20Manual.pdf.

    HAQESAC outputs can be controlled by d=X:
    0 - *.fas, *.grp, *.nwk, *.tree.txt, *.png, *.anc.*
    1 - *.bak (seqin), *.clean.fas, *.haq.fas, *.degap.fas
    2 - *.saq.*.fas, *.paq.*.fas, *.scanseq.fas
    3 - *.saqx.*.fas

Commandline Options:
    # General Dataset Input/Output Options # 
    seqin=FILE  : Loads sequences from FILE (fasta, phylip, aln, uniprot format, or list of fastacmd names for fasdb option)
    query=X     : Selects query sequence by name (or part of name, e.g. Accession Number)
    acclist=X   : Extract only AccNums in list. X can be FILE or list of AccNums X,Y,.. [None]
    fasdb=FILE  : Fasta format database to extract sequences from [None]
    basefile=X  : Basic 'root' for all files X.* [By default will use 'root' of seqin=FILE if given or haq_AccNum if qblast]
    v=X         : Sets verbosity (-1 for silent) [0]
    i=X         : Sets interactivity (-1 for full auto) [0]
    d=X         : Data output level (0-3, see docstring) [1]
    resdir=PATH : Output directory for d>0 outputs [./HAQESAC/]
    log=FILE    : Redirect log to FILE [Default = calling_program.log or basefile.log]
    newlog=T/F  : Create new log file. [Default = False: append log file]
    multihaq=T/F: If pickle present, will load and continue, else will part run then pickle and stop [False]
    
    # Pre-HAQESAC Data selection #
    qseqfile=FILE   : Sequence file from which to extract query (query=X) and peform BLAST [None]
    qblast=FILE     : BLAST database against which to BLAST query (see rje_blast.py options) [None] ['blaste=1e-7','blastv=1000,blastb=0']
    qfastacmd=X     : Extract query X from qblast database using fastacmd (may also need query=X) [None]

    # Pre-HAQESAC Sequence Filtering #
    gnspacc=T/F     : Convert sequences into gene_SPECIES__AccNum format wherever possible. [True] 
    backup=T/F      : Whether to backup initial fasta file. Will overwrite existing *.fas.bak. [True]
    accnr=T/F       : Check for redundant Accession Numbers/Names on loading sequences. [False]
    #!# filterseq=FILE  : Filters out sequences in given file (Log, Fasta or list of names)
    #!# filterspec=FILE : Filters out sequences according to species (codes) listed in given file
    unkspec=T/F     : Whether sequences of unknown species are allowed [True]
    dblist=X,Y,..,Z : List of databases in order of preference (good to bad)
    dbonly=T/F      : Whether to only allow sequences from listed databases [False]
    #!# keepdesc=FILE   : Only keeps sequences with 1+ of text listed in given file in sequence description
    minlen=X        : Minimum length of sequences [0]
    maxlen=X        : Maximum length of sequences (<=0 = No maximum) [0]
    goodX=LIST      : Filters where only sequences meeting the requirement of LIST are kept.
                      - LIST may be a list X,Y,..,Z or a FILE which contains a list [None]
                        - goodacc  = list of accession numbers
                        - goodseq  = list of sequence names
                        - goodspec = list of species codes
                        - gooddb   = list of source databases
                        - gooddesc = list of terms that, at least one of which must be in description line
    badX=LIST       : As goodX but excludes rather than retains filtered sequences

    # Pre-HAQ Data Cleanup #
    cleanup=T/F : Initial data cleanup [True]
    seqnr=T/F   : Make sequence Non-Redundant [True]
    specnr=T/F  : Non-Redundancy within same species only [True]
    nrid=X      : %Identity cut-off for Non-Redundancy [99.0]
    blastcut=X  : Maximum number of sequences to have in dataset (BLAST query against NR dataset.) [0]
    blaste=X    : E-Value cut-off for BLAST searches (BLAST -e X) [1e-4]
    blastv=X    : Number of one-line hits per query (BLAST -v X) [500]
    blastf=T/F  : Complexity Filter (BLAST -F X) [True]
    qcover=X    : Min. % (ordered) BLAST coverage of Query vs Hit or Hit vs Query [60.0]
    gablam=T/F  : Whether to use GABLAMO Global Alignment from BLAST Local Alignment Matrix (Ordered) rather than ALIGN [True]
    gabsim=T/F  : Whether to use %Similarity for GABLAMO comparisons, rather than %Identity [True]
    pairid=X    : Fasta Alignment ID cut-off for any pair of sequences [0.0]
    qryid=X     : Fasta Alignment ID with Query cut-off [40.0]
    maxgap=X    : Maximum proportion of sequence that may be gaps compared to nearest neighbour (<=0 = No maximum) [0.5]

    # General HAQ Options #
    qregion=X,Y     : Concentrate on the region of the query from (and including) residue X to residue Y [0,-1]
    haq=T/F         : Homologue Alignment Quality [True]
    noquery=T/F     : No Query for SAQ, Random Query for PAQ (else query=X or first sequence) [False]
    keep=T/F        : Keep all sequences (saqkl=0, paqkl=0) [False]
    usealn=T/F      : Use current alignment (do not realign, degap=F) [False]
    cwcut=X         : Total number of residues above which to use ClustalW for alignment in place of alnprog=X [0]
    haqmatrix=FILE  : File of AA vs AA scores used in SAQ and PAQ. [None]

    # Single Sequence Alignment Quality (SAQ) Options #
    saq=T/F     : Single Sequence AQ [True]
    saqc=X      : Min score for a residue in SAQ (Default matrix: no. seqs to share residue). [2]
    saqb=X      : SAQ Block length. [10]
    saqm=X      : No. residues to match in SAQ Block. [7]
    saqks=X     : Relative Weighting of keeping Sequences in SAQ. [3]
    saqkl=X     : Relative Weighting of keeping Length in SAQ. [1]
    mansaq=T/F  : Manual over-ride of sequence rejection decisions in SAQ [False]

    # Pairwise Alignment Quality (PAQ) Options #
    prepaq=T/F  : PrePAQ tree grouping [True]
    paq=T/F     : Pairwise AQ [True]
    paqb=X      : PAQ Block length. [7]
    paqm=X      : Min score in PAQ Block (Default matrix: No. residues to match). [3]
    paqks=X     : Relative Weighting of keeping Sequences in PAQ. [3]
    paqkl=X     : Relative Weighting of keeping Length in PAQ. [1]
    manpaq=T/F  : Manual over-ride of sequence rejection decisions in PAQ [False]

    # Establishment of Subfamilies (and PrePAQ) Options #
    es=T/F      : Establishment of Subfamilies [True]
    root=X      : Rooting of tree (rje_tree.py), where X is:
        - mid = midpoint root tree. [Default]
        - ran = random branch.
        - ranwt = random branch, weighted by branch lengths.
        - man = always ask for rooting options (unless i<0).
        - FILE = with seqs in FILE as outgroup. (Any option other than above)
    bootcut=X   : cut-off percentage of tree bootstraps for grouping.
    mfs=X       : minimum family size [3]
    fam=X       : minimum number of families (If 0, no subfam grouping) [0]
    orphan=T/F  : Whether orphans sequences (not in subfam) allowed. [True]
    allowvar=T/F: Allow variants of same species within a group. [False]
    qryvar=T/F  : Keep variants of query species within a group (over-rides allowvar=F). [False]
    groupspec=X : Species for duplication grouping [None]
    qspec=T/F   : Whether to highlight query species in PNG tree files [True]
    specdup=X   : Minimum number of different species in clade to be identified as a duplication [2]
    group=X     : Grouping of tree
        - man = manual grouping (unless i<0).
        - dup = duplication (all species unless groupspec specified).
        - qry = duplication with species of Query sequence (or Sequence 1) of treeseq
        - one = all sequences in one group
        - None = no group (case sensitive)
        - FILE = load groups from file
    phyoptions=FILE : File containing extra Phylip tree-making options ('batch running') to use [None]
    protdist=FILE   : File containing extra Phylip PROTDIST ('batch running') to use [None]
    maketree=X      : Program for making tree [None]
        - None = Do not make tree from sequences 
        - clustalw = ClustalW NJ method
        - neighbor = PHYLIP NJ method (NB. Bootstraps not yet supported)
        - upgma    = PHYLIP UPGMA (neighbor) method (NB. Bootstraps not yet supported)
        - fitch    = PHYLIP Fitch method (NB. Bootstraps not yet supported)
        - kitsch   = PHYLIP Kitsch (clock) method (NB. Bootstraps not yet supported)
        - protpars = PHYLIP MP method (NB. Bootstraps not yet supported)
        - proml    = PHYLIP ML method (NB. Bootstraps not yet supported)
        - PATH     = Alternatively, a path to a different tree program/script can be given. This should accept ClustalW parameters.

    # Ancestor Construction (GASP) Options
    ac=T/F      : Ancestor Construction (GASP) [True]
    pamfile=FILE: Sets PAM1 input file [jones.pam]
    pammax=X    : Initial maximum PAM matrix to generate [100]
    pamcut=X    : Absolute maximum PAM matrix [1000]
    fixpam=X    : PAM distance fixed to X [0].
    rarecut=X   : Rare aa cut-off [0.05].
    fixup=T/F   : Fix AAs on way up (keep probabilities) [True].
    fixdown=T/F : Fix AAs on initial pass down tree [False].
    ordered=T/F : Order ancestral sequence output by node number [False].
    pamtree=T/F : Calculate and output ancestral tree with PAM distances [True].
    desconly=T/F: Limits ancestral AAs to those found in descendants [True].
    xpass=X     : How many extra passes to make down & up tree after initial GASP [1].

    # System Info Options #
    See rje_seq and general commandline options for external program system path settings.


### ~~~~~ Module multihaq ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/multihaq.py] ~~~~~ ###

Program:      MultiHAQ
Description:  Multi-Query HAQESAC controller
Version:      1.3.0
Last Edit:    08/09/17
Citation:     Jones, Edwards et al. (2011), Marine Biotechnology 13(3): 496-504. [PMID: 20924652]
Copyright (C) 2009  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is a wrapper for multiple HAQESAC runs where different query proteins are to be BLASTed against the same
    search database(s) and run through HAQESAC with the same settings. The default expectation is that some queries will
    be returned by the HAQESAC runs of other queries and may therefore be skipped as a result, although this can be
    switched off using screenqry=F. For large runs, the first phase of MulitHAQ will take a long time to run. In these
    cases, it may be desirable to set the second, interactive, phase running before it has finished. This is achieved
    using the "chaser" option, which will set the second phase in motion, "chasing" the progress of the first. To avoid
    jumbled log output, this should be given a different log file using log=FILE.

    Note: that all options will be output into a haqesac.ini file in the haqdir path, for both HAQESAC runs within the
    framework of MultiHAQ itself and also for later runs using the batch file produced. Any generic HAQESAC options
    should therefore be placed into a multihaq.ini file, not a haqesac.ini file and multiple runs with different settings
    using the same haqdir should be avoided.

    Note: Because HAQESAC makes use of RJE_SEQ filtering options, they will NOT be applied to the MultiHAQ query input
    file prior to analysis. To filter this input, run it through rje_seq.py separately in advance of running multihaq.

Commandline:
    ### ~~~ INPUT OPTIONS ~~~ ###
    seqin=FILE      : Input query sequences [None]
    blast2fas=LIST  : List of databases to BLAST queries against prior to HAQESAC []
    addqueries=T/F  : Whether to add query database to blast2fas list [True]
    keepblast=T/F   : Whether to keep BLAST results files [True]
    blastcut=X      : Restrict HAQESAC and MultiHAQ BLAST searches to top X BLAST hits [0]
    multicut=X      : Restrict MultiHAQ BLASTs to the top X hits from each database (over-rides blastcut) [0]

    ### ~~~ MULTIHAQ OPTIONS ~~~ ###
    haqesac=T/F     : Run HAQESAC (True) or limit to batch file output (False) [True]
    multihaq=T/F    : Whether to run HAQESAC in two-phase multihaq mode [True]
    screenqry=T/F   : Whether to look for queries in previous runs and give option to skip [True]
    autoskip=T/F    : Whether to automatically skip queries found in previous runs [False]
    chaser=T/F      : Option for running second phase of multihaq as second run whilst first run in progress [False]
    force=T/F       : Whether to force re-running of stages (True) or pick-up pre-existing runs (False) [False]

    ### ~~~ OUTPUT OPTIONS ~~~~ ###
    haqdir=PATH     : Directory in which to output HAQESAC files and perform run [seqin_HAQESAC]

See also haqesac.py commands and rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~~~~~ Module pagsat ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/pagsat.py] ~~~~~~~ ###

Module:       PAGSAT
Description:  Pairwise Assembled Genome Sequence Analysis Tool
Version:      2.3.3
Last Edit:    23/06/17
Copyright (C) 2015  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for the assessment of an assembled genome versus a suitable reference. For optimal results, the
    reference genome will be close to identical to that which should be assembled. However, comparative analyses should
    still be useful when different assemblies are run against a related genome - although there will not be the same
    expectation for 100% coverage and accuracy, inaccuracies would still be expected to make an assembly less similar
    to the reference.

Input:
    Main input for PAGSAT is an assembled genome in fasta format (`assembly=FILE`) and a reference genome in fasta format
    (`refgenome=FILE` or `reference=FILE`) with corresponding `*.gb` or `*.gbk` genbank download for feature extraction.
    For full function, a features table plus protein and gene sequences should be provided. (These will be parsed from
    a Genbank reference file.) Basic contig-reference mapping and plotting will still be performed with a pure sequence
    reference that lacks features or gene sequences.

    ### ~ Reference Sequence Naming ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    PAGSAT expects a particular naming format for assembly sequences, which is a bit more constrained that most programs.
    This is to enable the full suite of visualisation with clear unambiguous labelling of contigs. Input sequence names
    must be in the form: `ctgXX_SPCODE__ACCBASE.YY`, where both `ctgXX` and `YY` are *unique* for each contig.
    (Generally `XX` and `YY` will match but this is not a requirement.) `ACCBASE` could be the same for all sequences, or
    it could be sequence-specific. It can also include `.` characters: only the final `.` element must be unique. `YY`
    must end with numbers. (These are expected to be contig numbers, possibly with a prefix.)

Output:
    Main output is a number of delimited text files and PNG graphics made with R. Details to follow.

    NOTE: Snapper is now used for the underlying Reference vs Assembly GABLAM searches (unless `snapper=F`). For speed,
    the SNP mapping functions are switched off. To get the full set of Snapper outputs, use `makesnp=T`.

    ### ~ MapFas Output ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    The `mapfas=T` function generates a new copy of the assembly (in the main PAGSAT output directory) in which contigs
    have been reorientated to be the same strand as the reference chromosomes where possible. This is performed on the
    basis of the reference chromosome with maximum unique coverage for each contig, e.g the contig will be matched to the
    reference chromosome for which it has the most bases that ONLY map to that chromosome. (Clearly this will only work
    if the reference is haploid!) Where the majority of matching bases are on the negative strand, the contig will be
    reverse complemented and the accnum updated such that `XXXX.YY` becomes `XXXX.RYY`.

    If the assembly has `*.coverage.tdt` and `*.depthplot.tdt` data (generated by
    `SeqSuite.SAMTools` or `PAGSAT` if a `*.sam` file is provided), these files will be converted to `*.map.*.tdt` that
    match the `*.map.fasta` contigs.

Commandline:
    ### ~ Input/Setup Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    assembly=FILE   : Fasta file of assembled contigs to assess [None]
    refgenome=FILE  : Fasta file of reference genome for assessment (also *.gb for full functionality) [None]
    spcode=X        : Species code for reference genome (if not already processed by rje_genbank) [None]
    minqv=X         : Minimum mean QV score for assembly contigs (read from *.qv.csv) [20]
    mincontiglen=X  : Minimum contig length to retain in assembly (QV filtering only) [1000]
    casefilter=T/F  : Whether to filter leading/trailing lower case (low QV) sequences [True]
    ### ~ Reference vs Assembly Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    minlocid=X      : Minimum percentage identity for local hits mapping to chromosome coverage [95.0]
    minloclen=X     : Mininum length for local hits mapping to chromosome coverage [250]
    genesummary=T/F : Whether to include reference gene searches in summary data [True]
    protsummary=T/F : Whether to include reference protein searches in summary data [True]
    tophitbuffer=X  : Percentage identity difference to keep best hits for reference genes/proteins. [1.0]
    diploid=T/F     : Whether to treat assembly as a diploid [False]
    minunique=X     : Minimum number of "Unique-mapping" nucleotides to retain a contig-chromosome link [250]
    snapper=T/F     : Run Snapper to generate "best" unique mapping of assembly contigs to Reference [True]
    makesnp=T       : Generate the full set of SNP outputs for Snapper [False]
    ### ~ Output Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : Basename for output files and directories. [assembly+ref]
    rgraphics=T/F   : Whether to generate PNG graphics using R. (Needs R installed and setup) [True]
    dotplots=T/F    : Whether to use gablam.r to output dotplots for all ref vs assembly. [False]
    report=T/F      : Whether to generate HTML report [True]
    genetar=T/F     : Whether to tar and zip the GeneHits/ and ProtHits/ folders (if generated & Mac/Linux) [True]
    chromalign=T/F  : [Discontinued] Whether to perform crude chromosome-contig alignment [False]
    orderedfas=T/F  : Whether to generate crude ordered contig output for e.g. Progressive Mauve [False]
    treeformats=LIST: Output formats for chromosome versus contig %identify UPGMA tree [nwk,png]
    dismatrix=T/F   : Whether to generate distance matrix of chromosome vs contig identities [False]
    ### ~ Comparison Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    compare=FILES   : Compare assemblies selected using a list of *.Summary.tdt files (wildcards allowed). []
    fragcov=LIST    : List of coverage thresholds to count min. local BLAST hits (checks integrity) [50,90,95,99]
    chromcov=LIST   : Report no. of chromosomes covered by a single contig at different %globID (GABLAM table) [95,98,99]
    ### ~ Assembly Tidy/Edit Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    mapfas=T/F      : Output assembly *.map.fasta file with RevComp contigs based on initial (automatic) mapping [True]
    tidy=T/F        : Execute semi-automated assembly tidy/edit mode to complete draft assembly [False]
    newacc=X        : New base for edited contig accession numbers (None will keep old accnum) [None]
    newchr=X        : Code to replace "chr" in new sequence names for additional PAGSAT compatibility [ctg]
    refchr=X        : Code used in place of "chr" for reference sequence names [chr]
    orphans=T/F     : Whether to include and process orphan contigs [True]
    joinsort=X      : Whether to sort potential chromosome joins by `Length` or `Identity` [Identity]
    joinmerge=X     : Merging mode for joining chromosomes (mid/start/end/longest) [mid]
    joinmargin=X    : Number of extra bases allowed to still be considered an end local BLAST hit [10]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~~ Module pagsat_V1 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/pagsat_V1.py] ~~~~ ###

Module:       PAGSAT
Description:  Pairwise Assembled Genome Sequence Analysis Tool
Version:      1.12.0
Last Edit:    26/09/16
Copyright (C) 2015  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for the assessment of an assembled genome versus a suitable reference. For optimal results, the
    reference genome will be close to identical to that which should be assembled. However, comparative analyses should
    still be useful when different assemblies are run against a related genome - although there will not be the same
    expectation for 100% coverage and accuracy, inaccuracies would still be expected to make an assembly less similar
    to the reference.

    Main input for PAGSAT is an assembled genome in fasta format (`assembly=FILE`) and a reference genome in fasta format
    (`refgenome=FILE` or `reference=FILE`) with corresponding `*.gb` or `*.gbk` genbank download for feature extraction.

Output:
    Main output is a number of delimited text files and PNG graphics made with R. Details to follow.

Commandline:
    ### ~ Input/Setup Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    assembly=FILE   : Fasta file of assembled contigs to assess [None]
    refgenome=FILE  : Fasta file of reference genome for assessment (also *.gb for full functionality) [None]
    spcode=X        : Species code for reference genome (if not already processed by rje_genbank) [None]
    minqv=X         : Minimum mean QV score for assembly contigs (read from *.qv.csv) [20]
    mincontiglen=X  : Minimum contig length to retain in assembly (QV filtering only) [1000]
    casefilter=T/F  : Whether to filter leading/trailing lower case (low QV) sequences [True]
    ### ~ Reference vs Assembly Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    minlocid=X      : Minimum percentage identity for local hits mapping to chromosome coverage [95.0]
    minloclen=X     : Mininum length for local hits mapping to chromosome coverage [250]
    genesummary=T/F : Whether to include reference gene searches in summary data [True]
    protsummary=T/F : Whether to include reference protein searches in summary data [True]
    tophitbuffer=X  : Percentage identity difference to keep best hits for reference genes/proteins. [1.0]
    diploid=T/F     : Whether to treat assembly as a diploid [False]
    ### ~ Output Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : Basename for output files and directories. [assembly+ref]
    chromalign=T/F  : Whether to perform crude chromosome-contig alignment [True]
    rgraphics=T/F   : Whether to generate PNG graphics using R. (Needs R installed and setup) [True]
    dotplots=T/F    : Whether to use gablam.r to output dotplots for all ref vs assembly. [False]
    report=T/F      : Whether to generate HTML report [True]
    genetar=T/F     : Whether to tar and zip the GeneHits/ and ProtHits/ folders (if generated & Mac/Linux) [True]
    ### ~ Comparison Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    compare=FILES   : Compare assemblies selected using a list of *.Summary.tdt files (wildcards allowed). []
    fragcov=LIST    : List of coverage thresholds to count min. local BLAST hits (checks integrity) [50,90,95,99]
    chromcov=LIST   : Report no. of chromosomes covered by a single contig at different %globID (GABLAM table) [95,98,99]
    ### ~ Assembly Tidy/Edit Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    tidy=T/F        : Execute semi-automated assembly tidy/edit mode to complete draft assembly [False]
    newacc=X        : New base for edited contig accession numbers (None will keep old accnum) [None]
    newchr=X        : Code to replace "chr" in new sequence names for additional PAGSAT compatibility [ctg]
    orphans=T/F     : Whether to include and process orphan contigs [True]
    chrmap=X        : Contig:Chromosome mapping mode for assembly tidy (unique/align) [unique]
    joinsort=X      : Whether to sort potential chromosome joins by `Length` or `Identity` [Identity]
    joinmerge=X     : Merging mode for joining chromosomes (consensus/end) [end]
    joinmargin=X    : Number of extra bases allowed to still be considered an end local BLAST hit [10]
    snapper=T/F     : Run Snapper on ctidX/haploid output following PAGSAT Tidy. (Re-Quiver recommended first.) [False]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~ Module peptcluster ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/peptcluster.py] ~~~ ###

Module:       PeptCluster
Description:  Peptide Clustering Module
Version:      1.5.4
Last Edit:    01/11/17
Webserver:    http://bioware.soton.ac.uk/peptcluster.html
Copyright (C) 2012  Richard J. Edwards - See source code for GNU License Notice

Function:
    This program is for simple sequence-based clustering of short (aligned) peptide sequences. First, a pairwise distance
    matrix is generated from the peptides. This distance matrix is then used to generate a tree using a distance method
    such as Neighbour-Joining or UPGMA.

    Default distances are amino acid property differences loaded from an amino acid property matrix file.

    Version 1.5.0 incorporates a new peptide alignment mode to deal with unaligned peptides. This is controlled by the
    `peptalign=T/F/X` option, which is set to True by default. If given a regular expression, this will be used to guide
    the alignment. Otherwise, the longest peptides will be used as a guide and the minimum number of gaps added to
    shorter peptides. Pairwise peptide distance measures are used to assess different variants, starting with amino acid
    properties, then simple sequence identity (if ties) and finally PAM distances. One of the latter can be set as
    the priority using `peptdis=X`. Peptide alignment assumes that peptides have termini (^ & $) or flanking wildcards
    added. If not, set `termini=F`.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    peptides=LIST   : These can be entered as a list or file. Lines following '#' or '>' ignored [peptides.txt]
    aaprop=FILE     : File of amino acid properties [aaprop.txt]
    aadis=FILE      : Alternative amino acid distance matrix [None]
    peptalign=T/F/X : Align peptides. Will use as guide regular expression, else T/True for regex-free alignment. []
    termini=T/F     : Whether peptides for alignment have termini (^ & $) or X flanking regex match [True]
    maxgapvar=X     : Maximum number of consecutive gaps to allow for peptide alignment without Regex guide [3]
    maxgapx=X       : Maximum total number of gaps to allow for peptide alignment without Regex guide [5]

    ### ~ CLUSTER OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    peptdis=X       : Method for generating pairwise peptide distances (id/prop/pam) [prop]
    peptcluster=X   : Clustering mode (upgma/wpgma/neighbor) [upgma]
    
    ### ~ OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    savedis=FILE    : Output distance matrix to file [peptides.*]
    outmatrix=X     : Type for output matrix - tdt / csv / mysql / phylip / png [tdt]
    savetree=FILE   : Save generated tree as FILE [peptides.peptdis.peptcluster.*]
    treeformats=LIST: List of output formats for generated trees (nsf/nwk/text/r/png/bud/qspec/cairo/te/svg/html) [nwk,text]
    
See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_obj, rje_zen
Other modules needed: None

### ~~~~~~~~ Module picsi ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/picsi.py] ~~~~~~~~ ###

Program:      PICSI
Description:  Proteomics Identification from Cross-Species Inference
Version:      1.2
Last Edit:    26/03/14
Citation:     Jones, Edwards et al. (2011), Marine Biotechnology 13(3): 496-504. [PMID: 20924652]
Copyright (C) 2010  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for cross-species protein identifications using searches against NCBInr, for example. MASCOT
    processing uses BUDAPEST. Hits are then converted into peptides for redundancy removal. Hits from a given (known)
    query species are preferentially kept and any peptides belonging to those hits are purged from hits returned by
    other species. All hits are then classified:
    - UNIQUE    : Contains 2+ peptides, including 1+ unique peptides
    - NR        : Contains 2+ peptides; None unique but 1+ peptides only found in other "NR" proteins
    - REDUNDANT : Contains 2+ peptides but all found in proteins also containing UNIQUE peptides
    - REJECT    : Identified by <2 peptides once query-species peptides subtracted

Commandline:
    ### ~~~ INPUT OPTIONS ~~~ ###
    seqin=FILE      : File containing sequences hit during searches [None]
    resfiles=FILES  : List of CSV MASCOT output files []
    sumfile=FILE    : Delimited summary file containing search,prot_hit_num,prot_acc,prot_desc,pep_seq
    qryspec=FILE    : Species code for Query species [EMIHU]
    spectdt=FILE    : File of UniProt species translations ['/scratch/RJE_Filestore/SBSBINF/Databases/DBase_090505/UniProt/uniprot.spec.tdt']
    ### ~~~ OUTPUT OPTIONS ~~~ ###
    basefile=FILE   : Base for output files [picsi]
    delimit=X       : Delimiter for output files [tab]

See also rje.py generic commandline options.

### ~~~~~ Module pingu_V3 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/pingu_V3.py] ~~~~~ ###

Program:      PINGU
Description:  Protein Interaction Network & GO Utility
Version:      3.9
Last Edit:    16/07/13
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This utility was originally created for handling proteomics data with EnsEMBL peptide IDs. The data needed to be
    mapped onto Genes, overlaps and redundancies identified, gene lists output for GO analysis with FatiGO, and PPI data
    from HPRD and BioGRID to identify potential complexes.

    See rje_ensembl documentation for details of what to download for EnsGO files and how to make EnsLoci files etc. The
    ens_SPECIES.GO.tdt file used for go mapping should be suitable for the ensmap=FILE file.

    Giving a ppioutdir=PATH will produce combined PPI sequence files for all genes. A pingu.combinedppi.tdt summary file
    will be placed in resdir.

    QSLiMFinder=FILE will perform an analysis for shared motifs between primary interactors of those genes identified
    in a given sample and the original sequence used for the pulldown. FILE should be a fasta file where names of the
    sequences match Sample names. Datasets will then be formed that contain that sequence plus the primary PPI of each
    gene in that sample as a dataset named SAMPLE_GENE.fas in a directory RESDIR/SLiMFinder.

Commandline:
    ### Main Input Options ###
    data=LIST       : List of files of results containing "Sample" and "Identifier" columns
    ensmap=FILE     : Mappings from EnsEMBL - peptides, genes and HGNC IDs (from BioMart)
    ipilinks=FILE   : IPI Links file with 'IPI', 'Symbol' and 'EnsG' fields []
    ensloci=FILE    : File of EnsEMBL genome EnsLoci treatment []
    baits=LIST      : List of genes of interest for overlap analysis []
    addbaits=T/F    : Whether to add primary interactors of baits as additional samples [False]
    combaits=X      : Whether to combine bait PPIs into single sample (X) (if addbaits=T) []
    controls=LIST   : List of sample names that correspond to controls [Control]
    experiments=LIST: List of sample names that correspond to key samples of interest []
    exponly=T/F     : Limited analysis to samples listed as experiments (before baits added etc.) [False]
    addalias=FILE   : Extra (manual?) aliases to add to GeneMap object following loading of pickles etc. [None]

    ### Processing Options ###
    hgnconly=T/F    : Whether to restrict PPI data to only those proteins with Gene Symbol links [False]
    pickle=T/F      : Whether to save/load pickle of parsed/combined data rather than regenerating each time [True]
    pingupickle=FILE: Full path to Pingu pickle file to look for/use/save [pingu.pickle]
    nocontrols=T/F  : Whether to remove genes found in designated controls from designated experiments [False]
    gablam=T/F      : Whether to run all-by-all GABLAM on EnsLoci and add homology to networks [True]
    ppitype=LIST    : List of acceptable interaction types to parse out []
    badtype=LIST    : List of bad interaction types, to exclude [indirect_complex,neighbouring_reaction]
    makefam=X       : GABLAM Percentage identity threshold for grouping sequences into families [0.0]
    gofilter=LIST   : List of GO IDs to filter out of gene lists []
    goexcept=LIST   : List of GO ID exceptions to filtering []
    remsticky=X     : Remove "sticky" hubs as defined by >X known PPI [0]
    stickyhubs=T/F  : Only remove "sticky" spokes but keep sticky hubs [False]
    stickyppi=T/F   : Only remove "sticky" hubs from samples, not from total PPI [False]
    addlinks=T/F    : Add linking proteins (linking two Sample proteins) [False]
    
    ### Main Output Options ###
    resdir=PATH     : Redirect output files to specified directory [./]
    basefile=X      : Results file prefix if no data file given with data=FILE [pingu]
    fulloutput=T/F  : Generate all possible outputs from one input [False]
    genelists=T/F   : Generate lists of genes for each sample (e.g. for FatiGO upload) [False]
    gosummary=T/F   : Make a GO summary table [False]
    summaryhgnc=T/F : Generate a summary table of genes in dataset, including peptide lists for each sample [False]
    mapout=T/F      : Generate a summary table of full peptide mapping [False]
    dbcomp=T/F      : Comparison of PPI databases [False]
    dbsizes=T/F     : Outputs a file of PPI dataset sizes (histogram) [False]
    allbyall=X      : Generates an all-by-all table of PPI links upto X degrees of separation (sample only) [0]
    pathfinder=X    : Perform (lengthy) PathFinder analysis to link genes upto X degree separation (-1 = no limit) [0]
    pathqry=LIST    : Limit PathFinder analysis to start with given queries []
    overlap=T/F     : Produce a table of the overlap (mapped through HGNC) between samples (and bait 1y PPI) [False]
    cytoscape=T/F   : Produce old cytoscape input files from allbyall table (reads back in) [False]
    xgmml=T/F       : Produce an XGMML file with all Cytoscape data and more [False]
    xgformat=T/F    : Whether to add colour/shape formatting to XGMML output [False]
    xgexpand=X      : Expand XGMML network with additional levels of interactors [0]
    xgcomplex=T/F   : Restrict XGMML output (and expansion) to protein complex edges [False]
    compresspp=T/F  : Whether to compress multiple samples of interest into ShareX for cytoscape [False]
    seqfiles=T/F    : Whether to generate protein sequence fasta files using EnsLoci [False]
    goseqdir=PATH   : Path to output full GO fasta files (No output if blank/none) []
    ppioutdir=PATH  : Path to output combined PPI files (No output if blank/none) []
    acconly=T/F     : Whether to output lists of Accession numbers only, rather than full fasta files [False]
    ensdat=PATH     : Path to EnsDAT files to use for making combined PPI datasets [None]
    qslimfinder=FILE: File containing sequences matching Sample names for Query SLiMFinder runs [None]
    screenddi=FILE  : Whether to screen out probably domain-domain interactions from file [None]
    domppidir=PATH  : Produce domain-based PPI files and output into PATH (No output if blank/none) []
    nocomplex=T/F   : Perform crude screening of complexes (PPI triplets w/o homodimers) [False]
    fasid=X         : Text ID for PPI fasta files ['ppi']
    association=T/F : Perform experiment association analysis [False]
    asscombo=T/F    : Whether to subdivide genes further based on combinations of experiments containing them [False]
    noshare=T/F     : Whether to exclude those genes that are shared between samples when comparing those samples [True]
    selfonly=T/F    : Whether to only look at associations within experiments, not between [False]
    randseed=X      : Seed for randomiser [0]
    randnum=X       : Number of randomisations [1000]
    
    ### Database/Path options ###
    enspath=PATH    : Path to EnsEMBL downloads
    ensgopath=PATH  : Path to EnsGO files   (!!! Restricted to Humans Currently !!!)
    unipath=PATH    : Path to UniProt files [UniProt/]
    hprd=PATH       : Path to HPRD flat files [None]
    biogrid=FILE    : BioGRID flat file [None]
    intact=FILE     : IntAct flat file [None]
    mint=FILE       : MINT flat file [None]
    reactome=FILE   : Reactome interactions flat file [None]
    dip=FILE        : DIP interactions flat file [None]
    domino=FILE     : Domino interactions flat file [None]
    pairwise=FILE   : Load interaction data from existing Pingu Pairwise file [None]
    addppi=FILE     : Add additional PPI from a simple delimited file IDA,IDB,Evidence [None]
    genepickle=FILE : Pickled GeneMap object. Alternatively, use below commands to make GeneMap object [None]
    - hgncdata/sourcedata/pickledata/aliases : See rje_genemap docstring.
    pfamdata=FILE   : Delimited files containing domain organisation of sequences [None]
    evidence=FILE   : Mapping file for evidence terms [None]

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~~~ Module pingu_V4 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/pingu_V4.py] ~~~~~ ###

Module:       PINGU
Description:  Protein Interaction Network & GO Utility
Version:      4.9.0
Last Edit:    18/01/17
Copyright (C) 2013  Richard J. Edwards - See source code for GNU License Notice

Function:
    PINGU (Protein Interaction Network & GO Utility) is designed to be a general utility for Protein Protein Interaction
    (PPI) and Gene Ontology (GO) analysis. Earlier versions of PINGU contained a lot of the code for processing PPI and
    GO data, which have subsequently been moved to `rje_ppi.py` and `rje_go.py` libraries.

    PINGU 3.x was dominated by code to compile PPI data from multiple databases and map onto sequences from different
    sources, in combination with `rje_dbase.py` database downloads and processing. There was a substantial amount of
    code for mapping data with different IDs, including peptide-based MS Ensembl identifications on to HGNC gene
    identifiers. Some of this code is now handled by `rje_genemap.py` whilst some of it has been depracated due to newer
    (better) datasets and/or a shift in focus of the Edwards lab.

    PINGU 4.x is designed to work in a more streamlined fashion with a more controlled subset of data, making
    documentation and re-use a bit simpler and clearer. Many of the older functions are therefore run using
    `pingu_V3.py`, in which case a `#PINGU` log statement will be generated. Some older functions will only be possible
    by running `pingu_V3.py` directly.

    PINGU 4.0 is designed to work with HINT interaction data and UniProt sequences. The initial PPI download and
    compilation is based on `SLiMBench`. This has been updated in 4.9.0 following some changed to HINT downloads
    (http://hint.yulab.org/download/) - there may be some additional unexpected/unwelcome consequences of these changes.

    PINGU 4.1 updated the PPI compilation methods of PINGU 3.x, which can be triggered using `ppicompile=T`. This will
    need a database cross-reference file (`xrefdata=LIST`).

    PINGU 4.2 will download and use HGNC as a database xref file if xrefdata=HGNC. Clearly, this will only work for
    human data. Note that HINT is mapped to genes via Uniprot entries and does not use the xrefdata table.

    PINGU 4.3 add domain-based domppi dataset generation (`domppi=T`). This uses Pfam domain composition from Uniprot to
    generate datasets of proteins that interact with hubs sharing a domain.

    PINGU 4.4.x replaced `ppicompile=T` with `ppicompile=CDICT`. `HPRD`, `HINT` and `Reactome` will be recognised and parsed
    using custom methods: `hprd=PATH` and `reactome=FILE` must be set; HINT data will be read from `sourcepath=PATH/`.
    Otherwise, entries will be treated as files (wildcard lists allowed) and either parsed as a pairwise PPI file (`Hub`
    and `Spoke` fields found) else a MITAB file (see rje_mitab for advanced field settings). The compiled PPI data
    will be output to `BASEFILE.pairwise.tdt` and used as `ppisource=X` for additional processing/output.

    Default mapping fields for XRef mapping are: `Secondary,Ensembl,Aliases,Accessions,RefSeq,Previous Symbols,Synonyms`.
    `Secondary` will be added from Uniprot data if missing from the XRef table. `unifield=X` will also be added to the
    map fields if not included.

    PINGU 4.6.x fixed/updated the PPI Fasta output methods (`ppifas=T`). These will output to a directory named after the
    `ppisource` file and `ppispec`. Each hub gene will produce a fasta file, `gene.fasid.fas` where `fasid` is set by the
    `ppisource` unless changed with `fasid=X`. If `combineppi=T`, a single `spec.fasid.fas` file will be created. The
    `xhubppi=T` setting will generate a set of files containing spoke proteins that have x+ Hub interactors. Note that
    `*.1hub.fas` is essentially the same as the `combineppi=T` `spec.fasid.fas` file.

    PINGU 4.7 added `ppidbreport=T/F` output for PPI compilation, summarising the evidence codes and PPITypes read from
    different sources.

Commandline:
    ### ~ SOURCE DATA OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    sourcepath=PATH/    : Will look in this directory for input files if not found ['SourceData/']
    sourcedate=DATE     : Source file date (YYYY-MM-DD) to preferentially use [None]
    ppisource=X         : Source of PPI data. (HINT/FILE) FILE needs 'Hub', 'Spoke' and 'SpokeUni' fields. ['HINT']
    ppispec=LIST        : List of PPI files/species/databases to generate PPI datasets from [HUMAN]
    download=T/F        : Whether to download files directly from websites where possible if missing [True]
    integrity=T/F       : Whether to quit by default if source data integrity is breached [True]
    xrefdata=LIST       : List of files with delimited data of identifier cross-referencing (see rje_xref) []
    unifield=X          : Uniprot accession number field identifier for xrefdata ['Uniprot']
    mapfields=LIST      : List of XRef fields to use for identifier mapping (plus unifield) [see docs]

    ### ~ OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    resdir=PATH         : Redirect output files/directories to specified directory [./]
    basefile=X          : Results file prefix [pingu]
    acconly=T/F         : Whether to output lists of Accession numbers only, rather than full fasta files [False]
    fasid=X             : Text ID for fasta files (*.X.fas) [default named after ppisource(+'-dom')]

    ### ~ PPI OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    ppiout=FILE         : Save pairwise PPI file following processing (if rest=None) [None]
    ppifas=T/F          : Whether to output PPI fasta files [False]
    domppi=T/F          : Whether to generate Pfam Domain-based PPI files instead of protein-based PPI files [False]
    minppi=X            : Minimum number of PPI for file output [0]
    combineppi=T/F      : Whether to combine all spokes into a single fasta file [False]
    xhubppi=T/F         : Whether to generate PPI files of spokes interacting with X+ hubs [False]
    queryppi=FILE       : Load a file of 'Query','Hub' PPI and generate expanded PPI Datasets in PPI.*/ [None]
    queryseq=FILE       : Fasta file containing the Query protein sequences corresponding to QuerySeq [*.fas]
    allquery=T/F        : Whether to include all the new Queries from QueryPPI in all files for a given hub [True]

    ### ~ ADVANCED/DEV OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    sourceurl=CDICT     : Dictionary of Source URL mapping (see code)

    ### ~ PPI COMPILATION/FILTERING OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    hublist=LIST        : List of hub genes to restrict pairwise PPI to []
    hubonly=T/F         : Whether to restrict pairwise PPI to those with both hub and spoke in hublist [False]
    ppicompile=CDICT    : List of db:file PPI Sources to compile and generate *.pairwise.tdt []
    ppidbreport=T/F     : Summary output for PPI compilation of evidence/PPIType/DB overlaps [True]
    symmetry=T/F        : Whether to enforce Hub-Spoke symmetry during PPI compilation [True]
    hprd=PATH           : Path to HPRD flat files [None]
    taxid=LIST          : List of NCBI Taxa IDs to use [9606]
    badppi=LIST         : PPI Types to be removed. Will only remove PPI if no support remains []
    goodppi=LIST        : Reduce PPI to those supported by listed types []
    baddb=LIST          : PPI Types to be removed. Will only remove PPI if no support remains []
    gooddb=LIST         : Reduce PPI to those supported by listed types []

    ### ~ OBSOLETE OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    biogrid=FILE        : BioGRID flat file [None]
    intact=FILE         : IntAct flat file [None]
    mint=FILE           : MINT flat file [None]
    reactome=FILE       : Reactome interactions flat file [None]
    dip=FILE            : DIP interactions flat file [None]
    domino=FILE         : Domino interactions flat file [None]
    evidence=FILE       : Mapping file for evidence terms [None]    #!# Not currently implemented! #!#
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

See also rje.py generic commandline options.

### ~~~~ Module presto_V5 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/presto_V5.py] ~~~~ ###

Program:      PRESTO
Description:  Protein Regular Expression Search Tool
Version:      5.0
Last Edit:    22/01/07
Note:         This program has been superceded in most functions by SLiMSearch.
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    PRESTO is what the acronym suggests: a search tool for searching proteins with peptide sequences or motifs using an
    algorithm based on Regular Expressions. The simple input and output formats and ease of use on local databases make
    PRESTO a useful alternative to web resources for high throughput studies.

    The additional benefits of PRESTO that make it more useful than a lot of existing tools include:
    * PRESTO can be given alignment files from which to calculate conservation statistics for motif occurrences.
    * searching with mismatches rather than restricting hits to perfect matches.
    * additional statistics, inlcuding protein disorder, surface accessibility and hydrophobicity predictions
    * production of separate fasta files containing the proteins hit by each motif.
    * production of both UniProt format results and delimited text results for easy incorporation into other applications.
    * inbuilt tandem Mass Spec ambiguities. 

    PRESTO recognises "n of m" motif elements in the form <X:n:m>, where X is one or more amino acids that must occur n+
    times across which m positions. E.g. <IL:3:5> must have 3+ Is and/or Ls in a 5aa stretch.

    Main output for PRESTO is a delimited file of motif/peptide occurrences but the motifaln=T and proteinaln=T also allow
    output of alignments of motifs and their occurrences. PRESTO has an additional motinfo=FILE output, which produces a
    summary table of the input motifs, inlcuding Expected values if searchdb given and information content if motifIC=T.
    Hit proteins can also be output in fasta format (fasout=T) or UniProt format with occurrences as features (uniprot=T).
    
Release Notes:
    Expectation scores have now been modified since PRESTO Version 1.x. In addition to the expectation score for the no.
    of occurrences of a given motif (given the number of mismatches) in the entire dataset ("EXPECT"), there is now an
    estimation of the probability of the observed number of occurrences, derived from a Poisson distribution, which is
    output in the log file ("#PROB"). Further more, these values are now also calculated per sequence individually
    ("SEQ_EXP" and "SEQ_PROB").

    Note on MS-MS mode: The old Perl version of Presto had a handy MS-MS mode for searching peptides sequenced from tandem
    mass-spec data. (In this mode [msms=T], amino acids of equal mass (Leu-Ile [LI], Gln-Lys [QK], MetO-Phe [MF]) are
    automatically placed as possible variants and additional output columns give information of predicted tryptic fragment
    masses etc.) Implementation of MS-MS mode has been started in this version but discontinued due to lack of demand. As a
    result, extra tryptic fragment data is not produced. If you would like to use it, contact me at richard.edwards@ucd.ie
    and I will finish implementing it.

    Note for compare=T mode: This is still fully functional but main documentation has been moved to comparimotif.py.

    !!!NEW!!! for version 3.7, PRESTO has an additional domfilter=FILE option. This is quite crude and will read in domains
    to be filtered from the FILE given. This file MUST be tab-delimited and must have at least three columns, with headers
    'Name','Start' and 'Stop', where Name matches the short name of the Hit and 'Start' and 'End' are the positions of the
    domain 1-N. This will output two additional columns, plus a further two if iupred=T:
    * DOM_MASK = Gives the motif a score of the length of the domain if it would be masked out by masking domains or 0 if not
    * DOM_PROP = Gives the proportion of motif positions in a domain
    * DOM_DIS = Gives the motif the mean disorder score for the *domain* if in the domain, else 1.0 if not
    * DOM_COMB = Gives positions in the domain the mean disorder score for the domain, else they keep their own scores

    !!!NEW!!! for version 4.0, PRESTO has a Peptide design mode (peptides=T), using winsize=X to set size of peptides around
    occurrences. This will output peptide sequences into a fasta file and additional columns to the main PRESTO output file:
    * PEP_SEQ = Sequence of peptide
    * PEP_DESIGN = Peptide design comments. "OK" if all looking good, else warnings bad AA combos (DP, DC, DG, NG, NS or PP)

Development Notes: (To be assimilated with release notes etc. when version is fully functional.)
    Main output is now determined by outfile=X and/or basefile=X, which will set the self.info['Basefile'] attribute,
    using standard rje module commands. If it is not set (i.e. is '' or 'None'), it will be generated using the motif and
    searchdb files as with the old PRESTO. Main search output will use this file leader and add the appropriate extension based
    on the output type and delimiter:
    * resfile = Main PRESTO search = *.presto.tdt
    * motifaln = Produce fasta files of local motif alignments *.motifaln.fas
    * uniprot = Output of hits as a uniprot format file = *.uniprot.presto 
    * motinfo = Motif summary table = *.motinfo.tdt
    * ftout = Make a file of UniProt features for extracted parent proteins, where possible, incoroprating SLIMs [*.features.tdt]
    * peptides = Peptides designed around motifs = *.peptides.fas

    Other special output will generate their names using protein and/or motif names using the root PATH of basefile
    (e.g. the PATH will be stripped and ProteinAln/ or HitFas/ directories made for output):
    * proteinaln=T/F  : Search for alignments of proteins containing motifs and produce new file containing motifs in [False]
    * fasout=T/F      : Whether to output hit sequences as a fasta format file motif.fas [False]
    
    Reformatting and ouputting motifs require a file name to be given:
    * motifout=FILE   : Filename for output of reformatted (and filtered?) motifs in PRESTO format [None]
    
    

PRESTO Commands:
    ## Basic Input Parameters ##
    motifs=FILE     : File of input motifs/peptides [None]
                      Single line per motif format = 'Name Sequence #Comments' (Comments are optional and ignored)
                      Alternative formats include fasta, SLiMDisc output and raw motif lists.
    minpep=X        : Min length of motif/peptide X aa [2]
    minfix=X        : Min number of fixed positions for a motif to contain [0]
    minic=X         : Min information content for a motif (1 fixed position = 1.0) [2.0]
    trimx=T/F       : Trims Xs from the ends of a motif [False]
    nrmotif=T/F     : Whether to remove redundancy in input motifs [False]
    searchdb=FILE   : Protein Fasta file to search (or second motif file to compare) [None]
    xpad=X          : Adds X additional Xs to the flanks of the motif (after trimx if trimx=T) [0]
    xpaddb=X        : Adds X additional Xs to the flanks of the search database sequences (will mess up alignments) [0]
    minimotif=T/F   : Input file is in minimotif format and will be reformatted (PRESTO File format only) [False]
    goodmotif=LIST  : List of text to match in Motif names to keep (can have wildcards) []

    ## Basic Output Parameters ##
    outfile=X       : Base name of results files, e.g. X.presto.tdt. [motifsFILE-searchdbFILE.presto.tdt]
    expect=T/F      : Whether to give crude expect values based on AA frequencies [True]
    nohits=T/F      : Save list of sequence IDs without motif hits to *.nohits.txt. [False]
    useres=T/F      : Whether to append existing results to *.presto.txt and *.nohits.txt (continuing afer last sequence)
                      and/or use existing results in to search for conservation in alignments if usealn=T. [False]
    mysql=T/F       : Output results in mySQL format - lower case headers and no spaces [False]
    hitname=X       : Format for Hit Name: full/short/accnum [short]
    fasout=T/F      : Whether to output hit sequences as a fasta format file motif.fas [False]
    datout=T/F      : Whether to output hits as a uniprot format file *.uniprot.presto [False]
    motinfo=T/F     : Whether to output motif summary table *.motinfo.tdt [None]
    motifout=FILE   : Filename for output of reformatted (and filtered?) motifs in PRESTO format [None]

    ## Advanced Output Options ##
    winsa=X         : Number of aa to extend Surface Accessibility calculation either side of motif [0]
    winhyd=X        : Number of aa to extend Eisenberg Hydrophobicity calculation either side of motif [0]
    windis=X        : Extend disorder statistic X aa either side of motif (use flanks *only* if negative) [0]
    winchg=X        : Extend charge calculations (if any) to X aa either side of motif [0]
    winsize=X       : Sets all of the above window sizes (use flanks *only* if negative) [0]
    slimchg=T/F     : Calculate Asolute, Net and Balance charge statistics (above) for occurrences [False]
    iupred=T/F      : Run IUPred disorder prediction [False]
    foldindex=T/F   : Run FoldIndex disorder prediction [False]
    iucut=X         : Cut-off for IUPred results (0.0 will report mean IUPred score) [0.0]
    iumethod=X      : IUPred method to use (long/short) [short]
    iupath=PATH     : The full path to the IUPred exectuable [c:/bioware/iupred/iupred.exe]
    domfilter=FILE  : Use the DomFilter options, reading domains from FILE [None]
    runid=X         : Adds an additional Run_ID column identifying the run (for multiple appended runs [None]
    restrict=LIST   : List of files containing instances (hit,start,end) to output (only) []
    exclude=LIST    : List of files containing instances (hit,start,end) to exclude []
    peptides=T/F    : Peptide design mode, using winsize=X to set size of peptides around motif [False]
    newscore=LIST   : Lists of X:Y, create a new statistic X, where Y is the formula of the score. []

    ## Basic Search Parameters ##
    mismatch=X,Y    : Peptide must be >= Y aa for X mismatches
    ambcut=X        : Cut-off for max number of choices in ambiguous position to be shown as variant [10]
    expcut=X        : The maximum number of expected occurrences allowed to still search with motif [0] (if -ve, per seq)
    alphabet=X,Y,.. : List of letters in alphabet of interest [AAs]
    reverse=T/F     : Reverse the motifs - good for generating a test comparison data set [False]
    *** No longer outputs *.rev.txt - use motifout=X instead! ***

    msms=T/F        : Whether searching Tandem Mass Spec peptides [False]
    ranking=T/F     : Whether to rank hits by their rating in MSMS mode [False]
    memsaver=T/F    : Whether to store all results in Objects (False) or clear as search proceeds (True) [True]    
    startfrom=X     : Accession Number / ID to start from. (Enables restart after crash.) [None]

    ## Conservation Parameters ##
    usealn=T/F      : Whether to search for and use alignemnts where present. [False]
    gopher=T/F      : Use GOPHER to generate missing orthologue alignments in alndir - see gopher.py options [False]
    fullforce=T/F   : Force GOPHER to re-run even if alignment exists [False]
    alndir=PATH     : Path to alignments of proteins containing motifs [./] * Use forward slashes (/)
    alnext=X        : File extension of alignment files, accnum.X [aln.fas]
    alngap=T/F      : Whether to count proteins in alignments that have 100% gaps over motif (True) or (False) ignore
                      as putative sequence fragments [False]  (NB. All X regions are ignored as sequence errors.)
    conspec=LIST    : List of species codes for conservation analysis. Can be name of file containing list. [None]
    conscore=X      : Type of conservation score used:  [pos]
                        - abs = absolute conservation of motif using RegExp over matched region
                        - pos = positional conservation: each position treated independently 
                        - prop = conservation of amino acid properties
                        - all = all three methods for comparison purposes
    consamb=T/F     : Whether to calculate conservation allowing for degeneracy of motif (True) or of fixed variant (False) [True]
    consinfo=T/F    : Weight positions by information content (does nothing for conscore=abs) [True]
    consweight=X    : Weight given to global percentage identity for conservation, given more weight to closer sequences [0]
                        - 0 gives equal weighting to all. Negative values will upweight distant sequences.
    posmatrix=FILE  : Score matrix for amino acid combinations used in pos weighting. (conscore=pos builds from propmatrix) [None]
    aaprop=FILE     : Amino Acid property matrix file. [aaprop.txt]
    consout=T/F     : Outputs an additional result field containing information on the conservation score used [False]

    ## Additional Output for Extracted Motifs ##
    motific=T/F     : Output Information Content for motifs [False]
    motifaln=T/F    : Produce fasta files of local motif alignments [False]  
    proteinaln=T/F  : Search for alignments of proteins containing motifs and produce new file containing motifs [False]
    protalndir=PATH : Directory name for output of protein aligments [ProteinAln/]
    flanksize=X     : Size of sequence flanks for motifs [30]
    xdivide=X       : Size of dividing Xs between motifs [10]
    ftout=T/F       : Make a file of UniProt features for extracted parent proteins, where possible, incoroprating SLIMs [*.features.tdt]
    unipaths=LIST   : List of additional paths containing uniprot.index files from which to look for and extract features ['']
    statfilter=LIST : List of stats to filter (*discard* occurrences) on, consisting of X*Y where:
                      - X is an output stat (the column header),
                      - * is an operator in the list >, >=, !=, =, >= ,<    !!! Remember to enclose in "quotes" for <> !!!
                      - Y is a value that X must have, assessed using *.
                      This filtering is crude and may behave strangely if X is not a numerical stat!

    ## Motif Comparison Parameters ##
    compare=T/F     : Compare the motifs from the motifs FILE with the searchdb FILE (or self if None) [False]
    minshare=X      : Min. number of non-wildcard positions for motifs to share [2]
    matchfix=X      : If >0 must exactly match *all* fixed positions in the motifs from:  [0]
                        - 1: input (motifs=FILE) motifs
                        - 2: searchdb motifs
                        - 3: *both* input and searchdb motifs
    matchic=T/F     : Use (and output) information content of matched regions to asses motif matches [True]
    motdesc=X       : Sets which motifs have description outputs (0-3 as matchfix option) [3]
    outstyle=X      : Sets the output style for the resfile [normal]
                        - normal = all standard stats are output
                        - multi = designed for multiple appended runs. File names are also output
                        - single = designed for searches of a single motif vs a database. Only motif2 stats are output
                        - normalsplit/multisplit = as normal/multi but stats are grouped by motif rather than by type

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_aaprop, rje_disorder, rje_motif_V3, rje_motif_cons, rje_scoring, rje_seq, rje_sequence,
    rje_blast, rje_pam, rje_uniprot
Other modules needed: rje_dismatrix, 

### ~~~ Module qslimfinder ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/qslimfinder.py] ~~~ ###

Program:      QSLiMFinder
Description:  Query Short Linear Motif Finder
Version:      2.2.0
Last Edit:    20/01/17
Citation:     Palopoli N, Lythgow KT & Edwards RJ. Bioinformatics 2015; doi: 10.1093/bioinformatics/btv155 [PMID: 25792551]
SLiMFinder:   Edwards, Davey & Shields (2007), PLoS ONE 2(10): e967. [PMID: 17912346]
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    QSLiMFinder is a modification of the basic SLiMFinder tool to specifically look for SLiMs shared by a query sequence
    and one or more additional sequences. To do this, SLiMBuild first identifies all motifs that are present in the query
    sequences before removing it (and its UPC) from the dataset. The rest of the search and stats takes place using the
    remainder of the dataset but only using motifs found in the query. The final correction for multiple testing is made
    using a motif space defined by the original query sequence, rather than the full potential motif space used by the
    original SLiMFinder. This is offset against the increased probability of the observed motif support values due to the
    reduction of support that results from removing the query sequence but could potentially still identify SLiMs will
    increased significance.

    Note that minocc and ambocc values *include* the query sequence, e.g. minocc=2 specifies the query and ONE other UPC.    
    
Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ### 
    seqin=FILE      : Sequence file to search [None]
    batch=LIST      : List of files to search, wildcards allowed. (Over-ruled by seqin=FILE.) [*.dat,*.fas]
    query=LIST      : Return only SLiMs that occur in 1+ Query sequences (Name/AccNum/Seq Number) [1]
    addquery=FILE   : Adds query sequence(s) to batch jobs from FILE [None]
    maxseq=X        : Maximum number of sequences to process [500]
    maxupc=X        : Maximum UPC size of dataset to process [0]
    sizesort=X      : Sorts batch files by size prior to running (+1 small->big; -1 big->small; 0 none) [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main QSLiMFinder results table [qslimfinder.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [QSLiMFinder/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMFinder/]
    force=T/F       : Force re-running of BLAST, UPC generation and SLiMBuild [False]
    pickup=T/F      : Pick-up from aborted batch run by identifying datasets in resfile using RunID [False]
    dna=T/F         : Whether the sequences files are DNA rather than protein [False]
    alphabet=LIST   : List of characters to include in search (e.g. AAs or NTs) [default AA or NT codes]
    megaslim=FILE   : Make/use precomputed results for a proteome (FILE) in fasta format [None]
    megablam=T/F    : Whether to create and use all-by-all GABLAM results for (gablamdis) UPC generation [False]
    ptmlist=LIST    : List of PTM letters to add to alphabet for analysis and restrict PTM data []
    ptmdata=DSVFILE : File containing PTM data, including AccNum, ModType, ModPos, ModAA, ModCode
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMBuild Options I: Evolutionary Filtering  ###
    efilter=T/F     : Whether to use evolutionary filter [True]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    homcut=X        : Max number of homologues to allow (to reduce large multi-domain families) [0]

    ### SLiMBuild Options II: Input Masking ###
    masking=T/F     : Master control switch to turn off all masking if False [True]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out (True=EM,DOMAIN,TRANSMEM) []
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M (can be useful if termini=T) [True]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []
    qregion=X,Y     : Mask all but the region of the query from (and including) residue X to residue Y [0,-1]
    
    ### SLiMBuild Options III: Basic Motif Construction ###
    termini=T/F     : Whether to add termini characters (^ & $) to search sequences [True]
    minwild=X       : Minimum number of consecutive wildcard positions to allow [0]
    maxwild=X       : Maximum number of consecutive wildcard positions to allow [2]
    slimlen=X       : Maximum length of SLiMs to return (no. non-wildcard positions) [5]
    minocc=X        : Minimum number of unrelated occurrences for returned SLiMs. (Proportion of UP if < 1) [0.05]
    absmin=X        : Used if minocc<1 to define absolute min. UP occ [3]
    alphahelix=T/F  : Special i, i+3/4, i+7 motif discovery [False]

    ### SLiMBuild Options IV: Ambiguity ###
    ambiguity=T/F   : (preamb=T/F) Whether to search for ambiguous motifs during motif discovery [True]
    ambocc=X        : Min. UP occurrence for subvariants of ambiguous motifs (minocc if 0 or > minocc) [0.05]
    absminamb=X     : Used if ambocc<1 to define absolute min. UP occ [2]
    equiv=LIST      : List (or file) of TEIRESIAS-style ambiguities to use [AGS,ILMVF,FYW,FYH,KRH,DE,ST]
    wildvar=T/F     : Whether to allow variable length wildcards [True]
    combamb=T/F     : Whether to search for combined amino acid degeneracy and variable wildcards [False]

    ### SLiMBuild Options V: Advanced Motif Filtering ###
    musthave=LIST   : Returned motifs must contain one or more of the AAs in LIST (reduces search space) []
    focus=FILE      : FILE containing focal groups for SLiM return (see Manual for details) [None]
    focusocc=X      : Motif must appear in X+ focus groups (0 = all) [0]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMChance Options ###
    cloudfix=T/F    : Restrict output to clouds with 1+ fixed motif (recommended) [False]
    slimchance=T/F  : Execute main QSLiMFinder probability method and outputs [True]
    sigprime=T/F    : Calculate more precise (but more computationally intensive) statistical model [False]
    sigv=T/F        : Use the more precise (but more computationally intensive) fix to mean UPC probability [False]
    qexact=T/F      : Calculate exact Query motif space (True) or over-estimate from dimers (False) (quicker) [True]
    probcut=X       : Probability cut-off for returned motifs [0.1]
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [False]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) (!!!Experimental!!!) [None]
    negatives=FILE  : Multiply raw probabilities by under-representation in FILE (!!!Experimental!!!) [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=T/F      : Whether to upweight for multiple occurrences in same sequence (heuristic) [False]
    probscore=X     : Score to be used for probability cut-off and ranking (Prob/Sig) [Sig]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Advanced Output Options I: Output data ###
    clouds=X        : Identifies motif "clouds" which overlap at 2+ positions in X+ sequences (0=minocc / -1=off) [2]
    runid=X         : Run ID for resfile (allows multiple runs on same data) [DATE:TIME]
    logmask=T/F     : Whether to log the masking of individual sequences [True]
    slimcheck=FILE  : Motif file/list to add to resfile output [] 

    ### Advanced Output Options II: Output formats ###
    teiresias=T/F   : Replace TEIRESIAS, making *.out and *.mask.fasta files [False]
    slimdisc=T/F    : Emulate SLiMDisc output format (*.rank & *.dat.rank + TEIRESIAS *.out & *.fasta) [False]
    extras=X        : Whether to generate additional output files (alignments etc.) [1]
                        --1 = No output beyond main results file
                        - 0 = Generate occurrence file and cloud file
                        - 1 = Generate occurrence file, alignments and cloud file
                        - 2 = Generate all additional QSLiMFinder outputs
                        - 3 = Generate SLiMDisc emulation too (equiv extras=2 slimdisc=T)
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle
                        - 2 = Delete all bar *.upc (pickle added to tar)
                        - 3 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)

    ### Advanced Output Options III: Additional Motif Filtering ### 
    topranks=X      : Will only output top X motifs meeting probcut [1000]
    minic=X         : Minimum information content for returned motifs [2.1]
    allsig=T/F      : Whether to also output all SLiMChance combinations (Sig/SigV/SigPrime/SigPrimeV) [False]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    

### ~~~~ Module seqmapper ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/seqmapper.py] ~~~~ ###

Module:       SeqMapper
Description:  Sequence Mapping Program
Version:      2.2.0
Last Edit:    31/10/17
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for mapping one set of protein sequences onto a different sequence database, using Accession Numbers
    etc where possible and then using GABLAM when no direct match is possible. The program gives the following outputs:
    - *.*.mapped.fas = Fasta file of successfully mapped sequences
    - *.*.missing.fas = Fasta file of sequences that could not be mapped
    - *.*.mapping.tdt = Delimited file giving details of mapping (Seq, MapSeq, Method)
    If combine=T then the *.missing.fas file will not be created and unmapped sequences will be output in *.mapped.fas.
    Note that the possible mappings are all identified through BLAST and so a protein with matching IDs etc. but not
    hitting with BLAST will NOT be mapped. Currently only mapping of protein or nucleotides onto a protein database is
    supported.

    Unless the interactivity setting is set to 2 or more (i=2), sequences that are mapped using Name, AccNum, Sequence
    (100% identical sequences), ID or DescAcc will be mapped onto the first appropriate sequence. If automap > 0, then
    the best sequence according to the mapstat will be mapped automatically. If two sequences tie, the other two possible
    stats will also be used to rank the hits. If still tied and mapfocus is not "both" then the sequences will be ranked
    using both query and hit stats. If still tied, the first sequence will be selected.

    Any sequences that fall below automap (or i>1) but meet the minmap criteria will be ranked according to their BLAST
    rankings and then presented for a user decision. Presentation will be in reverse order, so that in the case of many
    possible mappings, the best options remain clear and on screen. The default choice (selected by hitting ENTER) will
    be the best ranked according to GABLAM stats, which will have been moved to position 1 if not already there. (BLAST
    rankings and GABLAM rankings will not always agree.)

    SeqMapper will enter a user menu if i>1 or seqin and/or mapdb are missing. If i=0 and one of these is missing, a
    simple prompt will ask for the missing files. If i<0 and one of these is missing, the program will exit. 

Commandline:
    ### Input Options ###
    seqin=FILE      : File of sequences to be mapped [None]
    mapdb=FILE      : File of sequences to map sequences onto [None]
    startfrom=X     : Shortname or AccNum of seqin file to startfrom (will append results) (memsaver=T only) [None]

    ### Output Options ###
    resfile=FILE    : Base of output filenames (*.mapped.fas, *.missing.fas & *.mapping.tdt)  [seqin.mapdb]
    combine=T/F     : Combine both fasta files in one (e.g. include unmapped sequences in *.mapping.fas) [False]
    gablamout=T/F   : Output GABLAM statistics for mapped sequences, including "straight" matches [True]
    append=T/F      : Append rather than overwrite results files [False]
    delimit=X       : Delimiter for *.mapping.* file (will set extension) [tab]
    basefile=FILE   : Set resfile=FILE and log=FILE at the same time []

    ### Mapping Options ###
    i=X             : Set interactivity. i=-1 full auto. i=0 no menu. i=1 interactive menu. [1]
    mapspec=X       : Maps sequences onto given species code. "Self" = same species as query. "None" = any. [None]
    mapping=LIST    : Possible ways of mapping sequences (in pref order) [Name,AccNum,Sequence,ID,DescAcc,GABLAM,grep]
        - Name = First word of sequence name
        - Sequence = Identical sequence
        - grep = grep-based searching of sequence if no hits
        - ID = SwissProt style ID of GENE_SPECIES (note that the species may be changed according to mapspec)
        - AccNum = Primary Accession Number
        - DescAcc = Accession Number featured in description line in form "\WAccNum\W", where \W is non-
    skipgene=LIST   : List of "genes" in protein IDs to ignore [ens,nvl,ref,p,hyp,frag]
    mapstat=X       : GABLAM Stat to use for mapping assessment (if GABLAM in mapping list) (ID/Sim/Len) [ID]
    minmap=X        : Minimum value of mapstat for any mapping to occur [90.0]
    automap=X       : Minimum value of mapstat for automatic mapping to occur (if i<1) [99.5]
    ordered=T/F     : Whether to use GABLAMO rather than GABLAM stat [True]
    mapfocus=X      : Focus for mapping statistic, i.e. which sequence must meet requirements [query]
        - query = Best if query is ultimate focus and maximises closeness of mapped sequence)
        - hit = Best if lots of sequence fragments are in mapdb and should be allowed as mappings
        - either = Best if both above conditions are true
        - both = Gets most similar sequences in terms of length but can be quite strict where length errors exist

    ### Advanced BLAST Options ###
    blaste=X    : E-Value cut-off for BLAST searches (BLAST -e X) [1e-4]
    blastv=X    : Number of BLAST hits to return per query (BLAST -v X) [20]
    blastf=T/F  : Complexity Filter (BLAST -F X) [False]


### ~~~~~ Module seqsuite ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/seqsuite.py] ~~~~~ ###

Module:       SeqSuite
Description:  Miscellaneous biological sequence analysis toolkit
Version:      1.14.0
Last Edit:    22/08/17
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    SeqSuite is designed to be a front end for the expanding range of miscellaneous sequence utilities that are found
    within the SLiMSuite libraries/ modules. The relevant tool is given by the first system command, or selected using
    `prog=X` (or `program=X`). As much as possible, SeqSuite will emulate running that tool from the commandline, adding
    any matching `X.ini` file to the default commandline options read in (*before* settings read from seqsuite.ini
    itself). By default, the SeqList tool will be called (libraries/rje_seqlist.py) and read in commands from seqlist.ini.

    Help for the selected tool can be accessed using the `help=T` option. Note that `-h`, `-help` or `help` alone will
    trigger the SeqSuite help (this!). As `-help` or `help` will also set `help=T`, these commands will trigger both the
    SeqSuite help and the selected program help (unless over-ruled by `help=F`). An explicit `help=T` command will only
    trigger the selected program help.

SeqSuite tools:
    The list of tools recognised by `prog=X` will be added here as the relevant code is added:
    - BLAST = rje_blast_V2.BLASTRun. BLAST+ Control Module.
    - DBase = rje_dbase.DatabaseController. Database downloading and processing.
    - Ensembl = rje_ensembl.EnsEMBL. EnsEMBL Processing/Manipulation.
    - ExTATIC = extatic.ExTATIC. !!! Development only. Not available in main download. !!!
    - FIESTA = fiesta.FIESTA. Fasta Input EST Analysis. Transcriptome annotation/querying.
    - GABLAM = gablam.GABLAM. Global Analysis of BLAST Local AlignMents
    - Genbank = rje_genbank.GenBank. Genbank fetching/parsing module.
    - GOPHER = gopher.GOPHER. Generation of Orthologous Proteins from Homology-based Estimation of Relationships.
    - HAQESAC = haqesac.HAQESAC. Homologue Alignment Quality, Establishment of Subfamilies and Ancestor Construction.
    - MITAB = rje_mitab.MITAB. MITAB PPI parser.
    - MultiHAQ = multihaq.MultiHAQ. Multi-Query HAQESAC controller.
    - PAGSAT = pagsat.PAGSAT. Pairwise Assembled Genome Sequence Analysis Tool. (Development only)
    - PINGU = pingu_V4.PINGU. Protein Interaction Network & GO Utility.
    - PyDocs = rje_pydocs.PyDoc. Python Module Documentation & Distribution.
    - RJE_Seq = rje_seq.SeqList. Fasta file sequence manipulation/reformatting.
    - SAMTools = rje_samtools.SAMTools. SAMTools mpileup analysis tool. (Development only)
    - SeqList = rje_seqlist.SeqList. Fasta file sequence manipulation/reformatting.
    - SMRTSCAPE (/PacBio) = smrtscape.SMRTSCAPE. SMRT Subread Coverage & Assembly Parameter Estimator. (Development only)
    - Snapper = snp_mapper.SNPMap. SNV to feature annotations mapping and rating tool. (Development only)
    - Taxonomy = rje_taxonomy.Taxonomy. Taxonomy download/conversion tool.
    - Tree = rje_tree.Tree. Phylogenetic Tree Module.
    - Uniprot = rje_uniprot.Uniprot. Uniprot download and parsing module.
    - XRef = rje_xref.XRef. Identifier cross-referencing module.
    - Zen - rje_zen.Zen. Random Zen wisdom generator and test code.

Commandline:
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    prog=X      # Identifies the tool to be used. Will load defaults from X.ini (before seqsuite.ini) [seqlist]
    help=T/F    # Show the help documentation for program X. [False]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    batchrun=FILELIST   # Batch run the program on selected files (wildcards allowed) []
    batcharg=X          # Commandline argument to use for batchrun files ['seqin']
    batchlog=X          # Generate separate basefile.X log files for each batch run file (None for single log) [log]
    batchbase=T/F       # Whether to give each batch run a separate basefile=X command in place of log=X [True]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~~ Module slimbench ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimbench.py] ~~~~ ###

Module:       SLiMBench
Description:  Short Linear Motif prediction Benchmarking
Version:      2.14.0
Last Edit:    08/11/17
Citation:     Palopoli N, Lythgow KT & Edwards RJ. Bioinformatics 2015; doi: 10.1093/bioinformatics/btv155 [PMID: 25792551]
Copyright (C) 2012  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMBench has two primary functions:

    1. Generating SLiM prediction benchmarking datasets from ELM (or other data in a similar format). This includes
    options for generating random and/or simulated datasets for ROC analysis etc.

    2. Assessing the results of SLiM predictions against a Benchmark. This program is designed to work with SLiMFinder
    and QSLiMFinder output, so some prior results parsing may be needed for other methods.

    If `generate=F benchmark=F`, SLiMBench will check and optionally download the input files but perform no additional
    processing or analysis.

    Please see the SLiMBench manual for more details.

Commandline:
    ### ~ SOURCE DATA OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    sourcepath=PATH/    : Will look in this directory for input files if not found [SourceData/]
    sourcedate=DATE     : Source file date (YYYY-MM-DD) to preferentially use [None]
    elmclass=FILE       : Download from ELM website of ELM classes [elm_classes.tsv]
    elminstance=FILE    : Download from ELM website of ELM instances [elm_instances.tsv]
    elminteractors=FILE : Download from ELM website of ELM interactors [elm_interactions.tsv]
    elmdomains=FILE     : Download from ELM website of ELM Pfam domain interactors [elm_interaction_domains.tsv]
    elmdat=FILE         : File of downloaded UniProt entries (See rje_uniprot for more details) ['ELM.dat']
    ppisource=X         : Source of PPI data. (See documentation for details.) (HINT/FILE) ['HINT']
    ppispec=LIST        : List of PPI files/species/databases to generate PPI datasets from [HUMAN,MOUSE,DROME,YEAST]
    ppid=X              : PPI source protein identifier type (gene/uni/none; will work out from headers if None) [None]
    randsource=FILE     : Source for random/simulated dataset sequences. If species, will extract from UniProt [HUMAN]
    randat=T/F          : Whether to use DAT file for random source [False]
    download=T/F        : Whether to download files directly from websites where possible if missing [True]
    integrity=T/F       : Whether to quit by default if source data integrity is breached [False]
    unipath=PATH        : Path to UniProt download. Will query website if "URL" [URL]

    ### ~ GENERAL/ELM BENCHMARK GENERATION OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    genpath=PATH        : Output path for datasets generated with SLiMBench file generator [./SLiMBenchDatasets/]
    generate=T/F        : Whether to generate SLiMBench datasets from ELM input. [False]
    genspec=LIST        : Restrict ELM/OccBench datasets to listed species (restricts ELM instances) []
    slimmaker=T/F       : Whether to use SLiMMaker to "reduce" ELMs to more findable SLiMs [True]
    minupc=X            : Minimum number of UPC for benchmark dataset [3]
    maxseq=X            : Maximum number of sequences for benchmark datasets [0]
    minic=X             : Min information content for a motif (1 fixed position = 1.0) [2.0; 1.1 for OccBench]
    filterdir=X         : Directory suffix for filtered benchmarking datasets [_Filtered/]
    queries=T/F         : Whether to generate datasets with specific Query proteins [False]
    flankmask=LIST      : List of flanking mask options (used with queries and simbench) [none,win100,flank5,site]
    elmbench=T/F        : Whether to generate ELM datasets [True]
    ppibench=T/F        : Whether to generate ELM PPI datasets [True]
    domlink=T/F         : Link ELMs to PPI via Pfam domains (True) or (False) just use direct protein links [True]
    itype=X             : Interaction identifer for PPI datasets [first element of ppisource]
    dombench=T/F        : Whether to generate Pfam domain ELM PPI datasets [True]
    occbench=T/F        : Whether to generate ELM OccBench datasets [True]
    
    ### ~ RANDOM/SIMULATION BENCHMARK GENERATION OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    simbench=T/F        : Whether to generate simulated datasets using reduced ELMs (if found) [False]
    ranbench=T/F        : Whether to generate randomised datasets (part of simulation if simbench=T) [False]
    randreps=X          : Number of replicates for each random (or simulated) datasets [8]
    simcount=LIST       : Number of "TPs" to have in dataset [4,8,16]
    simratios=LIST      : List of simulated ELM:Random ratios [0,1,3,7,15,31]
    randir=PATH         : Output path for creation of randomised datasets [./SLiMBenchDatasets/Random/]
    randbase=X          : Base for random dataset name if simbench=F [ran]
    masking=T/F         : Whether to use SLiMCore masking for query selection [True]
    searchini=FILE      : INI file containing SLiMProb search options that restrict returned positives []
    maxseq=X            : Maximum number of randsource sequences for SLiM to hit (also maxaa and maxupc limits) [1000]

    ### ~ BENCHMARK ASSESSMENT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    benchmark=T/F       : Whether to perfrom SLiMBench benchmarking assessment against motif file [False]
    datatype=X          : Type of data to be generated and/or benchmarked (occ/elm/ppi/dom/sim/simonly) [elm]
    queries=T/F         : Whether to datasets have specific Query proteins [False]
    resfiles=LIST       : List of (Q)SLiMFinder results files to use for benchmarking [*.csv]
    balanced=T/F        : Whether to reduce benchmarking to datasets found for all RunIDs [True]
    compdb=FILE         : Motif file to be used for benchmarking [elmclass file] (reduced unless occ/ppi)
    occbenchpos=FILE    : File of all positive occurrences for OccBench [genpath/ELM_OccBench/ELM.full.ratings.csv]
    benchbase=X         : Basefile for SLiMBench benchmarking output [slimbench]
    runid=LIST          : List of factors to split RunID column into (on '.') [Program,Analysis]
    bycloud=X           : Whether to compress results into clouds prior to assessment (True/False/Both) [Both]
    sigcut=LIST         : Significance thresholds to use for assessment [0.1,0.05,0.01,0.001,0.0001]
    iccut=LIST          : Minimum IC for (Q)SLiMFinder results for elm/sim/ppi benchmark assessment [2.0,2.1,3.0]
    slimlencut=LIST     : List of individual SLiM lengths to return results for (0=All) [0,3,4,5]
    noamb=T/F           : Filter out ambiguous patterns [False]

    ### ~ GENERAL OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    force=T/F           : Whether to force regeneration of outputs (True) or assume existing outputs are right [False]
    backups=T/F         : Whether to (prompt if interactive and) generate backups before overwriting files [True]

See also rje.py generic commandline options.

### ~~~ Module slimfarmer ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimfarmer.py] ~~~ ###

Module:       SLiMFarmer
Description:  SLiMSuite HPC job farming control program
Version:      1.7.0
Last Edit:    09/05/17
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to control and execute parallel processing jobs on an HPC cluster using PBS and QSUB. If
    qsub=T it will generate a job file and use qsub to place that job in the queue using the appropriate parameter
    settings. If `slimsuite=T` and `farm=X` gives a recognised program (below) or `hpcmode` is not `fork` then the qsub
    job will call SLiMFarmer with the same commandline options, plus `qsub=F i=-1 v=-1`. If `seqbyseq=T`, this will be
    run in a special way. (See SeqBySeq mode.) Otherwise `slimsuite=T` indicates that `farm=X` is a SLiMSuite program,
    for which the python call and `pypath` will be added. If this program uses forking then it should parallelise over a
    single multi-processor node. If `farm=X` contains a `/` path separator, this will be added to `pypath`, otherwise it
    will be assumed that `farm` is in `tools/`. If `slimsuite=F` then farm should be a program call to be queued in the
    PBS job file instead.

    Currently recognised SLiMSuite programs for farming: SLiMFinder, QSLiMFinder, SLiMProb, SLiMCore.

    Currently recognised SLiMSuite programs for rsh mode only qsub farming: GOPHER, SLiMSearch, UniFake.

    NOTE: Any commandline options that need bracketing quotes will need to be placed into an ini file. This can either
    be the ini file used by SLiMFarmer, or a `jobini=FILE` that will only be used by the farmed programs. Note that
    commands in `slimfarmer.ini` will not be passed on to other SLiMSuite programs unless `ini=slimfarmer.ini` is given
    as a commandline argument.

    The runid=X setting is important for SLiMSuite job farming as this is what separates different parameter setting
    combinations run on the same data and is also used for identifying which datasets have already been run. Running
    several jobs on the same data using the same SLiMSuite program but with different parameter settings will therefore
    cause problems. If runid is not set, it will default to the job=X setting.

    The hpcmode=X setting determines the method used for farming out jobs across the nodes. hpcmode=rsh uses rsh to spawn
    the additional processes out to other nodes, based on a script written for the IRIDIS HPC by Ivan Wolton.
    hpcmode=fork will restrict analysis to a single node and use Python forking to distribute jobs. This can be used even
    on a single multi-processor machine to fork out SLiMSuite jobs. basefile=X will set the log, RunID, ResFile, ResDir
    and Job: RunID and Job will have path stripped; ResFile will have .csv appended.

    Initially, it will call other programs but, in time, it is envisaged that other programs will make use of SLiMFarmer
    and have parallelisation built-in.

SeqBySeq Mode:
    In SeqBySeq mode, the program assumes that seqin=FILE and basefile=X are given and farm=X states the Python program
    to be run, which should be SLiMSuite program. (The SLiMSuite subdirectory will also need to be given unless
    slimsuite=F, in which case the whole path to the program should be given. pypath=PATH can set an alternative path.)

    Seqin will then be worked through in turn and each sequence farmed out to the farm program. Outputs given by OutList
    are then compiled, as is the Log, into the correct basefile=X given. In the case of *.csv and *.tdt files, the header
    row is copied for the first file and then excluded for all subsequent files. For all other files extensions, the
    whole output is copied.

Commandline:
    ### ~ Basic QSub Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    qsub=T/F        : Whether to execute QSub PDB job creation and queuing [False]
    jobini=FILE     : Ini file to pass to the farmed HPC jobs with SLiMFarmer options. Overrides commandline. [None]
    slimsuite=T/F   : Whether program is an RJE *.py script (adds log processing) [True]
    nodes=X         : Number of nodes to run on [1]
    ppn=X           : Processors per node [16]
    walltime=X      : Walltime for qsub job (hours) [12]
    vmem=X          : Virtual Memory limit for run (GB) [126]
    job=X           : Name of job file (.job added) [slimfarmer]

    ### ~ Advanced QSub Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    hpc=X           : Name of HPC system ['katana']
    pypath=PATH     : Path to python modules [slimsuite home directoy]
    qpath=PATH      : Path to change directory too [current path]
    pause=X         : Wait X seconds before attempting showstart [5]
    email=X         : Email address to email job stats to at end ['']
    mailstart=T/F   : Whether to email user at start of run [False]
    depend=LIST     : List of job ids to wait for before starting job (dependhpc=X added) []
    dependhpc=X     : Name of HPC system for depend ['blue30.iridis.soton.ac.uk']
    report=T/F      : Pull out running job IDs and run showstart [False]
    modules=LIST    : List of modules to add in job file e.g. blast+/2.2.31,clustalw []
    modpurge=T/F    : Whether to purge loaded modules in qsub job file prior to loading [True]
    precall=LIST    : List of additional commands to run between module loading and program call []

    ### ~ Main SLiMFarmer Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    farm=X          : Execute a special SLiMFarm analysis on HPC [batch]
                        - batch will farm out a batch list of commands read in from subjobs=LIST
                        - gopher/slimfinder/qslimfinder/slimprob/slimcore/slimsearch/unifake = special SLiMSuite HPC.
                        - if seqbyseq=T, farm=X will specify the program to be run (see docs)
                        - otherwise, farm=X will be executed as a system call in place of SLiMFarmer
    hpcmode=X       : Mode to be used for farming jobs between nodes (rsh/fork) [fork]
    forks=X         : Number of forks to be used when hpcmode=fork and qsub=F. [1]
    jobini=FILE     : Ini file to pass to the farmed SLiMSuite run. (Also used for SLiMFarmer options if qsub=T.) [None]

    ### ~ Standard HPC Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    subsleep=X      : Sleep time (seconds) between cycles of subbing out jobs to hosts [1]
    subjobs=LIST    : List of subjobs to farm out to HPC cluster []
    iolimit=X       : Limit of number of IOErrors before termination [50]
    memfree=X       : Min. proportion of node memory to be free before spawning job [0.1]
    test=T/F        : Whether to produce extra output in "test" mode [False]
    keepfree=X      : Number of processors to keep free on head node [1]

    ### ~ SeqBySeq Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqbyseq=T/F    : Activate seqbyseq mode - assumes basefile=X option used for output [False]
    seqin=FILE      : Input sequence file to farm out [None]
    basefile=X      : Base for output files - compiled from individual run results [None]
    outlist=LIST    : List of extensions of outputs to add to basefile for output (basefile.*) []
    pickhead=X      : Header to extract from OutList file and used to populate AccNum to skip []

    ### ~ SLiMSuite Farming Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    runid=X         : Text identifier for SLiMSuite job farming [`job`]
    resfile=FILE    : Main output file for SLiMSuite run [`farm`.csv]
    pickup=T/F      : Whether to pickup previous run based on existing results and RunID [True]
    sortrun=T/F     : Whether to sort input files by size and run big -> small to avoid hang at end [True]
    loadbalance=T/F : Whether to split SortRun jobs equally between large & small to avoid memory issues [True]
    basefile=X      : Set the log, RunID, ResFile, ResDir and Job to X [None].

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~ Module slimfinder ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimfinder.py] ~~~ ###

Program:      SLiMFinder
Description:  Short Linear Motif Finder
Version:      5.3.3
Last Edit:    06/09/17
Citation:     Edwards RJ, Davey NE & Shields DC (2007), PLoS ONE 2(10): e967. [PMID: 17912346]
ConsMask Citation: Davey NE, Shields DC & Edwards RJ (2009), Bioinformatics 25(4): 443-50. [PMID: 19136552]
SigV/SigPrime Citation: Davey NE, Edwards RJ & Shields DC (2010), BMC Bioinformatics 11: 14. [PMID: 20055997]
SLiMScape/REST Citation: Olorin E, O'Brien KT, Palopoli N, Perez-Bercoff A & Shields DC, Edwards RJ (2015), F1000Research 4:477.
SLiMMaker Citation: Palopoli N, Lythgow KT & Edwards RJ (2015), Bioinformatics 31(14): 2284-2293. [PMID: 25792551]
Webserver:    http://bioware.ucd.ie/
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    Short linear motifs (SLiMs) in proteins are functional microdomains of fundamental importance in many biological
    systems. SLiMs typically consist of a 3 to 10 amino acid stretch of the primary protein sequence, of which as few
    as two sites may be important for activity, making identification of novel SLiMs extremely difficult. In particular,
    it can be very difficult to distinguish a randomly recurring "motif" from a truly over-represented one. Incorporating
    ambiguous amino acid positions and/or variable-length wildcard spacers between defined residues further complicates
    the matter.

    SLiMFinder is an integrated SLiM discovery program building on the principles of the SLiMDisc software for accounting
    for evolutionary relationships [Davey NE, Shields DC & Edwards RJ (2006): Nucleic Acids Res. 34(12):3546-54].
    SLiMFinder is comprised of two algorithms:

    1. `SLiMBuild` identifies convergently evolved, short motifs in a dataset. Motifs with fixed amino acid positions are
    identified and then combined to incorporate amino acid ambiguity and variable-length wildcard spacers. Unlike
    programs such as TEIRESIAS, which return all shared patterns, SLiMBuild accelerates the process and reduces returned
    motifs by explicitly screening out motifs that do not occur in enough unrelated proteins. For this, SLiMBuild uses
    the "Unrelated Proteins" (UP) algorithm of SLiMDisc in which BLAST is used to identify pairwise relationships.
    Proteins are then clustered according to these relationships into "Unrelated Protein Clusters" (UPC), which are
    defined such that no protein in a UPC has a BLAST-detectable relationship with a protein in another UPC.  If desired,
    `SLiMBuild` can be used as a replacement for TEIRESIAS in other software (teiresias=T slimchance=F).

    2. `SLiMChance` estimates the probability of these motifs arising by chance, correcting for the size and composition
    of the dataset, and assigns a significance value to each motif. Motif occurrence probabilities are calculated
    independently for each UPC, adjusted for the size of a UPC using the Minimum Spanning Tree algorithm from SLiMDisc.
    These individual occurrence probabilities are then converted into the total probability of the seeing the observed
    motifs the observed number of (unrelated) times. These probabilities assume that the motif is known before the
    search. In reality, only over-represented motifs from the dataset are looked at, so these probabilities are adjusted
    for the size of motif-space searched to give a significance value. The returned corrected probability is an estimate
    of the probability of seeing ANY motif with that significance (or greater) from the dataset (i.e. an estimate of the
    probability of seeing that motif, *or another one like it*). These values are calculated separately for each length
    of motif.

    SLiMFinder version 4.0 introduced a more precise (but more computationally intensive) statistical model, which can
    be switched on using sigprime=T. Likewise, the more precise (but more computationally intensive) correction to the
    mean UPC probability heuristic can be switched on using sigv=T. (Note that the other `SLiMChance` options may not
    work with either of these options.) The allsig=T option will output all four scores. In this case, SigPrimeV will be
    used for ranking etc. unless probscore=X is used.

    ### Clouds and Statistics:
    Where significant motifs are returned, SLiMFinder will group them into Motif "Clouds", which consist of physically
    overlapping motifs (2+ non-wildcard positions are the same in the same sequence). This provides an easy indication
    of which motifs may actually be variants of a larger SLiM and should therefore be considered together. From version
    V4.7, `*.cloud.txt` output includes a `SLiMMaker` summary Regex for the whole cloud. NOTE: This may not necessarily
    match all occurrences in the cloud.

    Additional Motif Occurrence Statistics, such as motif conservation, are handled by the `rje_slimlist` module and
    `rje_slimcalc` modules. Please see the documentation for these module for a full list of commandline options. These
    options have not been fully tested in SLiMFinder, so please report issues and/or request desired functions. Note that
    occfilter=LIST *does* affect the motifs returned by SLiMBuild and thus the TEIRESIAS output (as does min. IC and min.
    Support) but the overall Motif slimfilter=LIST *only* affects SLiMFinder output following SLiMChance calculations.

    ### Secondary Functions:
    The "MotifSeq" option will output fasta files for a list of X:Y, where X is a motif pattern and Y is the output file.

    The "Randomise" function will take a set of input datasets (as in Batch Mode) and regenerate a set of new datasets
    by shuffling the UPC among datasets. Note that, at this stage, this is quite crude and may result in the final
    datasets having fewer UPC due to common sequences and/or relationships between UPC clusters in different datasets.

    Where pre-known motifs are also of interest, these can be given with the slimcheck=MOTIFS option and will be added to
    the output. In general, it is better to use `SLiMProb` to look for enrichment (or depletion) of pre-defined motifs.

Input/Output:
    ### SLiMFinder Input:
    The main input for SLiMFinder is the `seqin=SEQFILE` file of protein sequences, which can be Uniprot plain text
    (`DATFILE`) or fasta (`FASFILE`) format. A batch of files (incorporating wildcards) can be given using
    batch=FILELIST. Alternative primary input is uniprotid=LIST. This requires an active internet connection to retrieve
    the corresponding Uniprot entries.

    ### SLiMFinder Output:
    Please see Manual for details.

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options: ###
    seqin=SEQFILE   : Sequence file to search. Over-rules batch=FILE and uniprotid=LIST [None]
    batch=FILELIST  : List of files to search, wildcards allowed. (Over-ruled by seqin=FILE.) [*.dat,*.fas]
    uniprotid=LIST  : Extract IDs/AccNums in list from Uniprot into BASEFILE.dat and use as seqin=FILE. []
    maxseq=X        : Maximum number of sequences to process [500]
    maxupc=X        : Maximum UPC size of dataset to process [0]
    sizesort=X      : Sorts batch files by size prior to running (+1 small->big; -1 big->small; 0 none) [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main SLiMFinder results table [slimfinder.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [SLiMFinder/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMFinder/]
    force=T/F       : Force re-running of BLAST, UPC generation and SLiMBuild [False]
    pickup=T/F      : Pick-up from aborted batch run by identifying datasets in resfile [False]
    pickid=T/F      : Whether to use RunID to identify run datasets when using pickup [True]
    pickall=T/F     : Whether to skip aborted runs (True) or only those datasets that ran to completion (False) [True]
    dna=T/F         : Whether the sequences files are DNA rather than protein [False]
    alphabet=LIST   : List of characters to include in search (e.g. AAs or NTs) [default AA or NT codes]
    megaslim=FILE   : Make/use precomputed results for a proteome (FILE) in fasta format [None]
    megablam=T/F    : Whether to create and use all-by-all GABLAM results for (gablamdis) UPC generation [False]
    ptmlist=LIST    : List of PTM letters to add to alphabet for analysis and restrict PTM data []
    ptmdata=DSVFILE : File containing PTM data, including AccNum, ModType, ModPos, ModAA, ModCode

SLiMBuild: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMBuild Options I (Evolutionary Filtering): ###
    efilter=T/F     : Whether to use evolutionary filter [True]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=DSVFILE  : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    homcut=X        : Max number of homologues to allow (to reduce large multi-domain families) [0]
    newupc=PATH     : Look for alternative UPC file and calculate Significance using new clusters [None]

    ### SLiMBuild Options II (Input Masking): ###
    masking=T/F     : Master control switch to turn off all masking if False [True]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out (True=EM,DOMAIN,TRANSMEM) []
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M (can be useful if termini=T) [True]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []
    qregion=X,Y     : Mask all but the region of the query from (and including) residue X to residue Y [0,-1]

    ### SLiMBuild Options III (Basic Motif Construction): ###
    termini=T/F     : Whether to add termini characters (^ & $) to search sequences [True]
    minwild=X       : Minimum number of consecutive wildcard positions to allow [0]
    maxwild=X       : Maximum number of consecutive wildcard positions to allow [2]
    slimlen=X       : Maximum length of SLiMs to return (no. non-wildcard positions) [5]
    minocc=X        : Minimum number of unrelated occurrences for returned SLiMs. (Proportion of UP if < 1) [0.05]
    absmin=X        : Used if minocc<1 to define absolute min. UP occ [3]
    alphahelix=T/F  : Special i, i+3/4, i+7 motif discovery [False]
    fixlen=T/F      : If true, will use maxwild and slimlen to define a fixed total motif length [False]
    palindrome=T/F  : Special DNA mode that will search for palindromic sequences only [False]

    ### SLiMBuild Options IV (Ambiguity): ###
    ambiguity=T/F   : (preamb=T/F) Whether to search for ambiguous motifs during motif discovery [True]
    ambocc=X        : Min. UP occurrence for subvariants of ambiguous motifs (minocc if 0 or > minocc) [0.05]
    absminamb=X     : Used if ambocc<1 to define absolute min. UP occ [2]
    equiv=LIST      : List (or file) of TEIRESIAS-style ambiguities to use [AGS,ILMVF,FYW,FYH,KRH,DE,ST]
    wildvar=T/F     : Whether to allow variable length wildcards [True]
    combamb=T/F     : Whether to search for combined amino acid degeneracy and variable wildcards [False]

    ### SLiMBuild Options V (Advanced Motif Filtering): ###
    altupc=PATH     : Look for alternative UPC file and filter based on minocc [None]
    musthave=LIST   : Returned motifs must contain one or more of the AAs in LIST (reduces search space) []
    query=LIST      : Return only SLiMs that occur in 1+ Query sequences (Name/AccNum) []
    focus=FILE      : FILE containing focal groups for SLiM return (see Manual for details) [None]
    focusocc=X      : Motif must appear in X+ focus groups (0 = all) [0]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    
SLiMChance: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    cloudfix=T/F    : Restrict output to clouds with 1+ fixed motif (recommended) [False]
    slimchance=T/F  : Execute main SLiMFinder probability method and outputs [True]
    sigprime=T/F    : Calculate more precise (but more computationally intensive) statistical model [False]
    sigv=T/F        : Use the more precise (but more computationally intensive) fix to mean UPC probability [False]
    dimfreq=T/F     : Whether to use dimer masking pattern to adjust number of possible sites for motif [True]
    probcut=X       : Probability cut-off for returned motifs (sigcut=X also recognised) [0.1]
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [True]
    aafreq=AAFILE   : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) (!!!Experimental!!!) [None]
    negatives=SEQFILE : Multiply raw probabilities by under-representation in FILE (!!!Experimental!!!) [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=T/F      : Whether to upweight for multiple occurrences in same sequence (heuristic) [False]
    probscore=X     : Score to be used for probability cut-off and ranking (Prob/Sig/S/R) [Sig]

Advanced: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Advanced Output Options I (Output data): ###
    clouds=X        : Identifies motif "clouds" which overlap at 2+ positions in X+ sequences (0=minocc / -1=off) [2]
    runid=X         : Run ID for resfile (allows multiple runs on same data) [DATE]
    logmask=T/F     : Whether to log the masking of individual sequences [True]
    slimcheck=MOTIFS : Motif file/list to add to resfile output []

    ### Advanced Output Options II (Output formats): ###
    teiresias=T/F   : Replace TEIRESIAS, making *.out and *.mask.fasta files [False]
    slimdisc=T/F    : Emulate SLiMDisc output format (*.rank & *.dat.rank + TEIRESIAS *.out & *.fasta) [False]
    extras=X        : Whether to generate additional output files (alignments etc.) [1]
                        --1 = No output beyond main results file
                        - 0 = Generate occurrence file
                        - 1 = Generate occurrence file, alignments and cloud file
                        - 2 = Generate all additional SLiMFinder outputs
                        - 3 = Generate SLiMDisc emulation too (equiv extras=2 slimdisc=T)
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle
                        - 2 = Delete all bar *.upc (pickle added to tar)
                        - 3 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)

    ### Advanced Output Options III (Additional Motif Filtering): ###
    topranks=X      : Will only output top X motifs meeting probcut [1000]
    oldscores=T/F   : Whether to also output old SLiMDisc score (S) and SLiMPickings score (R) [False]
    allsig=T/F      : Whether to also output all SLiMChance combinations (Sig/SigV/SigPrime/SigPrimeV) [False]
    minic=X         : Minimum information content for returned motifs [2.1]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *

    ### Additional Functions I (MotifSeq): ###
    motifseq=LIST   : Outputs fasta files for a list of X:Y, where X is the pattern and Y is the output file []
    slimbuild=T/F   : Whether to build motifs with SLiMBuild. (For combination with motifseq only.) [True]

    ### Additional Functions II (Randomised datasets): ###
    randomise=T/F   : Randomise UPC within batch files and output new datasets [False]
    randir=PATH     : Output path for creation of randomised datasets [Random/]
    randbase=X      : Base for random dataset name [rand]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

### ~~~~ Module slimmaker ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimmaker.py] ~~~~ ###

Module:       SLiMMaker
Description:  SLiM generator from aligned peptide sequences
Version:      1.7.0
Last Edit:    01/02/17
Citation:     Palopoli N, Lythgow KT & Edwards RJ. Bioinformatics 2015; doi: 10.1093/bioinformatics/btv155 [PMID: 25792551]
Webserver:    http://www.slimsuite.unsw.edu.au/servers/slimmaker.php
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    This program has a fairly simple function of reading in a set of sequences and generating a regular expression motif
    from them. It is designed with protein sequences in mind but should work for DNA sequences too. Input sequences can
    be in fasta format or just plain text (with no sequence headers) and should be aligned already. If varlength=F then
    gapped positions will be ignored (treated as Xs) and variable length wildcards are not returned. If varlength=T, any
    gapped positions will be assessed based on the ungapped peptides at that position and a variable length inserted.
    This variable-length position may be a wildcard or it may be a defined position if there is sufficient signal in the
    peptides with amino acids at that position.

    SLiMMaker considers each column of the input in turn and compresses it into a regular expression element according to
    some simple rules, screening out rare amino acids and converting particularly degenerate positions into wildcards.
    Each amino acid in the column that occurs at least X times (as defined by minseq=X) is considered for the regular
    expression definition for that position. The full set of amino acids meeting this criterion is then assessed for
    whether to keep it as a defined position, or convert into a wildcard. First, if the number of different amino acids
    meeting this criterion is zero or above a second threshold (maxaa=X), the position is defined as a wildcard. Second,
    the proportion of input sequences matching the amino acid set is compared to a minimum frequency criterion
    (minfreq=X). Failing to meet this minimum frequency will again result in a wildcard. Otherwise, the amino acid set is
    added to the SLiM definition as either a fixed position (if only one amino acid met the `minseq` criterion) or as a
    degenerate position. Finally, leading and trailing wildcards are removed.

    By default, each defined position in a motif will contain amino acids that (a) occur in at least three sequences
    each, (b) have a combined frequency of >=75%, and (c) have 5 or fewer different amino acids (that occur in 3+
    sequences). The same `minseq=X` threshold is also used to determine whether flexible length *defined* positions are
    generated (if `varlength=T`), i.e. to have a flexible-length non-wildcard position, at least minseq sequences must
    have a gap at that position. This does not apply to flexible-length wildcards.

    Note. Unless the "iterate" function is used, the final motif only contains defined positions that match a given 
    frequency of the input (75% by default). Because positions are considered independently, however, the final motif
    might occur in fewer than 75% of the input sequences. SLiMSearch can be used to check the occurrence stats.

    Version 1.5.0 incorporates a new peptide alignment mode to deal with unaligned peptides. This is controlled by the
    `peptalign=T/F/X` option, which is set to True by default. If given a regular expression, this will be used to guide
    the alignment. Otherwise, the longest peptides will be used as a guide and the minimum number of gaps added to
    shorter peptides. PeptCluster peptide distance measures are used to assess different variants, starting with simple
    sequence identity, then amino acid properties (if ties) and finally PAM distances. One of the latter can be set as
    the priority using `peptdis=X`. Peptide alignment assumes that peptides have termini (^ & $) or flanking wildcards
    added. If not, set `termini=F`.

    Version 1.6.0 added the option to incorporate amino acid equivalencies to extend motif sites beyond the top X% of
    amino acids. This works by identifying a degenerate set of amino acids as normal using `minseq=X` and then checking
    whether these form a subset of an equivalence group prior to the `minfreq=X` filter. If so, it will try extending the
    degenerate position to incorporate additional members of the equivalence group. For example, `IL` could incorporate
    additional `MVF` amino acids of an `FILMV` group. Only amino acids represented in the peptides will be added. Single
    amino acids will also be extended, e.g. `S` could be extended to `ST`. This mode is switched on with `extendaa=T`.
    The `equiv=LIST` option sets the equivalence groups.

    If two or more equivalence groups could be extended, the one with the most members will be chosen. If tied, the one
    with fewest possible amino acids (from `equiv=LIST`) will be chosen. If still tied, the first group in the list will
    take precedence.

Commandline:
    ### ~ SLiMMaker Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    peptides=LIST   : These can be entered as a list or a file. If a file, lines following '#' or '>' are ignored
    maxlen=INT      : Maximum length for peptide [50]
    peptalign=T/F/X : Align peptides. Will use as guide regular expression, else T/True for regex-free alignment. [True]
    minseq=X        : Min. no. of sequences for an aa to be in [3]
    minfreq=X       : Min. combined freq of accepted aa to avoid wildcard [0.75]
    maxaa=X         : Max. no. different amino acids for one position [5]
    ignore=X        : Amino acid(s) to ignore. (If nucleotide, would be N-) ['X-']
    dna=T/F         : Whether "peptides" are actually DNA fragments [False]
    iterate=T/F     : Whether to perform iterative SLiMMaker, re-running on matched peptides with each iteration [False]
    varlength=T/F   : Whether to identifies gaps in aligned peptides and generate variable length motif [True]
    extendaa=T/F    : Whether to extend ambiguous aa using equivalence list [False]
    equiv=LIST      : List (or file) of TEIRESIAS-style ambiguities to use [AGS,ILMVF,FYW,KRH,DE,ST]

See also rje.py generic commandline options.

### ~~~ Module slimmutant ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimmutant.py] ~~~ ###

Module:       SLiMMutant
Description:  Short Linear Motif Mutation Analysis Tool
Version:      1.3
Last Edit:    16/09/14
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMMutant is a Short Linear Motif Mutation Analysis Tool, designed to identify and assess mutations that create
    and/or destroy SLiMs. There are three main run modes:

    - Mode 1. Generating mutant datasets for analysis [`generate=T`]

    The main input is: (1) a file of protein sequence mutations [`mutfile=FILE`] in a delimited text format with aa
    substitution [`mutfield=X`] and protein accession number [`protfield=X`] data; (2) a corresponding sequence file
    [`seqin=FILE`]; (3) a file of SLiMs to analyse [`motifs=FILE`]. This will process the data and generate two sequence
    files: `*.wildtype.fas` and `*.mutant.fas`. These files will be named after the input `mutfile` unless `basefile=X`
    is used.

    - Mode 2. Run SLiMProb on datasets [`slimprob=T`]

    This will run SLiMProb on the two datasets, once per `*.ini` file given by `slimini=LIST`. These runs should have
    distinct `runid=X` settings. If no `*.ini` files are given, as single run will be made using commandline settings.

    - Mode 3. Compile results of SLiMProb runs. [`analyse=T`]

    This will compare the `*.wildtype.fas` and `*.mutant.fas` results from the `*.occ.csv` file produced by SLiMProb.
    All mutations analysed will be identified from `*.mutant.fas`. SLiM occurrences are then matched up between wildtype
    and mutant versions of the same sequence. If none of the mutations have effected the SLiM prediction, then the
    wildtype and all mutant sequences will return the motif. If, on the other hand, mutations have created/destroyed
    motifs, occurrences will be missing from the wildtype and/or 1+ mutant sequences. All unaffected SLiM instances are
    first removed and altered SLiM instances  output to `*.MutOcc.csv`. Differences between mutants and wildtypes are
    calculated for each `RunID`-`Motif` combination and summary results output to `*.Mut_vs_WT.csv`. If `motlist=LIST` is
    given, analysis is restricted to a subset of motifs.

    Unless `basefile=FILE` is given, output files will be named after `mutfile=FILE` but output into the current run
    directory. If running in batch mode, basefile cannot be used.

    NOTE: SLiMMutant is still in development and has not been thoroughly tested or benchmarked.

Commandline:
    ### ~ SEQUENCE GENERATION METHODS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    generate=T/F	: Whether to run sequence generation pipeline [False]
    mutfile=FILE	: Delimited text file with sequence mutation info. Sets basefile. []
    mutfield=X		: Field in mutfile corresponding to AA subsitution data ['AAChange']
    protfield=X		: Field in mutfile corresponding to protein accession number ['Uniprot']
    splitfield=X    : Field in mutfile to split data on (saved as basefile.X.tdt) []
    seqin=FILE		: Input file with protein sequences []
    motifs=FILE		: Input file of SLiMs []
    motlist=LIST	: List of input SLiMs to restrict analysis to []
    mutflanks=X		: Generate for casemask=Upper of X aa flanking mutation (None if < 1) [0]
    minmutant=X     : Minimum number of mutants for output [100]
    maxmutant=X     : Maximum number of mutants for output [100000]

    ### ~ SLiMProb Run Methods ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    slimprob=T/F	: Whether to run SLiMProb on *.wildtype.fas and *.mutant.fas (*=basefile) [False]
    slimini=LIST	: Lists of INI file with settings for SLiMProb run. Should include runid=X and resdir=PATH. []
    resdir=PATH   	: Location of output files. SLiMProb resdir should be in slimini [SLiMMutant/ (and SLiMProb/)]

    ### ~ SLiMProb Results Analysis ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    analyse=T/F		: Whether to analyse the results of a SLiMProb run [False]
    resfile=FILE   	: Main SLiMProb results table (*.csv and *.occ.csv) [slimprob.csv]
    runid=X			: Limit analysis to SLiMProb RunID (blank = analyse all) []
    buildpath=PATH 	: Alternative path to look for existing intermediate files (e.g. *.upc) [SLiMProb/]

    ### ~ SLiMProb PPI Analysis ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    slimppi=T/F         : Whether to perform SLiMPPI analysis (will set analyse=T) [False]
    sourcepath=PATH/    : Will look in this directory for input files if not found ['SourceData/']
    sourcedate=DATE		: Source file date (YYYY-MM-DD) to preferentially use [None]
    ppisource=X			: Source of PPI data. (HINT/FILE) FILE needs 'Hub' and 'SpokeUni' fields. ['HINT']
    dmifile=FILE        : Delimited text file containing domain-motif interaction data ['elm_interaction_domains.tsv']

    ### ~ Batch running ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    batch=FILELIST  : List of mutfiles to run in batch mode. Wildcards allowed. []

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~ Module slimparser ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimparser.py] ~~~ ###

Module:       SLiMParser
Description:  SLiMSuite REST output parsing tool.
Version:      0.5.0
Last Edit:    27/09/17
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for parsing the full REST output of a program into a couple of dictionaries, with options to output
    the data to files or convert to/from json format.

    If `restin=FILE` is a URL, this will be interpreted as a REST command for API access. Use with `rest=X` and
    `pureapi=T` to print the output to STDOUT once the run is complete. Use in conjunction with `v=-1` to avoid
    additional STDOUT output and `silent=T` to avoid log generation.

    REST URLs can include files to be uploaded. These must be prefixed with `file:`, e.g. `&seqin=file:input.fas`. If the
    specified file exists then the content will replace the file name in the REST call.

Commandline:
    ### ~ INPUT/OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    restin=FILE     # Full REST output file, REST jobid (number), or full REST call URL (http://...) []
    password=X      # Optional password for REST jobid retrieval [None]
    restout=T/F     # Whether to save extracted elements to individual files [False]
    rest=X          # Return text for just the REST output element X []
    restbase=X      # Basefile for parsed REST output that lacks defined filename [jobid]
    restoutdir=PATH # Path for output of parsed REST output [./]
    pureapi=T/F     # Whether to return the text returned from the REST call. Needs rest=X. [False]
    ### ~ REST API Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    resturl=X       # URL of rest server ['http://rest.slimsuite.unsw.edu.au/']
    refresh=X       # Initial number of seconds for between checks for job status (will double) [5]
    maxrefresh=X    # Maximum number of seconds for incrementing check refresh [600]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~~~ Module slimprob ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimprob.py] ~~~~~ ###

Program:      SLiMProb
Description:  Short Linear Motif Probability tool
Version:      2.5.1
Last Edit:    06/09/17
Citation:     Davey, Haslam, Shields & Edwards (2010), Lecture Notes in Bioinformatics 6282: 50-61. 
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMProb is a tool for finding pre-defined SLiMs (Short Linear Motifs) in a protein sequence database. SLiMProb
    can make use of corrections for evolutionary relationships and a variation of the SLiMChance alogrithm from
    SLiMFinder to assess motifs for statistical over- and under-representation. SLiMProb is replace for the original
    SLiMSearch, which itself was a replacement for PRESTO. The basic architecture is the same but it was felt that having
    two different "SLiMSearch" servers was confusing. 

    Benefits of SLiMProb that make it more useful than a lot of existing tools include:
    * searching with mismatches rather than restricting hits to perfect matches.
    * optional equivalency files for searching with specific allowed mismatched (e.g. charge conservation)
    * generation or reading of alignment files from which to calculate conservation statistics for motif occurrences.
    * additional statistics, including protein disorder, surface accessibility and hydrophobicity predictions
    * recognition of "n of m" motif elements in the form <X:n:m>, where X is one or more amino acids that must occur n+
    times across which m positions. E.g. <IL:3:5> must have 3+ Is and/or Ls in a 5aa stretch.

    Main output for SLiMProb is a delimited file of motif/peptide occurrences but the motifaln=T and proteinaln=T also
    allow output of alignments of motifs and their occurrences. The primary outputs are named *.occ.csv for the occurrence
    data and *.csv for the summary data for each motif/dataset pair. (This is a change since SLiMSearch.)

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ###
    motifs=FILE     : File of input motifs/peptides (also motif=X) [None]
                      Single line per motif format = 'Name Sequence #Comments' (Comments are optional and ignored)
                      Alternative formats include fasta, SLiMDisc output and raw motif lists.
    seqin=SEQFILE   : Sequence file to search. Over-rules batch=FILE and uniprotid=LIST [None]
    batch=FILELIST  : List of files to search, wildcards allowed. (Over-ruled by seqin=FILE.) [*.dat,*.fas]
    uniprotid=LIST  : Extract IDs/AccNums in list from Uniprot into BASEFILE.dat and use as seqin=FILE. []
    maxseq=X        : Maximum number of sequences to process [0]
    maxsize=X       : Maximum dataset size to process in AA (or NT) [100,000]
    maxocc=X        : Filter out Motifs with more than maximum number of occurrences [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main SLiMProb results table (*.csv and *.occ.csv) [slimprob.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [SLiMProb/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMProb/]
    force=T/F       : Force re-running of BLAST, UPC generation and search [False]
    dna=T/F         : Whether the sequences files are DNA rather than protein [False]
    alphabet=LIST   : List of characters to include in search (e.g. AAs or NTs) [default AA or NT codes]
    megaslim=FILE   : Make/use precomputed results for a proteome (FILE) in fasta format [None]
    megablam=T/F    : Whether to create and use all-by-all GABLAM results for (gablamdis) UPC generation [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SearchDB Options I: Input Protein Sequence Masking ###
    masking=T/F     : Master control switch to turn off all masking if False [False]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out (True=EM,DOMAIN,TRANSMEM) []
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [None]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M [False]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  []
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []

    ### SearchDB Options II: Evolutionary Filtering  ###
    efilter=T/F     : Whether to use evolutionary filter [True]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    occupc=T/F      : Whether to output the UPC ID number in the occurrence output file [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMChance Options ###
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [True]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) [None]
    negatives=FILE  : Multiply raw probabilities by under-representation in FILE [None]
    background=FILE : Use observed support in background file for over-representation calculations [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=X        : Restrict to sequences with X+ occurrences (adjust for high frequency SLiMs) [1]
    mergesplits=T/F : Whether to merge split SLiMs for recalculating statistics. (Assumes unique RunIDs) [True]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Output Options ###
    extras=X        : Whether to generate additional output files (alignments etc.) [2]
                        - 0 = No output beyond main results file
                        - 1 = Saved masked input sequences [*.masked.fas]
                        - 2 = Generate additional outputs (alignments etc.)
                        - 3 = Additional distance matrices for input sequences
    pickle=T/F      : Whether to save/use pickles [True]
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle files
                        - 2 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module slimsearch ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimsearch.py] ~~~ ###

Program:      SLiMSearch
Description:  Short Linear Motif Search tool
Version:      1.7.1
Last Edit:    03/12/15
Citation:     Davey, Haslam, Shields & Edwards (2010), Lecture Notes in Bioinformatics 6282: 50-61. 
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMSearch is a tool for finding pre-defined SLiMs (Short Linear Motifs) in a protein sequence database. SLiMSearch
    can make use of corrections for evolutionary relationships and a variation of the SLiMChance alogrithm from
    SLiMFinder to assess motifs for statistical over- and under-representation. SLiMSearch is a replacement for PRESTO
    and uses many of the same underlying modules.

    Benefits of SLiMSearch that make it more useful than a lot of existing tools include:
    * searching with mismatches rather than restricting hits to perfect matches.
    * optional equivalency files for searching with specific allowed mismatched (e.g. charge conservation)
    * generation or reading of alignment files from which to calculate conservation statistics for motif occurrences.
    * additional statistics, including protein disorder, surface accessibility and hydrophobicity predictions
    * recognition of "n of m" motif elements in the form <X:n:m>, where X is one or more amino acids that must occur n+
    times across which m positions. E.g. <IL:3:5> must have 3+ Is and/or Ls in a 5aa stretch.

    Main output for SLiMSearch is a delimited file of motif/peptide occurrences but the motifaln=T and proteinaln=T also
    allow output of alignments of motifs and their occurrences. The primary outputs are named *.csv for the occurrence
    data and *.summary.csv for the summary data for each motif/dataset pair. 
    
    NOTE: SLiMSearch has now been largely superseded by SLiMProb for motif statistics.

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ###
    motifs=FILE     : File of input motifs/peptides [None]
                      Single line per motif format = 'Name Sequence #Comments' (Comments are optional and ignored)
                      Alternative formats include fasta, SLiMDisc output and raw motif lists.
    seqin=FILE      : Sequence file to search [None]
    batch=LIST      : List of sequence files for batch input (wildcard * permitted) []
    maxseq=X        : Maximum number of sequences to process [0]
    maxsize=X       : Maximum dataset size to process in AA (or NT) [100,000]
    maxocc=X        : Filter out Motifs with more than maximum number of occurrences [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main SLiMSearch results table [slimsearch.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [SLiMSearch/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMSearch/]
    force=T/F       : Force re-running of BLAST, UPC generation and search [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SearchDB Options I: Input Protein Sequence Masking ###
    masking=T/F     : Master control switch to turn off all masking if False [False]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out [EM,DOMAIN,TRANSMEM]
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M [False]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []

    ### SearchDB Options II: Evolutionary Filtering  ###
    efilter=T/F     : Whether to use evolutionary filter [False]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    occupc=T/F      : Whether to output the UPC ID number in the occurrence output file [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMChance Options ###
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [True]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) [None]
    negatives=FILE  : Multiply raw probabilities by under-representation in FILE [None]
    background=FILE : Use observed support in background file for over-representation calculations [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=X        : Restrict to sequences with X+ occurrences (adjust for high frequency SLiMs) [1]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Output Options ###
    extras=X        : Whether to generate additional output files (alignments etc.) [1]
                        - 0 = No output beyond main results file
                        - 1 = Generate additional outputs (alignments etc.)
    pickle=T/F      : Whether to save/use pickles [True]
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle files
                        - 2 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

### ~~~~ Module slimsuite ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/slimsuite.py] ~~~~ ###

Module:       SLiMSuite
Description:  Short Linear Motif analysis Suite
Version:      1.7.0
Last Edit:    16/02/17
Citation:     Edwards RJ & Palopoli N (2015): Methods Mol Biol. 1268:89-141. [PMID: 25555723]
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMSuite is designed to be a front end for the SLiMSuite set of sequence analysis tools. The relevant tool is given
    by the first system command, or selected using `prog=X` (or `program=X`). As much as possible, SLiMSuite will emulate
    running that tool from the commandline, adding any matching `X.ini` file to the default commandline options read in
    (*before* settings read from slimsuite.ini itself). By default, the SLiMCore tool will be called
    (libraries/rje_slimcore.py) and read in commands from slimcore.ini.

    Help for the selected tool can be accessed using the `help=T` option. Note that `-h`, `-help` or `help` alone will
    trigger the SLiMSuite help (this!). As `-help` or `help` will also set `help=T`, these commands will trigger both the
    SLiMSuite help and the selected program help (unless over-ruled by `help=F`). An explicit `help=T` command will only
    trigger the selected program help.

    Running SLiMSuite should also try importing all the main SLiMSuite modules, testing for download errors etc.

SLiMSuite tools:
    The list of tools recognised by `prog=X` will be added here as the relevant code is added:
    - SLiMCore = rje_slimcore.SLiMCore. SLiMSuite core module with MegaSLiM and UPC functions.
    - SLiMFarmer = slimfarmer.SLiMFarmer. SLiMSuite job forking/HPC controller.
    - QSLiMFinder = qslimfinder.QSLiMFinder. Query-based Short Linear Motif Finder - de novo SLiM prediction.
    - SLiMFinder = slimfinder.SLiMFinder. Short Linear Motif Finder - de novo SLiM prediction.
    - SLiMList = rje_slimlist.SLiMList. Short Linear Motif manipulation/filtering module.
    - SLiMProb = slimprob.SLiMProb. Short Linear Motif Probability - known SLiM prediction.
    - SLiMBench = slimbench.SLiMBench. SLiM discovery benchmarking module.
    - SLiMMaker = slimmaker.SLiMMaker. Simple SLiM generation from aligned peptide sequences.
    - PeptCluster = peptcluster.PeptCluster. Peptide alignment, pairwise distance and clustering tool.

Example use to run SLiMFinder:
    python SLiMSuitePATH/tools/slimsuite.py slimfinder

Please also see the SeqSuite documentation for additional utilities, which can be run from SLiMSuite or SeqSuite.

Commandline:
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    prog=X      # Identifies the tool to be used. Will load defaults from X.ini (before slimsuite.ini) [help]
    help=T/F    # Return the help documentation for program X. [False]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~~ Module smrtscape ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/smrtscape.py] ~~~~ ###

Module:       SMRTSCAPE
Description:  SMRT Subread Coverage & Assembly Parameter Estimator
Version:      2.2.1
Last Edit:    14/07/17
Copyright (C) 2017  Richard J. Edwards - See source code for GNU License Notice

Summary:
    ### ~ Function ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    `SMRTSCAPE` (SMRT Subread Coverage & Assembly Parameter Estimator) is a tool for analysis of PacBio raw sequencing
    data to assist the design and execution of PacBio genomics projects. It has a number of functions concerned with
    predicting and/or assessing the quantity and quality of useable data produced:

    1. **Genome Coverage (`coverage=T`).** This method tries to predict genome coverage and accuracy for different depths of
    PacBio sequencing. This is useful for estimating genome coverage and/or required numbers of SMRT cells from predicted
    read outputs or emprical (average) SMRT cell data if the `BASEFILE.unique.tdt` output (generated by `summary=T`, below) is
    found. NOTE: Default settings for SMRT cell output are not reliable and you should speak to your sequencing provider
    for their up-to-date figures. By default, output for this mode is incremented by `XCoverage` but this can be switched
    to numbers of SMRT cells with `bysmrt=T`.

    SMRTSCAPE `coverage=T` mode can be run from the EdwardsLab server at:
    http://www.slimsuite.unsw.edu.au/servers/pacbio.php

    2. **Summarise subreads (`summarise=T`).** This function summarises subread data from a given `seqin=FILE` fasta
    file, or a set of subread fasta files given with `batch=FILELIST` (or listed in `*.fofn`). This produces sequence
    summary data (read lengths, N50 etc.) for each sequence file, SMRT cell and the combined dataset (`*.summary.tdt`).
    In addition, tables are generated that summarise each read individually, which can then be used for further read
    filtering or calculations. Summaries are produced for _all_ data (`*.zmw.tdt`) and just the **Unique** subread data,
    which is the _longest_ read from each ZMW (`*.unique.tdt`). FALCON only uses unique read data, and so it is these
    data that are used for the rest of SMRTSCAPE functions. A summary of **Read Quality (RQ)** data is also output
    (`*.rq.tdt`).

    3. **Calculate length cutoffs (`calculate=T`).** Calculates length cutoffs for different XCoverage and RQ
    combinations from subread summary data. Generates `*.cutoffs.tdt`.

    4. **Optimise (`optimise=T`).** This function attempts to generate predicted optimum assembly settings from the
    `summary=T` and `calculate=T` table data. NOTE: In `V1.x`, this option was `parameters=T`.

    5. **Filter subreads (`readfilter=T`).** This filters *unique* subreads into a new fasta file (`*.LXXXRQXXX.fasta`)
    based on min. read quality (`rqfilter=X`) and min. read length (`lenfilter=X`). NOTE: These filters are not
    available in FALCON, so sequence input must be pre-filtered in this way.

    6. **Preassembly fragmentation analysis (`preassembly=FILE`).** Processes a Preassembly fasta file to assess/correct
    over-fragmentation. Corrected preassembly reads are output to `*.smrtscape.fas` and summary data output to
    `*.fragment.tdt`.

    7. **Feature coverage (`ftxcov=INTLIST`).** Calculates full-length coverage for a list of feature lengths, as well as
    their probability of detection (in raw subread data) if present at different population frequencies
    (`ftxfreq=NUMLIST`).  If seqin=FILE is given, this will be used directly unless summarise=T, calculate=T or
    optimise=T. Otherwise, unique reads from (`*.unique.tdt`) will be used.

    These are explored in more detail in the **Details** section.


    ### ~ Retired Functions ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    NOTE: The following functions/settings have been retired in `V2.x` as HGAP3-specific options. To use, please use
    `SMRTSCAPE_V1` or contact the author if you wish to have them reinstated for updated versions of SMRTPipeline/SMRTLink:

    parseparam=FILES: Parse parameter settings from 1+ assembly runs []
    paramlist=LIST  : List of parameters to retain for parseparam output (file or comma separated, blank=all) []


    ### ~ Input/Output ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

    Main input for `SMRTSCAPE` is a set of subreads fasta files (`seqin=FILE` or `batch=FILELIST`). This is not required
    for `coverage=T`. Input for Preassembly Fragmentation analysis is a single preassembly fasta file, given with
    `preassembly=FILE`.

    Main outputs are named using `basefile=X` as the file name root. If `basefile=X` is not set (or =`''`/`None`),
    `seqin=FILE` will set the basefile name, trimming the file extension. If `seqin=FILE` is not given `preassembly=FILE`
    will be used. If neither `seqin` nor `preassembly` are given and `batch=FILELIST` points to a `*.fofn` "file of file
    names", this file will be used to set `basefile`. Otherwise, `basefile=smrtscape`.

    Re-running a particular mode will regenerate the relevant output (with options to back up unless `backups=F`) but
    existing data from previous stages will be read and reused if found. If `force=T` then earlier stages other than
    `summarise=T` output (e.g. `*.unique.tdt`). These will also be regenerated if `fullforce=T`. (The only `summarise=T`
    output that is dependent on input parameters are `XCoverage` fields, which use `genomesize=INT`, and the `Xerr`
    fields that also use `targeterr=X`.)

    All modes will produce a `*.log` file. Other outputs produced depend on the run mode selected:

    1. `coverage=T`:
        * `*.coverage.tdt` = Predict genome coverage and accuracy for different depths of PacBio sequencing.

    2. `summarise=T`:
        * `*.fofn` = File of input subread filenames.
        * `*.summary.tdt` = Sequence summary data (read lengths, N50 etc.) for each sequence file, SMRT cell and the
                            combined dataset.
        * `*.zmw.tdt` = Individual summary data for all subreads.
        * `*.unique.tdt` = Individual summary data for **Unique** subreads (longest per ZMW).
        * `*.rq.tdt`= A summary of **Read Quality (RQ)** data.

    3. `calculate=T`:
        * `*.cutoffs.tdt` = Table of different read length and RQ cutoffs and the predicted depth and quality of
                            resulting assemblies.
        * `*.accuracy.tdt` = Summary table of predicted accuracies at different RQ/Xdepth combinations.

    4. `optimise=T`: No additional output. Optimal parameter suggestions are output in log file.

    5. `readfilter=T`:
        * `*.LXXXRQXXX.fasta` = reads filteres on min. read quality (`rqfilter=X` = `RQXXX`) and min. read length
                                (`lenfilter=X` = `LXXX`).
        * `*.LXXXRQXXX.fasta.index` = index for fasta file, used by `SeqList`.

    6. `ftxcov=INTLIST`):
        * `smrtscape_ftfreq.ftxcov.tdt` = Calculated full-length coverage for list of feature lengths, as well as their
                                          probability of detection (in raw subread data) if present at different
                                          population frequencies (`ftxfreq=NUMLIST`).

    7. `preassembly=FILE`:
        * `*.smrtscape.fas` = fragmentation-corrected preassembly reads.
        * `*.fragment.tdt` =  summary fragmentation data.

    Details are provided in the **Details** Section.


Details:
    ### ~ How SMRTSCAPE works ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

    `SMRTSCAPE` is built on two ways of calculating "`X`" depth of coverage: (1) a simple calculation of mean X depth;
    (2) Empirical modelling of read depth distributions. These are combined as appropriate with estimations of the
    required depth of coverage to achieve the desired target genome accuracy. These calculations are described below.
    See descriptions of individual `SMRTSCAPE` functions for more information on how they are used in a given setting.

    As a result, `SMRTSCAPE` behaviour is largely controlled by four parameters:

    * `genomesize=INT`  : Genome size (bp) [0]
    * `targetcov=PERC`  : Target percentage coverage for final genome [99.999]
    * `targeterr=NUM`   : Target errors per base for preassembly [1/genome size]
    * `targetxcov=INT`  : Target 100% X Coverage for pre-assembly (e.g. error-corrected seed reads) [3]

    **NOTE:** If you are getting messages about insufficient data, you might want to try reducing the target error rate
    (`targeterr=NUM`) and/or the target "complete" coverage (`targetcov=PERC`).

    **WARNING:** Changing these settings between different runs with the same `basefile=X` setting may give some unusual
    behaviour. It is safest to use a new `basefile=X` if changing any of these settings. (Future `SMRTSCAPE` versions may
    enforce this.) The exception should be the `*.unique.tdt` and `*.zmw.tdt` outputs, which should be robust to changes
    in these settings. (These files are the slowest to generate, and so copying/reusing them could be useful. See
    `summarise=T` documentation for details.

    #### Simple calculation of mean X depth:

    This is a pure prediction based on average data. This is quite simpy the total amount of sequence (bp) divided by the
    genome size, given by `genomesize=INT` and equates to the traditional `XCoverage` value for genome sequence.

    #### Empirical modelling of read depth distributions:

    The heart of SMRTSCAPE is an empirical calculation based modelling read distributions, assuming random sampling of
    reads from across the genome. This calculates the total (summed) read lengths required to generate the desired genome
    coverage (`targetcov=X`) at different `X` depths, *e.g.* what total Xdepth is required to cover 99.999% of the genome
    at 3X (or more). Note that, with the exception of preassembly-based calculations, the _square root_ of the
    `targetcov=X` value is used;  the assembly process involves *two* layers of genome coverage: (1) coverage of seed
    reads by anchor reads to generate the error-corrected preassembly; (2) coverage of the preassembly.

    These "`XCovLimit`" data are calculated by incrementing total summed read lengths in 100 kb increments (adjusted with
    `xsteplen=X`). At each incremment, `genomesize=INT` is used to calculate the total `X` coverage, which is the mean
    depth at any given position. The probability of the target `X` coverage (starting at `1X`) given the total X coverage
    is then calculated using a Poisson distribution. If this probability exceeds the target genome coverage
    (`targetcov=X`), the current summed length is set as the `XCovLimit` for `X` and the target `X` increased by 1. The
    total summed read length is then incremented by `xsteplen` and the process repeated until the summed length reaches
    the total length of all subreads of at least the size set by `minreadlen=X` (default 500 bp).

    #### Target Genome Accuracy:

    The final piece of the puzzle is the depth of coverage required to achieve a particular level of accuracy. By
    default, `SMRTSCAPE` aims at perfection, which is less than 1 error per genome. (NOTE: For large genomes, this will
    require an unrealistic depth of coverage.)

    Accuracy is based on a majority reads covering a particular base with the correct call, assuming random calls at the
    other positions (*i.e.* the correct bases have to exceed 33% of the incorrect positions). For a given depth of
    coverage *for a given base*, the majority cutoff `N` is first calculated. Assuming random base calls for errors, this
    must exceed 1/4 of the read depth (rounded up):

        `N = int(0.25X) + 2`

    Accuracy is then calculated as the probability of achieving `N` correct calls, given the depth `X` and the `RQ` (or
    `1 = ErrPerBase`) using a binomial distribution with an even (random) distribution of errors. When using mean SMRT
    cell outputs, the `errperbase=NUM` parameter is used for this calculation. When using empirical data, the lowest read
    quality (`RQ`) value is used. (Calculations are made for different `RQ` cutoffs.)

    **NOTE:** Read Quality (`RQ`) values are *accuracy* measures, whereas `errperbase=NUM` sets an error rate. These are
    simply related, where `Accuracy = 1.0 - ErrPerBase`.


Example runs:

    To be added.



Commandline:
    ### ~ General Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=FILE   : Sets the root filename for all output, including the log [smrtscape]
    genomesize=INT  : Genome size (bp) [0]
    targetcov=PERC  : Target percentage coverage for final genome [99.999]
    targeterr=NUM   : Target errors per base for preassembly [1/genome size]
    targetxcov=X    : Target 100% X Coverage for pre-assembly (e.g. error-corrected seed reads) [3]
    minanchorx=X    : Minimum X coverage for anchor (preassembly error-correcting) subreads [6]
    xmargin=X       : "Safety margin" inflation of desired minimum X coverage [1]
    xsteplen=X      : [Adv.] Size (bp) of increasing coverage steps for calculating required depths of coverage [1e5]
    force=T/F       : [Adv.] Whether to force regeneration of existing data, except `unique` and `zmw` tables [False]
    fullforce=T/F   : [Adv.] Whether to force regeneration of existing data including `unique` and `zmw` tables [False]
    ### ~ Genome Coverage Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    coverage=T/F    : Whether to generate coverage report [False]
    avread=X        : Average read length (bp) [20000]
    smrtreads=X     : Average assemble output of a SMRT cell [50000]
    smrtunits=X     : Units for smrtreads=X (reads/Gb/Mb) [reads]
    errperbase=X    : Error-rate per base [0.14]
    maxcov=X        : Maximum X coverage to calculate for coverage=T analysis [100]
    bysmrt=T/F      : Whether to output estimated coverage by SMRT cell rather than X coverage [False]
    xnlist=LIST     : Additional columns giving % sites with coverage >= Xn [`minanchorx`->`targetxcov`+1]
    ### ~ SubRead Summary Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    summarise=T/F   : Generate subread summary statistics including ZMW summary data [False]
    batch=FILELIST  : Batch input of multiple subread fasta files (wildcards allowed) if seqin=None [basefile.fofn]
    seqin=FILE      : Subread sequence file for analysis (over-rides batch command) [None]
    ### ~ Assembly Parameter Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    calculate=T/F   : Calculate X coverage and target X coverage for given seed, anchor + RQ combinations [False]
    optimise=T/F    : Whether to output predicted "best" set of optimised parameters [False]
    minreadlen=X    : Absolute minimum read length for calculations (use minlen=X to affect summary also) [500]
    mapefficiency=X : Efficiency of mapping anchor subreads onto seed reads for correction [0.8]
    rq=X,Y          : Minimum (X) and maximum (Y) values for read quality cutoffs [0.8,0.9]
    rqstep=X        : Size of RQ jumps for calculation (min 0.001) [0.01]
    calcx=NUMLIST   : Add calculate entries for given raw Xdepths [25,30]
    calclen=INTLIST : Add calculate entries for given read length cutoffs [12000,15000,18000]
    ### ~ Subread Filtering Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    readfilter=T/F  : Output filtered subreads into a new fasta file *.LXXXRQXXX.fasta
    rqfilter=X      : Min read quality for filtered subreads [0.85]
    lenfilter=X     : Min read length for filtered subreads [5000]
    ### ~ Feature Coverage Calculation Option ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    ftxcov=INTLIST  : List of feature lengths for which to predict full-length coverage []
    ftxfreq=NUMLIST : List of feature frequencies for which to calculate probability of detection [0.01,0.05,1.0]
    ### ~ Preassembly Fragmentation analysis Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    preassembly=FILE: Preassembly fasta file for which to assess/correct over-fragmentation [None]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~ Module smrtscape_V1 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/smrtscape_V1.py] ~~~ ###

Module:       SMRTSCAPE_V1
Description:  SMRT Subread Coverage & Assembly Parameter Estimator
Version:      1.10.1
Last Edit:    26/05/16
Copyright (C) 2015  Richard J. Edwards - See source code for GNU License Notice

Function:
    SMRTSCAPE (SMRT Subread Coverage & Assembly Parameter Estimator) is tool in development as part of our PacBio
    sequencing projects for predicting and/or assessing the quantity and quality of useable data required/produced for
    HGAP3 de novo whole genome assembly. The current documentation is below. Some tutorials will be developed in the
    future - in the meantime, please get in touch if you want to use it and anything isn't clear.

    The main functions of `SMRTSCAPE` are:

    1. Estimate Genome Coverage and required numbers of SMRT cells given predicted read outputs. NOTE: Default settings
    for SMRT cell output are not reliable and you should speak to your sequencing provider for their up-to-date figures.

    2. Summarise the amount of sequence data obtained from one or more SMRT cells, including unique coverage (one read
    per ZMW).

    3. Calculate predicted coverage from subread data for difference length and quality cutoffs.

    4. Predict HGAP3 length and quality settings to achieve a given coverage and accuracy.

	SMRTSCAPE `coverage=T` mode can be run from the EdwardsLab server at:
	<http://www.slimsuite.unsw.edu.au/servers/pacbio.php>

	NOTE: SMRTSCAPE Version 1 has been frozen at V1.10.1 (with the exception of bug fixes) and future development will
	be of SMRTSCAPE Version 2.x onwards. This is being reworked for FALCON assemblies.

Commandline:
    ### ~ General Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    genomesize=X    : Genome size (bp) [0]
    ### ~ Genome Coverage Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    coverage=T/F    : Whether to generate coverage report [False]
    avread=X        : Average read length (bp) [20000]
    smrtreads=X     : Average assemble output of a SMRT cell [50000]
    smrtunits=X     : Units for smrtreads=X (reads/Gb/Mb) [reads]
    errperbase=X    : Error-rate per base [0.14]
    maxcov=X        : Maximum X coverage to calculate [100]
    bysmrt=T/F      : Whether to output estimated  coverage by SMRT cell rather than X coverage [False]
    xnlist=LIST     : Additional columns giving % sites with coverage >= Xn [1+`minanchorx`->`targetxcov`+`minanchorx`]
    ### ~ SubRead Summary Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    summarise=T/F   : Generate subread summary statistics including ZMW summary data [False]
    seqin=FILE      : Subread sequence file for analysis [None]
    batch=FILELIST  : Batch input of multiple subread fasta files (wildcards allowed) if seqin=None []
    targetcov=X     : Target percentage coverage for final genome [99.999]
    targeterr=X     : Target errors per base for preassembly [1/genome size]
    calculate=T/F   : Calculate X coverage and target X coverage for given seed, anchor + RQ combinations [False]
    minanchorx=X    : Minimum X coverage for anchor subreads [6]
    minreadlen=X    : Absolute minimum read length for calculations (use minlen=X to affect summary also) [500]
    rq=X,Y          : Minimum (X) and maximum (Y) values for read quality cutoffs [0.8,0.9]
    rqstep=X        : Size of RQ jumps for calculation (min 0.001) [0.01]
    ### ~ Preassembly Fragmentation analysis Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    preassembly=FILE: Preassembly fasta file to assess/correct over-fragmentation (use seqin=FILE for subreads) [None]
    ### ~ Assembly Parameter Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    parameters=T/F  : Whether to output predicted "best" set of parameters [False]
    targetxcov=X    : Target 100% X Coverage for pre-assembly [3]
    xmargin=X       : "Safety margin" inflation of X coverage [1]
    mapefficiency=X : [Adv.] Efficiency of mapping anchor subreads onto seed reads for correction [1.0]
    xsteplen=X      : [Adv.] Size (bp) of increasing coverage steps for calculating required depths of coverage [1e5]
    parseparam=FILES: Parse parameter settings from 1+ assembly runs []
    paramlist=LIST  : List of parameters to retain for parseparam output (file or comma separated, blank=all) []
    predict=T/F     : Whether to add XCoverage prediction and efficiency estimation from parameters and subreads [False]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~~~~ Module snapper ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/snapper.py] ~~~~~~ ###

Module:       Snapper
Description:  Genome-wide SNP Mapper
Version:      1.6.0
Last Edit:    31/10/17
Copyright (C) 2016  Richard J. Edwards - See source code for GNU License Notice

Function:
    Snapper is designed to generate a table of SNPs from a BLAST comparison of two genomes, map those SNPs onto genome
    features, predict effects and generate a series of output tables to aid exploration of genomic differences.

    A basic overview of the Snapper workflow is as follows:

    1. Read/parse input sequences and reference features.

    2. All-by-all BLAST of query "Alt" genome against reference using GABLAM.

    3. Reduction of BLAST hits to Unique BLAST hits in which each region of a genome is mapped onto only a single region
    of the other genome. This is not bidirectional at this stage, so multiple unique regions of one genome may map onto
    the same region of the other.

    4. Determine Copy Number Variation (CNV) for each region of the genome based on the unique BLAST hits. This is
    determined at the nucleotide level as the number of times that nucleotide maps to unique regions in the other genome,
    thus establishing the copy number of that nucleotide in the other genome.

    5. Generate SNP Tables based on the unique local BLAST hits. Each mismatch or indel in a local BLAST alignment is
    recorded as a SNP.

    6. Mapping of SNPs onto reference features based on SNP reference locus and position.

    7. SNP Type Classification based on the type of SNP (insertion/deletion/substitution) and the feature in which it
    falls. CDS SNPs are further classified according to codon changes.

    8. SNP Effect Classification for CDS features predicting their effects (in isolation) on the protein product.

    9. SNP Summary Tables for the whole genome vs genome comparison. This includes a table of CDS Ratings based on the
    numbers and types of SNPs. For the `*.summary.tdt` output is, each SNP is only mapped to a single feature according
    to the FTBest hierarchy, removing SNPs mapping to one feature type from feature types lower in the list:
    - CDS,mRNA,tRNA,rRNA,ncRNA,misc_RNA,gene,mobile_element,LTR,rep_origin,telomere,centromere,misc_feature,intergenic

    More details are given in the Snapper manual.

    Version 1.1.0 introduced additional fasta output of the genome regions with zero coverage in the other genome, i.e.
    the regions in the *.cnv.tdt file with CNV=0. Regions smaller than `nocopylen=X` [default=100] are deleted and then
    those within `nocopymerge=X` [default=20] of each other will be merged for output. This can be switched off with
    `nocopyfas=F`.

    Version 1.6.0 added filterself=T/F to filter out self-hits prior to Snapper pipeline. seqin=FILE sequences that are
    found in the Reference (matched by name) will be renamed with the prefix `alt` and output to `*.alt.fasta`. This is
    designed for identifying unique and best-matching homologous contigs from whole genome assemblies, where seqin=FILE
    and reference=FILE are the same. In this case, it is recommended to increase the `localmin=X` cutoff.

Commandline:
    ### ~ Input/Output options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqin=FASFILE   : Input genome to identify variants in []
    reference=FILE  : Fasta (with accession numbers matching Locus IDs) or genbank file of reference genome. []
    basefile=FILE   : Root of output file names (same as SNP input file by default) [<SNPFILE> or <SEQIN>.vs.<REFERENCE>]
    nocopyfas=T/F   : Whether to output CNV=0 fragments to *.nocopy.fas fasta file [True]
    nocopylen=X     : Minimum length for CNV=0 fragments to be output [100]
    nocopymerge=X   : CNV=0 fragments within X nt of each other will be merged prior to output [20]
    makesnp=T/F     : Whether or not to generate Query vs Reference SNP tables [True]
    localsAM=T/F    : Save local (and unique) hits data as SAM files in addition to TDT [False]
    filterself=T/F  : Filter out self-hits prior to Snapper pipeline (e.g for assembly all-by-all) [False]
    ### ~ Reference Feature Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    spcode=X        : Overwrite species read from file (if any!) with X if generating sequence file from genbank [None]
    ftfile=FILE     : Input feature file (locus,feature,position,start,end) [*.Feature.tdt]
    ftskip=LIST     : List of feature types to exclude from analysis [source]
    ftbest=LIST     : List of features to exclude if earlier feature in list overlaps position [(see above)]
    ### ~ SNP Mapping Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    snpmap=FILE     : Input table of SNPs for standalone mapping and output (should have locus and pos info) [None]
    snphead=LIST    : List of SNP file headers []
    snpdrop=LIST    : List of SNP fields to drop []
    altpos=T/F      : Whether SNP file is a single mapping (with AltPos) (False=BCF) [True]
    altft=T/F       : Use AltLocus and AltPos for feature mapping (if altpos=T) [False]
    localsort=X     : Local hit field used to sort local alignments for localunique reduction [Identity]
    localmin=X      : Minimum length of local alignment to output to local stats table [10]
    localidmin=PERC : Minimum local %identity of local alignment to output to local stats table [0.0]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~~~~ Module unifake ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/tools/unifake.py] ~~~~~~ ###

Program:      UniFake
Description:  Fake UniProt DAT File Generator
Version:      1.3
Last Edit:    17/04/12
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    This program runs a number of in silico predication programs and converts protein sequences into a fake UniProt DAT
    flat file. Additional features may be given as one or more tables, using the features=LIST option. Please see the
    UniFake Manual for more details. 

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqin=FILE      : Input sequence file. See rje_seq documentation for filtering options. [None]
    spcode=X        : Species code to use if it cannot be established from sequence name [None]
    features=LIST   : List of files of addtional features in delimited form []    
    aliases=FILE    : File of aliases to be added to Accession number list (for indexing) [None]
    pfam=FILE       : PFam HMM download [None]
    unipath=PATH    : Path to real UniProt Datafile (will look here for DB Index file made with rje_dbase)
    unireal=LIST    : Real UniProt data to add to UniFake output ['AC','GN','RC','RX','CC','DR','PE','KW']
    fudgeft=T/F     : Fudge the real features left/right until a sequence match is found [True]

    ### ~ PROCESSING/OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    unifake=LIST    : List of predictions to add to entries [tmhmm,signalp,disorder,pfam,uniprot]
    datout=FILE     : Name of output DAT file [Default input FILE.dat]
    disdom=X        : Disorder threshold below which to annotate PFam domain as "DOMAIN" [0.0]
    makeindex=T/F   : Whether to make a uniprot index file following run [False]
    ensdat=T/F      : Look for acc/pep/gene in sequence name [False]
    tmhmm=FILE      : Path to TMHMM program [None]
    cleanup=T/F     : Remove TMHMM files after run [True]
    signalp=FILE    : Path to SignalP program [None]

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None




-extras:

### ~~~~~~ Module compass ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/compass.py] ~~~~~~ ###

Module:       compass
Description:  Comparison Of Motif Predictions Across Sequences/Servers
Version:      2.2
Last Edit:    19/05/06
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Outputs a table of Scansite predictions etc as arranged by alignment > *.compass.tdt. This file should then be
    opened using the COMPASS.xls Excel workbook for analysis using its Macros.

    This is designed to compare multiple aligned sequences - typically orthologues - and/or multiple servers for making
    the same kinds of predictions. Although explicitly designed with certain servers (eg. scansite, netphos) in mind,
    any data that is presented in the right format can be uploaded and compared. The 'Server' column of the results takes
    its values from the extension of the input files. E.g. *.scansite will have the server 'scansite'.

Input files:
    - sequence alignment (may be one sequence)
    - *.server or *.server.txt server results 

Commandline:
    seqin=FILE      : Input alignment file
    partial=T/F     : Allow partial scansite results (NULL values) [False]
    autorun=T/F     : Automated querying of web servers (within RCSI only) [False]
    results=T/F     : Whether to output summary file or simply check/generate results (Aligned seqs only) [True]
    recase=T/F      : Whether to look for accession numbers in case-independent fashion (scansite results) [True]
    txt=T/F         : Whether to look by deafult for *.server.txt files (True) or *.server files (False) [True]

    scansite=FILE   : Motif prediction files for sequences in scansite format [*.scansite, *.automotif & *.elm]
    netphos=FILE    : Motif prediction files for sequences in netphos format [*.netphos]
    uniprot=FILE    : Sequence features/details in UniProt download format [*.uniprot]
    tmhmm=FILE      : Transmembrane topology data in TMHMM single-line format [*.tmhmm]
    signalp=FILE    : SignalP results in single-line format [*.signalp]
    serverlist=X,Y,..,Z : List of servers for results to be read from
                        ['scansite','netphos','automotif','elm','uniprot','tmhmm','signalp','antigenic','disorder']
    
Uses general modules: copy, os, string, sys, time
Uses RJE modules: rje, rje_seq, rje_tm, rje_uniprot
Other modules needed: rje_blast, rje_dismatrix, rje_pam, rje_sequence

### ~~~ Module file_monster ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/file_monster.py] ~~~ ###

Module:       file_monster
Description:  Goes through directories etc and collects information on files etc. 
Version:      2.2
Last Edit:    05/12/13
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Functions:
    <1> File Monster:   [scavenge=T, organise=X and/or cleanup=T]
    Goes through directories etc and collects information on files etc. "scavenge=T" will output a file containing
    relevant file names and locations, ages and sizes. "monster=T" will compare for identical files and farm them off
    into another directory for possible deletion. 

    <2> DirSum: [dirsum=T]
    Goes through directories and subdirectories and summarises the number of files and subdirectories they contain. If a
    directory contains less than X subdirectories [dircut=X] then the subdirectories of that directory will also be
    summarised. Output is in the form: PATH,files,subdir

    <3> Rename: [rename=T]
    Renames all the chosen files [filelist] with the given prefix [prefix=X] into outdir. If usedate=T, dates will be
    added to the prefix (e.g. outdir/prefixdate_num) otherwise new names are just outdir/prefix_num.

    <4> Fix line endings [fixendings=LIST]
    Replace Mac \\r with \\n line endings in place with option to backup old file (unless backups=F).

Output:
    File = File (or directory) name (no path)
    Parent = Parent directory  from DirList
    Folder = Path containing file (or directory) 
    Type = File extension (or "DIR")
    Size = Size in bytes
    Date = Age converted to human readable string
    Age = Age of file (MTime) in seconds
    CTime = String representation of Creation Time
    MTime = String representation of Modified Time
    ATime = String representation of ATime
    FilePath = Full path to file

General Commandline:
    filelist=X,Y,..,Z   : List of files of interest. Can have wildcards. [*]
    dirlist=LIST        : List of directories to look in, in order of preference good -> bad for duplicates. [./]
    subfolders=T/F      : Whether to look in subfolders [True]
    stripnum=T/F        : Whether files may have -XXX numerical suffix from renaming, which should be stripped [False]

File Monster Commandline:
    oldmonster=T/F      : Whether to run old File Monster (V1.x). Will be retired once update complete. [False]
    outdir=PATH         : Output directory for renamed/reorganised files [./Organised/]
    dumpdir=PATH        : Directory in which to dump redundant files (don't move if "None") [./Redundant/]
    cleanup=T/F         : Whether to delete empty directories (and move/delete stripnum) [False]
    cleanfiles=LIST     : List of hidden files to delete during cleanup if directory seems empty ['.picasa.ini','.DS_Store']
    sizematch=X         : Size % similarity threshold to count as match [99.9]
    skiplist=LIST       : List of filenames to skip [thumbs.db]
    organise=X          : File reorganisation mode (none/date/month/compile) [None]
    orgprefix=X         : Prefix for organised outdir subdirectories ['']
    redundancy=T/F      : Whether to check/rate redundancy for scavenge etc. [True]
    scavenge=T/F        : Whether to perform collation of file information [False]
    searchid=X          : ID for search - allows multiple searches to be compared easily [None]

DirSum Commandline:    
    dirsum=T/F          : Whether to perform summary of directory contents [False]
    dircut=X            : Max number of subdirectories to have and still delve into them [50]
    dirdepth=X          : Max depth of subdirectories to delve into. Negative = all. [-1]
    extlist=LIST        : List of file extenstions to report individual stats for ['']

LineEndings Commandline:
    fixendings=FILELIST : Replace Mac with UNIX line endings for FILELIST (wildcards allowed) []

OLD COMMANDS:

File Monster Commandline:
    monster=T/F         : Whether to perform monster cleanup of redundant files [False]
    gooddir=LIST        : List of "good" directories to be automatically kept if i<1 (including subdirs) []
    baddir=LIST         : List of "bad" directories to be automatically screened if i<1 (including subdirs) []
    keepnew=T/F         : Preferentially keep newer files of same size if good/bad status equal [True]
    purgelist=LIST      : List of filenames (allowing wildcards) to purge (move/delete) [WS_FTP.LOG]

Rename Commandline:
    rename=T/F          : Whether to rename files [False]
    sortby=X            : Whether to sort by date or name [date]
    prefix=X            : Text prefix for new file names []
    usedate=T/F         : Whether to use date in new name [False]

Uses general modules: copy, glob, os, stat, re, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~ Module peptide_dismatrix ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/peptide_dismatrix.py] ~~~ ###

Module:       peptide_dismatrix
Description:  Peptide Distance Matrix Generator
Version:      1.1
Last Edit:    27/10/05
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Generates distance matrix for input peptides.

Commandline:
    seqin=FILE  : Peptide Sequence File
    aaprop=FILE : Amino Acid property matrix file. [aaprop.txt]
    outmatrix=X : Type for output matrix - text / mysql / phylip [Phylip]
    delimit=X   : Text delimiter for text or mysql file
    method=X    : Peptide Distance Method to use [ds_prop]
        - ds_prop = Denis Shields distance using AA properties
        - ds_id = Denis Shields distance using AA identity
        - pam = ML PAM distance
        - tot_prop = Summed difference across all properties in all residues
        - best_prop = Minimum aligned property difference by circularising one peptide and sliding versus other

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje, rje_aaprop, rje_dismatrix, rje_pam, rje_seq
Other modules needed: rje_blast, rje_sequence, rje_uniprot

### ~~~ Module peptide_stats ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/peptide_stats.py] ~~~ ###

Module:       peptide_stats
Description:  HRB Peptide Statistics Generator
Version:      1.0
Last Edit:    09/11/05
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Generates property outputs for HRB peptides.

Commandline:
    seqin=FILE  : Peptide Sequence File
    aaprop=FILE : Amino Acid property matrix file. [aaprop.txt]
    delimit=X   : Text delimiter for text or mysql file

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje, rje_aaprop, rje_pam, rje_seq
Other modules needed: rje_blast, rje_dismatrix, rje_sequence, rje_uniprot

### ~~~~~ Module pic_html ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/pic_html.py] ~~~~~ ###

Module:       pic_html
Description:  HTML generator for picture websites
Version:      1.1
Last Edit:    23/11/05
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to take one or more folders of pictures and generate linked HTML pages for them.
    An accessory application for making thumbnails is needed. This should be used prior to running the program,
    or at the prompt.

    At present, this script will generate photos for an assumed hiearchical ordering. The current directory
    should be the one from which the program is run, and this will contain the pictures.htm file which links
    to the other pages. Within this there are then three hierarchies of folder:
    1. A 'Type' of picture group, e.g. Holidays or Animals etc.
    2. A 'Group' of pictures within the type, which may itself have multiple subfolders
    3. An 'Element' of the group of pictures, which consists of a single set of linked photos

    The 'Group' is the primary focus of this script as photos will generally be added for a particular event
    (a holiday for example). In addition to the folders for individual elements, each group should have a thumbnails
    folder which has all the thumbnails for that group. Each element has its own folder for which a set of linked
    web pages is generated. These are linked directly to the Pictures page.

    The pages for each element consist of two frames:
    1. A margin, for easy navigation to individual photos and thumbnails pages etc.
    2. The main frame where the photos are displayed, with their names and a short description.

    Each photo has its own page, with arrows cycling through the previous and next photos (in a circular fashion
    via the thumbnails page) with the title of the picture underneath and, if desired, a small description. Clicking
    on the picture will open it full size in another page. In addition to these, there is a thumbnails page, which
    links to the individual web-pages, and a downloads page which links direct to the photos themselves.

    If usehome=T and/or edithome=T are used (both are by default) the pictures home page will be read for existing
    descriptions and edited to contain links to all folders and descriptions in the picdesc file, i.e. the content
    from the two files plus any additional folders added, will be merged. When reading from the pictures home page,
    the script recognises Types, Groups and Elements of photos on a strict pattern recognition in the HTML:
        Type:   '^<HR><H2>(.+)</H2>'    => Type Description
        Group:  '^<P><B>(.+)</B>'       => Group Description
        Element:'^<A HREF="(\S+)/(\S+)/(\S+)/index.htm" TARGET="_top">(.+\S)</A>' => type,group,el, Element Description
    In each case, the number of photos may be extracted from the description with '\s+[(\d+)]$'. Be careful that no other
    lines will match these patterns. (The Group pattern may occur BEFORE the first Type pattern but NOT AFTER it.)

Commandline:
    ### File locations etc. ###
    pichome=FILE        : File name for pictures home page [pictures.htm]
    usehome=T/F         : Whether to extract descriptions etc. from the pictures home page [True]
    edithome=T/F        : Whether to regenerate the pictures home page [True]
    picdesc=FILE        : File with picture descriptions Type:Group:Element Description [pic_descriptions.txt]
    extlist=X,Y,..,Z    : List of acceptable picture file extensions [jpg,gif]
    picfolder=X         : Name of primary folder to look in for photos [*]
    thumbnails=X        : Folder containing thumbnails for all pictures of this type [thumbnails]
    thumbname=X         : Name to distinguish thumbnails from actual pictures (will rename) [_thumb]

    ### Webpage Appearance ###
    homeback=FILE   : File to use for picture home background [None]
    hometitle="X"   : Title to use for home web page ["Photo Page"]
    picback=FILE    : File to use for other page backgrounds [None]
    fontface=X      : Font to use for text [Comic Sans MS]
    larrow=FILE     : Image file for Left Arrow (pref GIF) [larrow.gif]
    rarrow=FILE     : Image file for Right Arrow (pref GIF) [rarrow.gif]
    picids=T/F      : Whether pictures have picture IDs 'ID - Name.*' [True]
    thumbheight=X   : Height of thumbnails (pixels) in Preview pages [120]

    ### Other ###
    clearhtml=T/F   : Delete existing HTML in picture folders (*.htm and *.html) - may overwrite anyway. [True]

Uses general modules: copy, glob, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~~~ Module prodigis ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/prodigis.py] ~~~~~ ###

Module:       ProDigIS
Description:  Protein Digestion In Silico
Version:      0.2
Last Edit:    21/06/11
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    The function of this module is to take one or more lists of proteins, perform in silico digestions using different
    proteases (and combinations) and then output some predicted stats about peptide numbers etc. It is hoped that this
    data can be used in time to predict which proteins can be potentially identified by Mass Spec and, for a given
    proteome, which combination of proteases should maximise coverage.

Commandline:
    ## Basic Input Parameters ##
    seqfiles=LIST   : Sequence files for input. Wildcards permitted. RJE_SEQ filtering WILL be applied. [*.fas]
    source=FILE     : File containing source protein data (including UniProt AccNum column) [None]
    proteases=LIST  : List of proteases to use [tryp,aspn,gluc,lysc]
    peptides=FILE   : File containing list of identified peptides [None]
    pepcut=X        : Protease used to generate list of identified peptides [tryp]
    positives=FILE  : File of positively identified proteins matching peptide lists [None]

    ## Basic Processing Parameters ##
    combcut=T/F     : Whether to peform combined digestions with pairs of proteases [True]
    nterm=T/F       : Whether to include N-terminal peptides [False]
    nrpep=T/F       : Whether to only include the non-redundant (unique) peptides [False]
    cysweight=T/F   : Whether to weight peptide probabilities according to cysteine count [True]

    ## Basic Output Parameters ##
    minpeplen=X     : Minimum peptide length to consider [0]
    maxpeplen=X     : Maximum peptide length to individually return [40]
    pepmwt=T/F      : Whether to output peptide mol weights in addition to lengths [True]
    cyscount=T/F    : Whether to perform peptide count with Cysteine numbers [True]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~ Module rem_parser ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rem_parser.py] ~~~ ###

Module:       rem_parser
Description:  Module to Parse removed sequence links from Log Files
Version:      0.0
Last Edit:    26/04/05

Function:
    To be added.

Commandline:
    remlog=FILE : File containing removal data

Uses general modules: os, sys, threading, time
Uses RJE modules: rje

### ~~~~ Module rje_dbase ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_dbase.py] ~~~~ ###

Module:       rje_dbase
Description:  RJE Module to Handle Database manipulations and generations
Version:      2.3.1
Last Edit:    02/02/17
Copyright (C) 2007 Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to control generic database manipulations routinely used for me to generate customised
    databases:
    1. Download commonly used databases, primarily UniProt, EnsEMBL, PFam and PPI databases
    2. Reformat and index crucial UniProt data from uniprot.dat and trembl.dat for ease of extraction. (UNIX platform)
    3. Generate taxa-specific databases from input databases
    4. Generate non-redundant species-specific databases using EnsEMBL gene locus informtation

    By default, database paths are relative. To peform an update it is advised that a new directory is created and
    RJE_DBASE run in this directory with the dbdownload=LIST dbprocess=LIST and taxadb=FILE options. Once download and
    formatting is complete, the new files can be copied over the old files.

    Database download is controlled in two ways. UniProt and EnsEMBL are managed by their own respective modules. Other
    databases are currently read from a file, which is in (an attempt of) XML format of the basic form:
    <dbxml>
      <database name="EnsEMBL" ftproot="ftp://ftp.ensembl.org/pub/" outdir="EnsEMBL/Current-release">
        <file path="current_aedes_aegypti/data/fasta/pep/*.gz">Yellow Fever Mosquito</file>
      </database>
    </dbxml>

    The pre-version 1.2 options for making IPI-centred datasets can still be called using the makedb=FILE option along
    with its associated options: screenipi=T screenens=F ensloci=F.

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Primary Database download and processing options ###
    dbdownload=LIST : List of EnsEMBL/UniProt/XML files containing details of databases to download []
    dbprocess=LIST  : List of EnsEMBL/UniProt/IPI database types to process []
    datindex=T/F    : Create an index file for the Uniprot DAT files in unipath if UniProt in dbprocess [True]
    spectable=T/F   : Makes a table of species codes, taxonomy and taxon_id from DAT files if dbprocess UniProt [True]
    taxadb=FILE     : File containing details of taxanomic sub-databases to make [None]
    formatdb=T/F    : Whether to BLAST format database after making [True]
    force=T/F       : Whether to force regeneration of existing files [False]
    ignoredate=T/F  : Whether to ignore the relative timestamps of files when assessing whether to regenerate [False]
    ensloci=T/F     : Reduce EnsEMBL to a single protein per locus, mapping UniProt where possible [True]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Database Path Details ###
    unipath=PATH    : Path to UniProt files [UniProt/]
    ipipath=PATH    : Path to IPI files [IPI/]
    enspath=PATH    : Path to EnsEMBL file [EnsEMBL/]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Database and sub-database manufacture ###
    dbformat=LIST   : Reformats UniProt, IPI or EnsEMBL databases using RJE_SEQ []
    makedb=FILE     : Makes a database from combined databases [None]
                        - Note that rje_seq commandline options will be applied to this database with the addition of a
                        goodspec=X filter applied from the taxalist=LIST
    useX=T/F        : Whether to use certain aspects of databases,
                      where X is:   uniprot/sprot/trembl/ensembl/known/novel/abinitio/ipi [All but ipi True]
    taxalist=LIST   : List of taxanomic groups to extract spec_codes for reduced database (else all) [None]
    speconly=T/F    : Will simply output a list of SPECIES codes to the makedb file, rather than making dbase [False]
    inversedb=T/F   : TaxaList is a list of taxanomic groups *NOT* to be in database [False]
    screenipi=T/F   : Species represented by IPI databases will be screened out of UniProt and EnsEMBL. [False]
    screenens=T/F   : Species represented by EnsEMBL will be screened out of UniProt [True]
    seqfilter=T/F   : Use rje_seq to filter sequences (True) or simply filter on Species Codes (False) [False]
    ensfilter=T/F   : Run EnsEMBL genomes through rje_seq to apply filters, rather than just concatenating [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

### ~~~ Module rje_glossary ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_glossary.py] ~~~ ###

Module:       rje_glossary
Description:  RJE Glossary HTML Maker
Version:      1.4
Last Edit:    17/12/12
Copyright (C) 2012  Richard J. Edwards - See source code for GNU License Notice

Function:
    The function of this module is to convert a plain text file of glossary terms and definitions into a webpage complete
    with optional hyperlinks between terms.

Commandline:
    ### Input Options ###
    infile=FILE   : Input file of Term:Definition [glossary.tdt]
    termsplit=X   : Text to use for splitting term from description (if file extension not recognised) [tab]
    name=X        : Title for HTML output ['Glossary']
    terms=LIST    : List of terms to extract from glossary (all by default) []

    ### Term Linking Options ###
    aname=T/F     : Whether to hyperlink terms using a name refs [True]
    plurals=T/F   : Whether to map plurals onto singular terms [True]
    href=T/F      : Whether to added external hyperlinks for <url> and <url>[text] in descriptions [True]
        
    ### Output Options ###
    htmlstyle=X   : Output HTML style for splits (tab/table/h3) [h3]
    splits=X      : Number of sets to divide terms into [6]
    outfile=FILE  : Output file name (input.html by default) []
    copyright=X   : Copyright statement for page ['RJ Edwards 2012']
    keeporder=T/F : Keep output order the same as input order (unless terms=LIST given). Uses termsplit=X. [False]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_obj, rje_zen
Other modules needed: None

### ~~~ Module rje_itunes ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_itunes.py] ~~~ ###

Module:       rje_itunes
Description:  iTunes PlayList Processor
Version:      0.1
Last Edit:    01/01/14
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    The function of this module is to read in tab delimited exported iTunes playlists and process data about Albums,
    Artists etc. Files should be name *.YYMMDD.tdt so that the dates can be extracted and used to identify changes.

Commandline:
    itunes=LIST     : List of itunes playlist files *.YYMMDD.tdt [itunes.*.tdt]
    mintracks=X     : Min. number of tracks for averages [1]
    addscore=T/F    : Adds a score of 100x Rating for each track [True]
    tophtml=T/F     : Whether to output top tunes HTML summary [True]
    toplist=X       : Number of entries to feature in HTML top lists [20]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~~ Module rje_mysql ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_mysql.py] ~~~~ ###

Module:       rje_mysql
Description:  RJE Module for converting delimited text files to MySQL tables
Version:      1.2
Last Edit:    05/09/11
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to take a bunch of delimited text files and create appropriate build statements for making
    MySQL tables, checking the content if desired. The main MySQL class has the global parameters for the reading and
    handling of input files, which are converted into Table objects. Each Table object itself has a number of Class
    objects. Unless memsaver=T, rje_mysql will check to see if contents for each field are unique.

    The script processes files and generates builds statements in the following way:
    
    1.  The next file to be processed is displayed along with the default table name, its first line (split into
        fields), the delimiter used, and the number of fields. The list of headers (set by header=T/F) are displayed
        along with the second line of the file, also split into fields. The delimiter is a tab for *.tdt files, a
        comma for *.csv files, else is set by delimit=X.

    2.  There is then a user menu with the following options:
        < S >kip file = Skip this file and move on to the next one.
        < T >able Name/Descriptions = option to change the table name (default = file name without extension) and the
            description for the table that is printed in the build file's comment lines
        < H >eader Names/Descriptions = option to manually change the names and descriptions for field headers
        < A >utoname Headers = if the file does not meet the given header=T/F option, this will automatically rename
            all field headers using the first line if desired, or simply "field1" to "fieldn".
        Change < D >elimiter = change the delimiter selected for the file and return to step 1 above.
        < P >roceed = keep current settings and process file

    3.  The file is then read through to assign Types to each field:
        (a) Field contents are scanned to see if they contain pure numeric values and, if so, pure integer values.
            Otherwise, string contents are assumed.
        (b) The presence of Null (empty) values are recorded. Also, unless memsaver=T, the field contents is analysed
            to determine whether it is unique for each row.
        (c) The minimum and maximum lengths are recorded for each field. Minimum and maximum values are recorded for
            numeric values

    4.  Field types are assigned according to the field content characteristics:
        (a) Numeric fields are set as "Unsigned" if the minimum value is >= 0. This is used with the minimum and
            maximum values to assign the integer type if an integer or FLOAT if non-integer.
        (b) String fields are assigned a type based on the minimum and maximum lengths of the contents and whether
            these two values are different. This way a CHAR, VARCHAR or BLOB is assigned of the correct length.
        If i=1+, an option is given to change the read characteristics of each field.

    5.  Unique fields without null entries are identified as potential Primary Keys. If i=-1 then the first is used,
        else the user can choose. If there are no potential fields, are the user chooses none, then an
        auto-incrementing INT(10) field is added as a primary key.

    6.  Each field can be selected to be indexed. By default, any non-numeric fields which have a maximum length in
        the range set by indexlen=X,Y are indexed.

    7.  Based on all the information given, the build statement is generated. This consists of:
        - Comment lines first identify the name, description, file and number of lines.
        - DROP TABLE IF EXISTS command to clear existing data
        - CREATE TABLE using the Types, Primary Key and Indexed fields determined above
        - LOAD DATA statement, including the list of fields in the file if an auto-incremented key was added
        - DELETE FROM statement to remove header lines, allowing for data truncation upon loading
        Upon hitting <ENTER>, this will be written to the given output file (buildfile=FILE & combine=T/F)

Commandline:
    filelist=FILE(s): Input file or files. May be comma-separated (FILE1,FILE2) and include wildcards. [*.tdt,*.csv]
    subfolders=T/F  : Whether to look in subfolders [False]
    mysql=T/F       : Whether to assing data types and check data (else just report lengths of field contents) [True]
    checktypes=T/F  : Whether to check Data Types for given file [True]
    buildfile=FILE  : Output file for MySQL Build statements [mysql_build.txt]
    combine=T/F     : Whether to combine build statements in one file (True) or have separate file per table (False) [True]
    append=T/F      : Whether to append output files or generate new [True]
    header=T/F      : By default, the first line will be read as a header [True]
    memsaver=T/F    : Will not check for Unique fields if True. [False]
    indexlen=X,Y    : Will index all fields with non-numerics between X and Y letters by default [4,30]
    sqldump=FILE    : Read in an SQL dump file and convert into CSV [None]
    
Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_pattern_discovery ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_pattern_discovery.py] ~~~ ###

Module: 	  rje_pattern_discovery
Description:  Pattern Discovery Module 
Version:	  1.3
Last Edit:	  07/10/06
Copyright (C) 2006	Richard J. Edwards - See source code for GNU License Notice

Function:
	Calls and tidies TEIRESIAS. Will add SLIMDISC in time. Will also read motifs and output information content.

SLiMDisc Searching (Tested with slimdisc_V1.3.py):
	seqin=FILE			: Sequence File to search [None]
	slimfiles=LIST		: List of files for SLiMDisc discovery. May have wildcards. (Over-ruled by seqin=FILE.) [*.fas] 
	minsup=X    		: Min. number of sequences to have in file [3]
	maxsup=X    		: Max. number of sequences to have in file [0]
	slimdisc=T/F		: Whether to run list of files through SLiMDisc [False]
	slimopt="X"			: Text string of additional SLiMDisc options [""]
	useres=T/F			: Whether to use existing results files or overwrite (-BT -TT) [True]
	remhub=X			: If X > 0.0, removes "hub" protien ("HUB_PPI") and any proteins >=X% identity to hub [0.0]
						: Renames datasets as -RemHub, -KeptHub or -NoHub
	slimsupport=X		: Min. SLiMDisc support (-S X). If < 1, it is a proportion of input dataset size. [0.1]
	slimranks=X			: Return top X SLiMDisc ranked sequences [1000]
	slimwall=X			: TEIRESIAS walltime (minutes) in SLiMDisc run (-W X) [60]
	slimquery=T/F		: Whether to pull out Query Protein from name of file qry_QUERY [False]
	memsaver=T/F		: Whether to run SLiMDisc in memory_saver mode (-X T) [True]
	bigfirst=T/F		: Whether to run the biggest datasets first (e.g. for ICHEC taskfarm) [True]
	slimversion=X		: Version of SLiMDisc to run (See slimcall=X for batch file jobs) [1.4]  

Batch File Output options:
	batchout=FILE		: Create a batch file containing individual seqin=FILE calls. [None]
	slimcall="X"		: Call for SLiMDisc in batch mode. May have leading commands. ["python slimdisc_V1.4.py"]

TEIRESIAS Searching: *Currently windows-tested only* (Pretty obselete with functional SLiMDisc)
	mysql=T/F			: MySQL mode [True]
	delimit=X			: Text delimiter [\\t]
	info=FILE			: Calculate information content of motifs in FILE (based on AA Freq from seqin, if given) [None]
	igap=X				: Information Content "Gap penalty" (wildcard penalisation) [0]
	outfile=FILE		: Output file name [seqin.teiresias.*]
	teiresias=T/F		: Whether to perform TEIRESIAS search on seqin=FILE [True]
	teiresiaspath=PATH	: Path to TEIRESIAS ['c:/bioware/Teiresias/teiresias_char.exe'] * Use forward slashes (/)
	teiresiasopt=X		: Options for TEIRESIAS Search (Remember "X Y Z" for spaced cmds) e.g. "-bc:\bioware\Teiresias\equiv.txt"
						["-l3 -w10 -c1 -k2 -p"]

Uses general modules: copy, glob, math, os, re, string, sys, time
Uses RJE modules: rje, rje_seq, presto
Other modules needed: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_uniprot, rje_motif

### ~~~~~ Module rje_phos ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_phos.py] ~~~~~ ###

Module:       rje_phos
Description:  RJE Phosphorylation Module
Version:      0.1
Last Edit:    26/10/07
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for the control and management of different phosphorylation sources and for mapping known phosphosites
    onto other proteins. Initially, the reading and processing of PhosphoELM is the only source implemented, although
    phosphorylation prediction may be added with time.

    The phosBLAST method will take an input dataset and BLAST them against the phosphorylation database(s), align hits
    above a certain GABLAM %ID and mark the phosphosites on the sequence. A second %ID cut-off will be used to
    determine which sites are marked as identities and which as homologies. Outputs will include an alignment with all
    the phosphoHomologues plus a simpler "alignment" of the query protein and marked sites. Phosphorylation predictions
    may be added to this second simpler "alignment". A UniProt-format file could also be produced with phosphoSites
    marked.

Commandline:
    ### ~ PhosphoELM Input Options (Creates Fasta file if pELM given) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    pelm=FILE       : Filename for phosphoELM download [None]
    filterseq=T/F   : Apply rje_seq sequence filters to phosphoELM data [False]
    pelmfas=FILE    : Filename for fasta file output of pELM sequences [pelm.fas]
    ### ~ PhosBLAST Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    phosblast=FILE  : Fasta file of sequences to perform phosBLAST method against pELM [None]
    usespec=T/F     : Use species codes for determing same species for ID matches [True]
    idsim=X         : Percentage identity (GABLAM; phosblast qry) for marking as identity [95.0]
    homsim=X        : Percentage identity (GABLAM; phosblast qry) for marking as homologue [40.0]
    phosdat=T/F     : Whether to produce a modified UniProt-format file with potential phosphoSites as features [False]
    phosres=FILE    : Delimited text file containing input sequence, position and evidence [*.phosres.tdt]

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_seq
Other modules needed: None

### ~~~ Module rje_pydocs ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_pydocs.py] ~~~ ###

Module:       rje_pydocs
Description:  Python Module Documentation & Distribution
Version:      2.16.5
Last Edit:    05/09/17
Copyright (C) 2011 Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to aid handling and maintenance of linked Python Modules by creating limited documentation
    for the module, its objects and their methods. Possible interactions between modules and methods can also be
    identified. In addition, this module can be used to make the distribution directory for a python project and form the
    basis of linked webpages.    

Commandline:
    ### ~~~ Python Module Input Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    name=X          : Name for PyDoc run. Used for file naming and within documentation files. ['pydocs']
    pylist=LIST     : List of python modules to upload. Can have * wildcards. Will add '.py' if missing. ['*']
    pypath=PATH     : Path to python modules. Will also look in listed sourcedir subfolders. [../]
    sourcedir=LIST  : List of subdirectories in which to look for modules [tools,extras,libraries,legacy]
    addimports=T/F  : Whether to add identified imported modules to python module list [True]
    docsource=PATH  : Input path for Python Module documentation (manuals etc.) ['../docs/']

    ### ~~~ Python Module Processing and Documentation Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    html=T/F        : Whether to make a basic HTML page of module docstrings (will make linked fun later) [False]
    fulldoc=T/F     : Whether to generate full docstring output including Classes and Methods [False]
    methodskip=LIST : List of methods to skip documentation [makeInfo,cmdHelp,setupProgram,Main]
    methodcap=X     : Maximum number of method calls before collapsed to single line [0]
    calls=T/F       : Whether to output Method Calls [False]
    self=T/F        : Whether to include 'self' calls of methods if calls=T [False]
    docdir=PATH     : Output path for Python Module documentation ['../docs/']
    stylesheets=LIST: List of style sheets to use for HTML ['rje_tabber.css','re1u06.css']
    stylepath=PATH  : Path to style sheets and javascript code folder etc. [http://www.southampton.ac.uk/~re1u06/]
    author=X        : Author name to put at bottom of webpages [RJ Edwards]
    keywords=LIST   : List of keywords to add in additon to module names/descriptions []
    modlinks=LIST   : List of rje_ppi formats for module import links (xgmml,tdt,svg,r,png) [xgmml]
    makepages=T/F   : Special run to generate default cmd/ and docs/ pages for commandline option docs [False]
    logourl=URL     : URL to SLiMSuite program logos ['http://www.slimsuite.unsw.edu.au/graphics/']
    manualurl=URL   : URL to SLiMSuite program manuals ['http://docs.slimsuite.unsw.edu.au/software/slimsuite/docs/manuals/']
    resturl=URL     : URL for REST server to pull outfmt docs ['http://rest.slimsuite.unsw.edu.au/']

    ### ~~~ Distribution and Webpage options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###    
    distribute=LIST : Names of distributions - gets details from distributions.txt ['SLiMSuite']
    distdir=PATH    : Output directory for distribution directories ['../packages/']
    webdir=PATH     : Directory containing webpage resources [../html/']
    email=X         : E-Mail address for general contact [seqsuite@gmail.com]
    release=X       : Release for packages [YYYY-MM-DD]
    
See also rje.py generic commandline options.

### ~~~ Module rje_scansite ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_scansite.py] ~~~ ###

Module:       rje_scansite
Description:  Converter for Scansite to MySQL
Version:      0.0
Last Edit:    11/01/06
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    Converts multiple scansite output files into a single table for MySQL upload. (Use rje_mysql.py to generate Build
    Statement.)

Commandline:
    filelist=LIST   : List of files for upload. Can use wildcard. [*.scansite.txt]
    outfile=FILE    : Output file name [scansite.tdt]

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_seqgen ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_seqgen.py] ~~~ ###

Module:       rje_seqgen
Description:  Random Sequence Generator Module
Version:      1.7
Last Edit:    17/01/13
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to generate a number a random sequences based on input AA or Xmer frequencies and the desired
    order of markov chain from which to draw the amino acid (or nucleotide) probabilites.

    If poolgen=T, then the amino acid frequencies will be used to make a finite pool of amino acids from which sequences
    will be built. This will ensure that the total dataset has the correct amino acid frequencies. Because of the
    potential of this to get 'stuck' in impossible sequence space - especially if screenx > 0 - an additional parameter
    poolcyc=X determines how many times to retry the generation of sequences. If poolgen=F, then generation of sequences
    will be faster but the resulting dataset may have Xmer frequencies that differ greatly from the input frequencies,
    depending on how many (and which) redundant and/or screened Xmer-containing sequences are removed. (If seqin contains
    only one peptide then each random peptide will be a scramble of that peptide.)

    !!!NEW!!! Verson 1.3 has new scramble function, which takes in a list of peptides and tries to construct scrambled
    versions of them. In this case, screenx=X sets the length of common Xmers between the scrambled peptide and the
    original peptide at which a scrambled peptide will be rejected. This should set > 1, else all peptides will be
    rejected. (If left at the default of zero, no peptides will be rejected.) In this mode, outfile=FILE will set the
    name of a delimited output file containing two columns: peptide & scramble. (Default filename = scramble.tdt)

    !!!NEW!!! Version 1.5 has a new BLAST-centred method for making a random dataset from an input dataset, retaining the
    approximate evolutionary relationships as defined by BLAST homology, which should result in similar GABLAM statistics
    for the randomised dataset. For this, a random sequence is created first. Any BLAST hits between this and other
    sequences are then mapped, keeping the required percentage identity (and using different amino acids drawn from the
    frequency pool for the rest). The next sequence is taken, completed and then the same process followed, until all
    sequences have been made. Improvements to make: (a) incorporate similarity too; (b) adjust aa frequencies after BLAST
    mapping. This method is activated by the blastgen=T option and has limited options as yet.
    NB. The input dataset will *not* be subject to rje_seq filtering.

    !!!NEW!!! Version 1.6 has an EST randomiser. This will go through each sequence in turn and generate a new sequence of
    the same length using the NT frequencies (or markov chain frequencies) of just that sequence. Updated in V1.7 to make
    this work for proteins too.

Commandline:
    ## Generation options ##
    seqnum=X        : Number of random sequences to generate [24]
    seqlen=X,Y      : Range of lengths for random sequences [10]
    markovx=X       : Order of markov chain to use for sequence construction [1]
    aafreq=FILE     : File from which to read AA Freqs [None]
    xmerfile=FILE   : File from which to read Xmer frequencies for sequence generation [None]
    xmerseq=FILE    : Sequence file from which to calculate Xmer frequencies [None]
    * xmerseq is overwridden by xmerfile and aafreq. aafreq only works if markovx=1 and is over-ridden by xmerfile *
    nrgen=T/F       : Whether to generate a non-redundant sequence list (whole-sequence redundancies only) [True]
    poolgen=T/F     : Whether to build sequences using a fixed AA pool (exact freqs) or probabilities only [False]
    poolcyc=X       : Number of times to retry making sequences if rules are broken [1]
    maxhyd=X        : Maximum mean hydrophobicity score [10]

    ## Output & Naming ##
    outfile=FILE: Output file name [randseq.fas]
    randname=X  : Name 'leader' for output fasta file [randseq]
    randdesc=T/F: Whether to include construction details in description line of output file [True]
    idmin=X     : Starting numerical ID for randseq (allows appending) [1]
    idmax=X     : Max number for randseq ID. If < seqnum, will use seqnum. If <0, no zero-prefixing of IDs. [0]
    append=T/F  : Whether to append to outfile [False]

    ## Other Xmers of Interest ##
    screenfile=FILE : File of Xmers to screen in generated sequences [None]
    xmerocc=T/F     : Whether to output occurrences of screened Xmers [True]
    screenx=X       : Reject generated sequences containing screened Xmers >= X [0]
    screenrev=T/F   : Whether to screen reverse Xmers too [False]

    ## Peptide Scrambling Parameters ##
    # Uses seqnum=X, randdesc, idmin/max=X and randname=X but for each input peptide (oldname_randnameID)
    scramble=T/F    : Run peptide scrambler [False]
    fullscramble=T/F: Generate all possible scrambles for each peptide in TDT [False]
    scramblecyc=X   : Number of attempts to try each scramble before giving up [10000]
    seqin=FILE      : Sequence file containing peptides to scramble [None]
    peptides=LIST   : Alternative peptide sequence input for scrambling []
    outfile=FILE    : Output delimited file of scrambled peptides or peptide and scrambled sequence. [scramble.tdt]
    teiresias=X		    : Length of patterns to be screened by additional TEIRESIAS search on scrambled vs original [0]
    teiresiaspath=PATH	: Path to TEIRESIAS ['c:/bioware/Teiresias/teiresias_char.exe'] * Use forward slashes (/)

    ### BLAST-based dataset randomiser (uses some of the Output options listed) ###
    blastgen=T/F    : Activate the BLASTGen method [False]
    seqin=FILE      : Input sequence file to randomise [None]
    keepnames=T/F   : Whether to keep same input names in outfile [False]

    ### EST Randomiser ###
    estgen=T/F      : Whether to run EST randomiser method [False]
    
Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje, rje_markov, rje_seq, rje_blast
Other modules needed: rje_dismatrix, rje_pam, rje_sequence, rje_uniprot

### ~~~ Module rje_seqplot ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_seqplot.py] ~~~ ###

Module:       rje_seqplot
Description:  Sequence plotting module
Version:      0.0
Last Edit:    16/06/08
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to work in conjunction with Norman's sequence plotting tool(s).

    See also rje_seq, rje_uniprot and rje_disorder options.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqin=FILE      : File containing input sequences [None]
    occfile=FILE    : File containing motif occurrences to plot [None]
    plotstat=LIST   : List of stats to plot (cons/rel/dis/sa/hyd) [cons/rel/dis]
    plotft=LIST     : List of features to plot if input is UniProt [SIGNALP,TRANSMEMBRANE,DOMAIN,PFAM]
    plotre=LIST     : List of regular expressions to plot []
    plotwin=X       : Window for stats plot +/- [7]
    ### ~ OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    outfile=X       : Leader for output files [None]
    occonly=T/F     : Only output sequences with motif occurrences [False]
    rescale=T/F     : Whether to rescale plotstats (Truncate at 0.0 and normalise to max 1.0) [True]
    
Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_disorder, rje_seq, rje_zen
Other modules needed: None

### ~~~ Module rje_sleeper ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_sleeper.py] ~~~ ###

Module:       rje_sleeper
Description:  RJE Sleeper module to keep Putty active on Bioinformatics/Cerberus
Version:      0.0
Last Edit:    10/10/05
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Occasionally prints to screen to keep putty active.

Commandline:
    sleep=X : Seconds to sleep for between prints [600 (10 mins)]
    wake=X  : Total seconds until finishing [864,000 (10 days)]
    tofile=T/F  : Whether to print to file [True]
    toscreen=T/F  : Whether to print to file [False]

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~~~ Module rje_ssds ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_ssds.py] ~~~~~ ###

Module:       rje_ssds
Description:  SDSS Batch Wrapper
Version:      0.0
Last Edit:    14/12/09
Copyright (C) 2009  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module executes simple batch runs of SSDS.

Commandline:
    input=FILE  : Input sequence file
    strand=X    : Strand for RACE primers -1 (3'), 1 (5') or 0 (both) [0]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~~ Module rje_yeast ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/rje_yeast.py] ~~~~ ###

Module:       rje_yeast
Description:  Yeast PPI & Sequence Module
Version:      0.0
Last Edit:    22/12/10
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    This Yeast SLiMFinder Analysis Module is designed to read, assess, store and combine the different yeast information
    for SLiMFinder analysis, using the YGOB for generating alignments for conservation masking etc.

    The main (default) inputs are:
    - Y2H_union.txt = High quality binary interactions from Vidal lab
    - Pillars.tab = YGOB orthology pillars for yeast species
    - Proteins.fas = Fasta file of protein sequences to match pillars.tab
    
Commandline:
    ### ~ INPUT DATA ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqin=FILE      : Input sequence file containing yeast sequences [Proteins.fas]
    pillars=FILE    : YGOB pillars file [Pillars.tab]
    ppifile=FILE    : Input PPI data (two columns of binary interactors) [Y2H_union.txt]
    xref=FILE       : Yeast identifier XRef file (e.g. BioMart download) [yeast_xref.20101222.tdt]
    sgd2sp=T/F      : Convert SGD identifiers into SwissProt identifiers [False]
    gopher=T/F      : Whether to feed Pillars into GOPHER Orthology (at BLAST ID stage) [False]
    
See also rje.py generic commandline options.

### ~~~~ Module seqforker ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/seqforker.py] ~~~~ ###

Program:      SeqForker
Description:  Generic Sequence Analysis Forking Script
Version:      1.0
Last Edit:    23/09/05
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to take a large input sequence dataset, split it into chunks, fork out the chunks to some
    process and then stick all the results back together at the end. It is designed to be flexible and work for any
    analyses where each sequence is looked at independently and filenames can be given to the program performing the
    analysis.

Commandline:
    seqin=FILE          : Input sequence file [None]
    split=X             : Number of sequences per split file [0]
    startfrom=X         : Will pick up program at this sequence, where X is the name or accession number [None]
                          * Remember to set append=T if picking up a crashed run *
    append=T/F          : Will append to results files rather than overwrite [False]
    forkprog=PATH       : Common program system call for all forked splits of sequence file (e.g. program) [None]
    forkcmd="blah blah" : Common system commands for all forked splits of sequence file (e.g. program options) [None]
    outcmd=X            : Command line option for giving output file name. Will be altered to match forked splits (*.*) [None]
    seqincmd=X          : Command given to program for input file name [seqin=]
                          * SeqForker will stitch together "forkprog seqincmd=FILE forkcmd outcmd" *
    
General Commandline:
    v=X         : Sets verbosity (-1 for silent) [0]
    i=X         : Sets interactivity (-1 for full auto) [0]
    log=FILE    : Redirect log to FILE [Default = calling_program.log]
    newlog=T/F  : Create new log file. [Default = False: append log file]

Forking Commandline:
    noforks=T/F : Whether to avoid forks [False]
    forks=X     : Number of parallel sequences to process at once [0]
    killforks=X : Number of seconds of no activity before killing all remaining forks. [3600]

Uses general modules: copy, glob, os, re, string, sys, time
Uses RJE modules: rje, rje_seq
Other modules needed: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_uniprot

### ~~~~ Module sfmap2png ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/sfmap2png.py] ~~~~ ###

Module:       sfmap2png
Description:  Converts SLiMFinder Mapping files to PNGs
Version:      0.0
Last Edit:    01/09/08
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module takes a SLiMFinder *.mapping.fas file and uses some R visualisations to convert it into relative
    conservation/disorder PNG files.

Commandline:
    seqin=FILE  : *.mapping.fas file to use as input data []

See also rje.py generic commandline options and rje_disorder.py commands.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~ Module slim_pickings ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/slim_pickings.py] ~~~ ###

Module:       slimpickings
Description:  SLiMDisc results compilation and extraction
Version:      3.0
Last Edit:    18/01/07
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This is a basic results compiler for multiple SLiMDisc motif discovery datasets. There are currently the following
    functional elements to the module:

    1. Basic compilation of results from multiple datasets into a single file. This will search through the current
        directory and any subdirectories (unless subdir=F) and pull out results into a single comma-separated file
        (slimdisc_results.csv or outfile=FILE). With the basic run, the following statistics are output:
            ['Dataset','SeqNum','TotalAA','Rank','Score','Pattern','Occ','IC','Norm','Sim']
        This file can then be imported into other applications for analysis. (E.g. rje_mysql.py can be run on the
        file to construct a BUILD statement for MySQL, or StatTranfer can convert the file for STATA analysis etc.)
        !!! NB: If multiple datasets (e.g. in subdirectories) have the same name, slim_pickings will become confused and
        may generate erroneous data later. Please ensure that all datasets are uniquely named. !!!

    2. Additional optional stats based on the motifs sequences themselves to help rank and filter interesting results.
        These are:
            - AbsChg = Number of charged positions [KRDE]
            - NetChg = Net charge of motif [KR] - [DE]
            - BalChg = Balance of charge = Net charge in first half motif - Net charge in second half
            - AILMV = Whether all positions in the motif are A,I,L,M or V.
            - Aromatic = Count of F+Y+W
            - Phos = Potential phosphorylated residues X (none) or [S][T][Y], whichever are present

    3. Calculation of additional statistics from the input sequences, using PRESTO. These are:
            - Mean IUPred/FoldIndex Protein Disorder around the motif occurrences (including extended window either side)
            - Mean Surface Accessibility around the motif occurrences (including an extended window either side)
            - Mean Eisenberg Hydrophobicity around the motif occurrences (including an extended window either side)
            - SLiM conservation across orthologous proteins. (This calculation needs improving.)
        The mean for all occurrences of a motif will be output. In addition, percentile steps can be used to assess
        motifs according to selected threshold criteria (in another package). This will return the threshold at a
        given percentile, e.g. SA_pc75=2.0 would mean that 75% of occurrences have a mean Surface Accessibility value
        of 2.0 or greater. (For hydrophobicity, Hyd_pc50=0.3 would be 50% of occurrences have mean Hydrophobicity of
        0.3 or *less*. This is because low hydrophobicity is good for a (non-structural) functional motif.)

    4. Collation and extraction of key data for specific results. These may be by any combination of protein, motif
        or dataset. If a list of datasets is not given, then all datasets will be considered. (Likewise proteins and
        motifs.) To be very specific, all three lists may be specified (slimlist=LIST protlist=LIST datalist=LIST).
        Information is pulled out in a two-step process:
            (1) The slimdisc.*.index files are consulted for the appropriate list of datasets. If missing, these will
                be regenerated. (slimdisc.motif.index and slimdisc.protein.index both point to dataset names.
                slimdisc.dataset.index points these names to the full path of the results.) Only datasets returned by
                all appropriate lists will be analysed for data extraction.
            (2) The appropriate data on the motifs will be extracted into a directory as determined by outdir=PATH.
                Depending on the options selected, the following (by default all) data is returned:
                - *.motifaln.fas = customised fasta file with motifs aligned in different sequences, ready for dotplots
                    and manual inspection for homology not detected by BLAST.
                - *.dat = UniProt DAT file for as many parents as possible.
        These files will be saved in the directory set by outdir=PATH.

    5. Re-ranking of results. rerank=X will now re-rank the results for each dataset according to the statistic set by
    rankstat=X, and output the top X results only. By default, this is the "R-score" = ic * norm * occ / exp.  The output
    "Rank" will be replaced with the new rank and a new column "OldRank" added to the ouput. zscore=T/F turns on and off
    a simple Z-score calculation based on the slimranks read in. Version 2.5 added a new option for a crude length
    correction of the RScore, dividing by 20 to the power of the motif IC (as calculated by SLiM Pickings on a scale of
    1.0 per fixed position). This is controlled by the lencorrect=T/F option. By default this is False (for backwards
    compatibility) but with future versions this may become the default as it is assumed (by me) that it will improve
    performance. However, there is currently no justification for this, so use with caution!

    6. Filtering of results using the statfilter=LIST option, allowing results to be filtered according to a set of
    rules: LIST should be (a file containing) a comma-separated list of stats to filter on, consisting of X*Y where X is
    an output stat (the column header); * is an operator in the list >, >=, =, =< ,< ; and Y is a value that X must have,
    assessed using *. This filtering is crude and may behave strangely if X is not a numerical stat (although Python does
    seem to assess these alphabetically, so it may be OK)! This filtering is performed before the reranking of the motifs
    if rerank=X is used. This can make run times quite long as many more  motifs need stats calculations. (If rerank=X is
    used without statfilter, re-ranking is done earlier to save time.) See the manual for details.

    7. !!!NEW!!! with version 3.0, customised scores can be created using the newscore=LIST option, where LIST is in the
    form X:Y,X:Y, where in turn X is the name for the new score (a column with this name will be produced) and Y is the
    formula of the score. This formula may contain any output column names, numbers and the operators +-*/^ (^ is "to
    the power of"), using brackets to set the order of calculation. Without brackets, a strict left to right hierarchy is
    observed. e.g. newscore=Eg:3+2*6 will generate a column called "Eg" containing the value 30.0. Custom scores can
    feature previously defined custom scores in the command options, so a second newscore call could be
    newscore=Eg:3+2*6,Eg2:Eg^2 (= Eg squared = 900.0). This can be used in conjunction with statfilter,
    e.g. newscore=UDif:UHS-UP statfilter=UDif>1.

Commandline:
    ## Basic compilation options ##
    outfile=FILE    : Name of output file. [slimdisc_results.csv]
    dirlist=LIST    : List of directories from which to extract files (wildcards OK) [./]
    compile=T/F     : Compile motifs from SliMDisc rank files into output file. (False=index only) [True]
    append=T/F      : Append file rather than over-writing [False]
    slimranks=X     : Maximum number of SlimDisc ranks to exract from any given dataset [5000]
    rerank=X        : Re-ranks according to RScore (if expect=T) and only outputs top X new ranks (if > 0) [5000]
    rankstat=X      : Stat to use to re-rank data [RScore]
    motific=T/F     : Recalulate IC using PRESTO. Used for re-ranking. OldIC also output. [False]
    lencorrect=T/F  : Implements crude length correction in RScore [False]
    delimit=X       : Change delmiter to X [,]

    ## Advanced compilation options ##
    subdir=T/F      : Whether to search subdirectories for rank files [True]
    webid=LIST      : List of SLiMDisc webserver IDs to compile. (Works only on bioware!) []
    slimversion=X   : SLiMDisc results version for compiled output [1.4]

    ## Additonal statistics ##
    abschg=T/F      : Whether to output number of charged positions (KRDE) [True]
    netchg=T/F      : Whether to output net charge of motif (KR) - (DE) [True]
    balchg=T/F      : Whether to output the *balance* of charge (netNT - netCT) [True]
    ailmv=T/F       : Whether to output if all positions in the motif are A,I,L,M or V. [True]
    aromatic=T/F    : Whether to output count of F+Y+W [True]
    phos=T/F        : Whether to output potential phosphorylated residues X (none) or [S][T][Y], if present [True]
    expect=T/F      : Calculate min. expected occurrence of motif in search dataset [True]
    zscore=T/F      : Calculate z-scores for each motif using the entire dataset (<=slimranks) [True]
    newscore=LIST   : Lists of X:Y, create a new statistic X, where Y is the formula of the score. []
    custom=LIST     : Calulate Custom score as a produce of stats in LIST []

    ## Additional calculations to make: see PRESTO for additional relevant commandline options ##
    slimsa=T/F      : Calculate SA information for SLiMDisc Results [True]
    winsa=X         : Number of aa to extend Surface Accessibility calculation either side of motif [0]
    slimhyd=T/F     : Calculate Eisenbeg Hydophobicity for SLiMDisc Results [True]
    winhyd=X        : Number of aa to extend Eisenberg Hydrophobicity calculation either side of motif [0]
    slimcons=T/F    : Calculate Conservation stats for SLiMDisc results [False]
                    - See PRESTO conservation options. (NB. consamb does nothing.)
    slimchg=T/F     : Calculate selected charge statistics (above) for occurrences in addition to pattern [False]
    slimfold=T/F    : Calculate disorder using FoldIndex over the internet [False]
    slimiup=T/F     : Calculate disorder using local IUPred [True]
    windis=X        : Number of aa to extend disorder prediction each side of occurrence [0]
    iucut=X         : Cut-off for IUPred results [0.2]
    iumethod=X      : IUPred method to use (long/short) [short]
    iupath=PATH     : The full path to the IUPred exectuable [c:/bioware/iupred/iupred.exe]
    percentile=X    : Percentile steps to return in addition to mean [0]

    ## Collation and Extraction of specific results ##
    index=T/F       : Whether to create index files (slimpicks.*.index) for proteins, motifs and datasets [True]
    bigindex=T/F    : Whether to use the special makeBigIndexFiles() method [False]
    fullpath=T/F    : Whether to use full path (else relative) for dataset index [True]
    slimpath=PATH   : Path to place (or find) index files. *Cannot be used for extraction if fullpath=F* [./]
    slimlist=LIST   : List (A,B,C) or FILE containing list of SLiMs (motifs) to extract []
    protlist=LIST   : List (A,B,C) or FILE containing list of proteins for which to extract results  []
    datalist=LIST   : List (A,B,C) or FILE containing list of datasets for which to extract results []
    strict=T/F      : Only extract protein/occurrence details for those proteins in protlist [False]
                      (False = extract details for all proteins in datalist datasets containing slimlist motifs)
    outdir=PATH     : Directory into which extracted data will be placed. [./]
    picksid=X       : Outputs an extra 'PicksID' column containg the identifier X []
    inputext=LIST   : List of file extensions for original input files. (Should be in same dir as *.rank, or one dir above)
                      [dat,fas,fasta,faa]
    indexre=LIST    : List of alternative regular expression patterns to try for index retrieval []
                      - ipi     : 'ipi_HUMAN__(\S+)-*\d*=(\S.+)',           # IPI Human sequence
                      - ipi_sv  : '^ipi_HUMAN__([A-Za-z0-9]+)-*\d*=(\S.+)', # IPI Human UniProt splice variant
                      - ft      : '^(\S+)_HUMAN=(\S+)',                     # SLiMDisc FullText (UniProt format) retrieval
                      - ft_sv   : '^([A-Za-z0-9]+)-*\d*_HUMAN=(\S+)'        # SLiMDisc FullText (UniProt format) splice variant

    ## Additional Output for Extracted Motifs ##
    occres=FILE     : Output individual occurrence data in FILE [None]
    extract=T/F     : Extract additional data for motifs [True if datasets/SLiMs/accnums given, else False]
    motifaln=T/F    : Produce fasta files of local motif alignments [True]
    flanksize=X     : Size of sequence flanks for motifs [30]
    xdivide=X       : Size of dividing Xs between motifs [10]
    datout=FILE     : Extract UniProt entries from parent proteins where possible into FILE [uniprot_extract.dat]
    unitab=T/F      : Make tables of UniProt data using rje_uniprot.py [True]
    ftout=FILE      : Make a file of UniProt features for extracted parent proteins, where possible, incoroprating SLIMs [None]
    unipaths=LIST   : List of additional paths containing uniprot.index files from which to look for and extract features ['']
    peptides=T/F    : Peptide design around discovered motifs [False]

    ## Additional Output for Proteins ##
    proteinaln=T/F  : Search for alignments of proteins containing motifs and produce new file containing motifs [True]
    gopher=T/F      : Use GOPHER to generate missing orthologue alignments in alndir - see gopher.py options [False]
    alndir=PATH     : Path to alignments of proteins containing motifs [./] * Use forward slashes (/) [Gopher/ALN/]
    alnext=X        : File extension of alignment files, accnum.X [orthaln.fas]

    ## Advanced Filtering Options ##
    statfilter=LIST : List of stats to filter (remove matching motifs) on, consisting of X*Y where:
                      - X is an output stat (the column header),
                      - * is an operator in the list >, >=, !=, =, >= ,<    !!! Remember to enclose in "quotes" for <> !!!
                      - Y is a value that X must have, assessed using *.
                      This filtering is crude and may behave strangely if X is not a numerical stat!
    zfilter=T/F     : Calculate the Z-score on the filtered dataset (True) or the whole dataset (False) [False]
    rankfilter=T/F  : Re-ranks the filtered dataset (True) rather than the whole (pre-filtered) dataset (False) [True]
                      - NB. If zfilter=T then rankfilter=T.

    ## Old/obselete options ##
    advprob=T/F     : Calculate advanced probability based on actual sequences containing motifs [False] #!# Not right yet!! #!#
    advmax=X        : Max number of sequences to use computationally intensive advanced probability [35]

    *** See RJE_UNIPROT options for UniProt settings ***

Uses general modules: copy, math, os, re, string, sys, time
Uses RJE modules: rje, gopher, presto, rje_disorder, rje_motif, rje_scoring, rje_seq, rje_sequence, rje_uniprot
Other modules needed: rje_blast, rje_dismatrix, rje_pam 

### ~~~~~ Module wormpump ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/extras/wormpump.py] ~~~~~ ###

Module:       WormPump
Description:  Worm Pump Counter
Version:      0.0
Last Edit:    26/02/10
Copyright (C) 2010  Richard J. Edwards - See source code for GNU License Notice

Function:
    The function of this module will be added here.

Commandline:
    outfile=FILE    : Name of output file [wormpump.tdt]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None




-libraries:

### ~~~ Module ned_eigenvalues ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/ned_eigenvalues.py] ~~~ ###

Module:       ned_eigenvalues
Description:  Modified N. Davey Relative Local Conservation module
Version:      1.0
Last Edit:    03/09/09
Copyright (C) 2009 Norman E. Davey & Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is not for standalone running and has no commandline options (including 'help'). All options are handled
    by the parent module.

Uses general modules: operator, math, random

### ~~~ Module ned_rankbydistribution ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/ned_rankbydistribution.py] ~~~ ###

Module:       ned_rankbydistribution
Description:  Modified SLiMFinder stats module
Version:      1.2
Last Edit:    02/02/14
Copyright (C) 2009 Norman E. Davey & Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is a stripped down template for methods only. This is for when a class has too many methods and becomes
    untidy. In this case, methods can be moved into a methods module and 'self' replaced with the relevant object.

Commandline:
    This module is not for standalone running and has no commandline options (including 'help'). All options are handled
    by the parent module.

Uses general modules: re, copy, random, math, sys, time, os, pickle, sets, string, traceback
Uses RJE modules: rje_seq, rje_uniprot, rje, rje_blast, rje_slim

### ~~~~~~~~ Module rje ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje.py] ~~~~~~~~ ###

Module:       rje
Description:  Contains SLiMSuite and Sequite General Objects
Version:      4.19.0
Last Edit:    05/09/17
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    General module containing Classes used by all SLiMSuite and Sequite programs plus a number of miscellaneous methods.
    Controls output to screen, commandline parameters and Log files.

    Commandline options are all in the form X=Y or `-X Y`. Where Y is to include spaces, use X="Y".

Commandline:
    ### ~ General SLiMSuite Options ~ ###
    v=X             : Sets verbosity (-1 for silent) [0]
    i=X             : Sets interactivity (-1 for full auto) [0]
    log=FILE        : Redirect log to FILE [Default = calling_program.log]
    newlog=T/F      : Create new log file. [Default = False: append log file]
    silent=T/F      : If set to True will not write to screen or log. [False]
    errorlog=FILE   : If given, will write errors to an additional error file. [None]
    help            : Print help to screen

    ### ~ Common Options (most programs) ~ ###
    basefile=FILE   : This will set the 'root' filename for output files (FILE.*), including the log
    delimit=X       : Sets standard delimiter for results output files [\t]
    force=T/F       : Force to regenerate data rather than keep old results [False]
    backups=T/F     : Whether to generate backup files (True) or just overwrite without asking (False) [True]
    rest=X          : Variable that sets the output to be returned by REST services [None]

    ### ~ Forking Options (Some programs only) ~ ###
    noforks=T/F     : Whether to avoid forks [False]
    forks=X         : Number of parallel sequences to process at once [0]
    killforks=X     : Number of seconds of no activity before killing all remaining forks. [36000]

    ### ~ Program-Specific Commands (Some programs only) ~ ###
    outfile=FILE    : This will set the 'root' filename for output files (FILE.*), excluding the log
    mysql=T/F       : MySQL output
    append=T/F      : Append to results files rather than overwrite [False]
    maxbin=X        : Maximum number of trials for using binomial (else use Poisson) [-]
        
    ### ~ System Options ~ ###
    win32=T/F       : Run in Win32 Mode [False]
    osx=T/F         : Run in MacOSX Mode [False]
    pwin            : Run in PythonWin (** Must be 'commandline', not in ini file! **)
    cerberus        : Run on Cerberus cluster at RCSI
    memsaver=T/F    : Some modules will have a memsaver option to save memory usage [False]
    runpath=PATH    : Run program from given path (log files and some programs only) [path called from]
    rpath=PATH      : Path to installation of R ['R']

    ### ~ Development Options ~ ###
    debug=T/F       : Turn on additional debugging prints and prompts [False]
    warn=T/F        : Turn on program integrity check warnings (unless silent) [True]
    test=T/F        : Run additional testing methods and/or produce additional test outputs [False]
    dev=T/F         : Run development-specific code. (Added to keep main coding working during dev) [False]
    webserver=T/F   : Trigger webserver run and output [False]
    soaplab=T/F     : Implement special options/defaults for SoapLab implementations [False]

Classes:
    RJE_Object(log=None,cmd_list=[]):
        - Metclass for inheritance by other classes.
        >> log:Log = rje.Log object
        >> cmd_list:List = List of commandline variables
        On intiation, this object:
        - sets the Log object (if any)
        - sets verbosity and interactive attributes
        - calls the _setAttributes() method to setup class attributes
        - calls the _cmdList() method to process relevant Commandline Parameters   
    Log(itime=time.time(),cmd_list=[]):
        - Handles log output; printing to log file and error reporting
        >> itime:float = initiation time
        >> cmd_list:list of commandline variables
    Info(prog='Unknown',vers='X',edit='??/??/??',desc='Python script',author='Unknown',ptime=None):
        - Stores intro information for a program.
        >> prog:str = program name
        >> vers:str = version number
        >> edit:str = last edit date
        >> desc:str = program description
        >> author:str = author name
        >> ptime:float = starting time of program, time.time()
    Out(cmd=[]):
        - Handles basic generic output to screen based on Verbosity and Interactivity for modules without classes.
        >> cmd:list = list of command-line arguments

Uses general modules: glob, math, os, random, re, resource, string, sys, time, traceback

### ~~~ Module rje_aaprop ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_aaprop.py] ~~~ ###

Module:       rje_aaprop
Description:  AA Property Matrix Module
Version:      0.2.0
Last Edit:    09/04/15
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Takes an amino acid property matrix file and reads into an AAPropMatrix object. Converts in an all by all property
    difference matrix. By default, gaps and Xs will be given null properties (None) unless part of input file.

Commandline:
    aaprop=FILE : Amino Acid property matrix file. [aaprop.txt]
    aagapdif=X  : Property difference given to amino acid vs gap comparisons [5]
    aanulldif=X : Property difference given to amino acid vs null values (e.g. X) [0.5]

Uses general modules: re, string, sys, time
Uses RJE modules: rje

### ~~~ Module rje_ancseq ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_ancseq.py] ~~~ ###

Module:       rje_ancseq
Description:  Ancestral Sequence Prediction Module
Version:      1.3
Last Edit:    24/07/13
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the objects and methods for ancestral sequence prediction. Currently, only GASP (Edwards & Shields
    2004) is implemented. Other methods may be incorporated in the future.

GASP Commandline:
    fixpam=X\t: PAM distance fixed to X [0].
    rarecut=X\t: Rare aa cut-off [0.05].
    fixup=T/F\t: Fix AAs on way up (keep probabilities) [True].
    fixdown=T/F\t: Fix AAs on initial pass down tree [False].
    ordered=T/F\t: Order ancestral sequence output by node number [False].
    pamtree=T/F\t: Calculate and output ancestral tree with PAM distances [True].
    desconly=T/F\t: Limits ancestral AAs to those found in descendants [True].
    xpass=X\t: How many extra passes to make down & up tree after initial GASP [1].

Classes:
    Gasp(log=None,cmd_list=[],tree=None,ancfile='gasp'):
        - Handles main GASP algorithm.
        >> log:Log = rje.Log object
        >> cmd_list:List = List of commandline variables
        >> tree:Tree = rje_tree.Tree Object
        >> ancfile:str = output filename (basefile))        
    GaspNode(realnode,alphabet,log):
        - Used by Gasp Class to handle specific node data during GASP.
        >> realnode:Node Object (rje_tree.py)
        >> alphabet:list of amino acids for use in GASP
        >> log:Log Object

Uses general modules: copy, sys, time
Uses RJE modules: rje, rje_pam

### ~~~ Module rje_biogrid ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_biogrid.py] ~~~ ###

Module:       rje_biogrid
Description:  BioGRID Database processing module
Version:      1.6
Last Edit:    07/05/10
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed primarily for parsing the plain text ORGANISM downloads from the BioGRID database. These
    have names in the form: BIOGRID-ORGANISM-Saccharomyces_cerevisiae-2.0.27.tab.txt.

    BioGRID tables contain useful information that can be used for cross-referencing to other sources, namely the protein
    names and gene symbols/aliases. The latter will be added to the dict['Mapping'] links dictionary of the BioGRID
    object, linking each symbol to the primary protein ID. These protein IDs will be used for storing the PPI data (in
    dict['PPI']) and extracting gene data from external sequence databases. These sequence databases need to be provided
    separately. This will be read in and added to the dict['Protein'] which will also store gene symbol data etc.

    The selection of sequence files might turn out to be quite tricky, as different species have very different protein
    identifiers used. I will add a list of recommended sequence sources as I find them:
    * Yeast = EnsLoci treatment of the EnsEMBL yeast genome

    BioGRID contains data for a number of experimental types. Those of interest can be specified with the ppitype=LIST
    option. Choices include: Affinity Capture-MS; Affinity Capture-Western; Biochemical Activity; Co-crystal Structure;
    Co-fractionation; Co-purification; Dosage Lethality; Dosage Rescue; Far Western; FRET; Phenotypic Enhancement;
    Phenotypic Suppression; Protein-peptide; Reconstituted Complex; Synthetic Growth Defect; Synthetic Lethality;
    Synthetic Rescue; Two-hybrid;

    IntAct has the following:  anti bait coip | pull down | two hybrid pooling | two hybrid | tap | x-ray diffraction |
    anti tag coip | fluorescence imaging | cosedimentation | elisa | protein kinase assay | coip | biochemical |
    antibody array | confocal microscopy | beta galactosidase | imaging techniques | two hybrid array |
    molecular sieving | ion exchange chrom | affinity chrom | protein array | enzymatic study | inferred by curator |
    far western blotting | spr | fps | phosphatase assay | fret | dhfr reconstruction | bn-page | peptide array | nmr |
    facs | affinity techniques | crosslink | itc | one hybrid | fluorescence | solution sedimentati | ch-ip | emsa |
    complementation | density sedimentatio | comig non denat gel | filter binding | chromatography | ub reconstruction |
    reverse phase chrom | emsa supershift | electron microscopy | protein crosslink | competition binding | mappit |
    gallex | gtpase assay | in gel kinase assay | spa | biophysical | radiolabeled methyl | experimental interac |
    fluorescence spectr | cd | bret | protein tri hybrid | transcription compl | deacetylase assay | footprinting |
    yeast display | saturation binding | protease assay | lambda phage | light scattering | htrf | fcs | toxcat |
    phage display | t7 phage | kinase htrf | methyltransferase as

    MINT txt downloads can also be parsed. Experiment types for MINT include:  affinity chromatography technologies |
    affinity technologies | anti bait coimmunoprecipitation | anti tag coimmunoprecipitation |
    beta galactosidase complementation | beta lactamase complementation | biochemical |
    bioluminescence resonance energy transfer | biophysical | chromatography technologies | circular dichroism |
    classical fluorescence spectroscopy | coimmunoprecipitation | colocalization by fluorescent probes cloning |
    colocalization by immunostaining | colocalization/visualisation technologies | competition binding | copurification |
    cosedimentation | cosedimentation in solution | cosedimentation through density gradients | cross-linking studies |
    electron microscopy | enzymatic studies | enzyme linked immunosorbent assay | experimental interaction detection |
    far western blotting | filter binding | fluorescence-activated cell sorting | fluorescence microscopy |
    fluorescence polarization spectroscopy | fluorescence technologies | fluorescent resonance energy transfer |
    gst pull down | his pull down | imaging techniques | isothermal titration calorimetry | lambda phage display |
    mass spectrometry studies of complexes | molecular sieving | nuclear magnetic resonance | peptide array |
    phage display | protease assay | protein array | protein complementation assay | protein kinase assay | pull down |
    saturation binding | surface plasmon resonance | t7 phage display | two hybrid | two hybrid array |
    two hybrid fragment pooling approach | two hybrid pooling approach | ubiquitin reconstruction | unknown |
    x-ray crystallography

    Reactome interactions are restricted to those of the "reaction" type. There are also "neighbouring_reaction" and
    "direct_complex" and "indirect_complex"

    DIP interactions are restricted to those with two uniprotkb IDs. DIP has similar annotation to MINT, with MI nos.

    Domino  interactions are restricted to those with two uniprotkb IDs. Has similar annotation to MINT, with MI nos.

Commandline:
    ### ~ BioGRID parsing and PPI Dataset Generation Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    ppifile=FILE    : PPI database flat file [None]
    seqin=FILE      : Sequence file containing protein sequences with appropriate Accession Numbers/IDs [None]
    genecards=FILE  : File of links between IDs. For human, should have HGNC and EnsLoci columns. [None]
    ppitype=LIST    : List of acceptable interaction types to parse out []
    badtype=LIST    : List of bad interaction types, to exclude [indirect_complex,neighbouring_reaction]
    symmetry=T/F    : Enforce symmetry in interaction datasets [True]
    dbsource=X      : Source database (biogrid/dip/intact/mint/reactome) [biogrid]
    mitab=T/F       : Whether source file is in MITAB flat file format [True]
    species=X       : Name of species to use data for (will be read from file if BioGRID) [human]
    taxid=LIST      : List of NCBI Taxa IDs to use (for DIP and Domino) [9606]
    unipath=PATH    : Path to UniProt files [UniProt/]

    ### ~ Output Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    ppifas=T/F      : Whether to output PPI datasets as fasta files into Species/BIOGRID_Datasets/ [True]
    minseq=X        : Minimum number of PPI sequences in order to output fasta file [3]
    ppitab=T/F      : Whether to output PPI table with aliases etc. [True]
    alltypes=T/F    : Output a full list of PPITypes. (Will populate the PPIType list) [False]

    ### ~ Special Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    hostvirus=T/F   : Whether to pull out host-virus interactions only (MINT/IntAct only) [False]
    vcodes=LIST     : List/File of viral species codes for IntAct hostvirus=T []
    
Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_seq, rje_dismatrix
Other modules needed: None

### ~~~ Module rje_blast ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_blast.py] ~~~ ###

Module:       rje_blast
Description:  BLAST Control Module
Version:      1.14
Last Edit:    20/08/12
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Performs BLAST searches and loads results into objects. Peforms GABLAM conversion of local alignments into global
    alignment statistics. Remember to set blastp=X:  blastx for DNA vs prot; tblastn for Prot vs DNA)

Objects:
    BLASTRun = Full BLAST run
    BLASTSearch = Information for a single Query search within a BLASTRun
    BLASTHit = Detailed Information for a single Query-Hit pair within BLASTRun
    PWAln = Detailed Information for each aligned section of a Query-Hit Pair

Commandline:
    blastpath=X     : path for blast files [c:/bioware/blast/] *Use fwd slashes (*Cerberus is special!)
    
    blastp=X        : BLAST program (BLAST -p X) [blastp]
    blasti=FILE     : Input file (BLAST -i FILE) [None]
    blastd=FILE     : BLAST database (BLAST -d FILE) [None]
    formatdb=T/F    : Whether to (re)format BLAST database [False]
    blasto=FILE     : Output file (BLAST -o FILE) [*.blast]

    blaste=X        : E-Value cut-off for BLAST searches (BLAST -e X) [1e-4]
    blastv=X        : Number of one-line hits per query (BLAST -v X) [500]
    blastb=X        : Number of hit alignments per query (BLAST -b X) [250]  

    blastf=T/F      : Complexity Filter (BLAST -F X) [True]
    blastcf=T/F     : Use BLAST Composition-based statistics (BLAST -C X) [False]
    blastg=T/F      : Gapped BLAST (BLAST -g X) [True]

    blasta=X        : Number of processors to use (BLAST -a X) [1]
    blastopt=FILE   : File containing raw BLAST options (applied after all others) []
    ignoredate=T/F  : Ignore date stamps when deciding whether to regenerate files [False]

    gablamfrag=X    : Length of gaps between mapped residue for fragmenting local hits [100]
    localcut=X      : Cut-off length for local alignments contributing to global GABLAM stats) [0]

Uses general modules: os, re, string, sys, time
Uses RJE modules: rje

### ~~~ Module rje_blast_V1 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_blast_V1.py] ~~~ ###

Module:       rje_blast
Description:  BLAST Control Module
Version:      1.15
Last Edit:    20/08/13
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Performs BLAST searches and loads results into objects. Peforms GABLAM conversion of local alignments into global
    alignment statistics. Remember to set blastp=X:  blastx for DNA vs prot; tblastn for Prot vs DNA)

Objects:
    BLASTRun = Full BLAST run
    BLASTSearch = Information for a single Query search within a BLASTRun
    BLASTHit = Detailed Information for a single Query-Hit pair within BLASTRun
    PWAln = Detailed Information for each aligned section of a Query-Hit Pair

Commandline:
    blastpath=X     : path for blast files [''] (Use fwd slashes)
    
    blastp=X        : BLAST program (BLAST -p X) [blastp]
    blasti=FILE     : Input file (BLAST -i FILE) [None]
    blastd=FILE     : BLAST database (BLAST -d FILE) [None]
    formatdb=T/F    : Whether to (re)format BLAST database [False]
    blasto=FILE     : Output file (BLAST -o FILE) [*.blast]

    blaste=X        : E-Value cut-off for BLAST searches (BLAST -e X) [1e-4]
    blastv=X        : Number of one-line hits per query (BLAST -v X) [500]
    blastb=X        : Number of hit alignments per query (BLAST -b X) [250]  

    blastf=T/F      : Complexity Filter (BLAST -F X) [True]
    blastcf=T/F     : Use BLAST Composition-based statistics (BLAST -C X) [False]
    blastg=T/F      : Gapped BLAST (BLAST -g X) [True]

    blasta=X        : Number of processors to use (BLAST -a X) [1]
    blastopt=FILE   : File containing raw BLAST options (applied after all others) []
    ignoredate=T/F  : Ignore date stamps when deciding whether to regenerate files [False]

    gablamfrag=X    : Length of gaps between mapped residue for fragmenting local hits [100]
    localcut=X      : Cut-off length for local alignments contributing to global GABLAM stats) [0]

Uses general modules: os, re, string, sys, time
Uses RJE modules: rje

### ~~~ Module rje_blast_V2 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_blast_V2.py] ~~~ ###

Module:       rje_blast
Description:  BLAST+ Control Module
Version:      2.18.0
Last Edit:    30/10/17
Copyright (C) 2013  Richard J. Edwards - See source code for GNU License Notice

Function:
    This is an updated BLAST module to utilise the improved BLAST+ library rather than the old Legacy BLAST. During the
    upgrade, other improvements are also being made to the module organisation in line with more recent tools in the
    SeqSuite package. In particular, BLAST Search, Hit and PWAln objects are being replaced by rje_Database tables and
    entries. This will allow greater flexibility in summary outputs for future. GABLAM statistics will also be entered
    directly into a Database table. To minimise memory requirements, these tables can be cleared as each BLAST result is
    read in if the data is not needed. This revised structure will also enable reading of tabular results from other
    searches as required in future.

    In Version 2.0, the old BLAST options are included but these will be upgraded to newer BLAST+ options. To run the
    old version, use BLASTRun.run(oldblast=True).

Commandline:
    ## Search Options ##    
    blastprog=X     : BLAST program to use. blastp=X also recognised. (BLAST -p X) [blastp]
    blasti=FILE     : Input file (BLAST -i FILE) [None]
    blastd=FILE     : BLAST database (BLAST -d FILE) [None]
    formatdb=T/F    : Whether to (re)format BLAST database [False]

    blaste=X        : E-Value cut-off for BLAST searches (BLAST -e X) [1e-4]
    blastv=X        : Number of one-line hits per query (BLAST -v X) [500]
    blastb=X        : Number of hit alignments per query (BLAST -b X) [500]
    tophits=X       : Sets max number of BLAST hits returned (blastb and blastv) [500]

    blastf=T/F      : Complexity Filter (BLAST -F X) [True]
    blastcf=T/F     : Use BLAST Composition-based statistics (BLAST -C X) [False]
    blastg=T/F      : Gapped BLAST (BLAST -g X) [True]
    softmask=T/F    : Whether to use soft masking for searches [True]

    blastopt=FILE   : File containing raw BLAST options (applied after all others) []

    ## Standalone Run Options ##
    savelocal=LIST  : Whether to generate extra output for the local BLAST hits table (GFF3/SAM/TDT/TDTSEQ) []
    reftype=X       : Whether to map SAM/GFF3 hits onto the Qry, Hit, Both or Combined [Hit]
    qassemblefas=T/F: Special mode for running with outfmt=4 and then converting to fasta file [False]
    qcomplete=T/F   : Whether the query sequence should be full-length in qassemblefas output [False]
    qconsensus=X    : Whether to convert QAssemble alignments to consensus sequences (None/Hit/Full) [None]
    qfasdir=PATH    : Output directory for QAssemble alignments [./QFAS/]

    ## GABLAM Parameters ##
    gablamfrag=X    : Length of gaps between mapped residue for fragmenting local hits [100]
    fragmerge=X     : Max Length of gaps between fragmented local hits to merge [0]
    localcut=X      : Cut-off length for local alignments contributing to global GABLAM stats) [0]
    localidcut=PERC : Cut-off local %identity for local alignments contributing to global GABLAM stats [0.0]
    qassemble=T/F   : Whether to fully assemble query stats from all hits [False]
    selfsum=T/F     : Whether to also include self hits in qassemble output [False] * qassemble must also be T *

    ## Output options ##
    blasto=FILE     : Output file (BLAST -o FILE) [*.blast]
    restab=LIST     : Whether to output summary results tables (Run/Search/Hit/Local/GABLAM) [Search,Hit]
    runfield=T/F    : Whether to include Run Field in summary tables. (Useful if appending.) [False]

    ## System Parameters ##
    blastpath=PATH  : Path to BLAST programs ['']
    blast+path=PATH : Path to BLAST+ programs (will use blastpath if not given) ['']
    legacy=T/F      : Whether to run in "legacy" mode using old BLAST commands etc. (Currently uses BLAST) [False]
    oldblast=T/F    : Whether to run with old BLAST programs rather than new BLAST+ ones [False]
    blasta=X        : Number of processors to use (BLAST -a X) [1]
    blastforce=T/F  : Whether to force regeneration of new BLAST results if already existing [False]
    ignoredate=T/F  : Ignore date stamps when deciding whether to regenerate files [False]
    gzip=T/F        : Whether to gzip (and gunzip) BLAST results files if keeping (not Windows) [True]

See also rje.py generic commandline options.

### ~~~ Module rje_conseq ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_conseq.py] ~~~ ###

Module:       rje_conseq
Description:  Sequence Conservation Methods
Version:      0.1
Last Edit:    18/05/06
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Sequence Conservation Methods.

Commandline:
    rank=T/F        : Whether to calculate ranks
    trimtrunc=T/F   : Whether to trim the leading and trailing gaps (within groups) -> change to X [False]
    winsize=X       : Window size for window scores

Uses general modules: math, os, string, sys, time
Uses RJE modules: rje

### ~~~~~ Module rje_db ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_db.py] ~~~~~ ###

Module:       rje_db
Description:  R Edwards Relational Database module
Version:      1.8.6
Last Edit:    06/11/17
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to read in and store data as a series of tables in a similar fashion to a database, e.g.
    MySQL. Although undoubtedly slower than such dedicated software for querying etc., this module is primarily designed
    for used to make links and manipulate data and tables on the fly in other python modules.

    The main Database Class will control the linking etc. of tables, which are in turn stored in the Table Class.

    NB. This module should not be confused with rje_dbase, which is for downloading and processing public databases.

Commandline:
    dbindex=T/F : Whether to run in "index" mode, storing a file position rather than all data (read only) !Not yet implemented! [False]
    Additional Commandline functionality will be added with time.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_dismatrix ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_dismatrix.py] ~~~ ###

Module:       rje_dismatrix
Description:  Distance Matrix Module 
Version:      1.0
Last Edit:    05/12/05
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    DisMatrix Class. Stores distance matrix data.

Commandline:
    outmatrix=X : Type for output matrix - text / mysql / phylip

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_dismatrix_V2 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_dismatrix_V2.py] ~~~ ###

Module:       rje_dismatrix
Description:  Distance Matrix Module 
Version:      2.10
Last Edit:    09/02/14
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    DisMatrix Class. Stores distance matrix data and contains methods for extra calculations, such as MST. This module
    is primarily for use within other modules but can be used for slmple distance matrix conversions and UPGMA tree
    construction.    

Commandline:
    loadmatrix=FILE : Loads a matrix from FILE [None]
    symmetric=T/F   : Whether the matrix should be symmetrical (e.g. DisAB = DisBA) [False]
    outmatrix=X     : Type for output matrix - text / mysql / phylip / png
    nsf2nwk=T/F     : Whether to convert extension for Newick Standard Format from nsf to nwk (for MEGA) [False]

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_dismatrix_V3 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_dismatrix_V3.py] ~~~ ###

Module:       rje_dismatrix
Description:  Distance Matrix Module 
Version:      3.0
Last Edit:    28/08/14
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    DisMatrix Class. Stores distance matrix data and contains methods for extra calculations, such as MST. This module
    is primarily for use within other modules but can be used for slmple distance matrix conversions and UPGMA tree
    construction.    

Commandline:
    loadmatrix=FILE : Loads a matrix from FILE [None]
    symmetric=T/F   : Whether the matrix should be symmetrical (e.g. DisAB = DisBA) [False]
    outmatrix=X     : Type for output matrix - text / mysql / phylip / png
    nsf2nwk=T/F     : Whether to convert extension for Newick Standard Format from nsf to nwk (for MEGA) [False]

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_disorder ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_disorder.py] ~~~ ###

Module:       rje_disorder
Description:  Disorder Prediction Module
Version:      0.8
Last Edit:    06/08/14
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module currently has limited function and no standalone capability, though this may be added with time. It is
    designed for use with other modules. The disorder Class can be given a sequence and will run the appropriate
    disorder prediction software and store disorder prediction results for use in other programs. The sequence will have
    any gaps removed.

    Currently four disorder prediction methods are implemented:
    * IUPred : Dosztanyi Z, Csizmok V, Tompa P & Simon I (2005). J. Mol. Biol. 347, 827-839. This has to be installed
    locally. It is available on request from the IUPred website and any use of results should cite the method. (See
    http://iupred.enzim.hu/index.html for more details.) IUPred returns a value for each residue, which by default,
    is determined to be disordered if > 0.5.
    * FoldIndex : This is run directly from the website (http://bioportal.weizmann.ac.il/fldbin/findex) and more simply
    returns a list of disordered regions. You must have a live web connection to use this method!
    * ANCHOR : Meszaros B, Simon I & Dosztanyi Z (2009). PLoS Comput Biol 5(5): e1000376. This has to be installed
    locally. It is available on request from the ANCHOR website and any use of results should cite the method. (See
    http://anchor.enzim.hu/ for more details.) ANCHOR returns a probability value for each residue, which by default,
    is determined to be disordered if > 0.5.
    * Parse: Parsed disorder from protein sequence name, e.g. DisProt download.
    #X-Y = disordered region; &X-Y = ordered region [0.0]

    For IUPred, the individual residue results are stored in Disorder.list['ResidueDisorder']. For both methods, the
    disordered regions are stored in Disorder.list['RegionDisorder'] as (start,stop) tuples.
    
Commandline:
    ### General Options ###
    disorder=X  : Disorder method to use (iupred/foldindex/anchor/parse) [iupred]
    iucut=X     : Cut-off for IUPred/ANCHOR results [0.2]
    iumethod=X  : IUPred method to use (long/short) [short]
    sequence=X  : Sequence to predict disorder for (autorun) []
    name=X      : Name of sequence to predict disorder for []
    minregion=X : Minimum length of an ordered/disordered region [0]

    ### System Settings ###
    iupath=PATH : The full path to the IUPred executable [c:/bioware/iupred/iupred.exe]
    anchor=PATH : Full path to ANCHOR executable []
    filoop=X    : Number of times to try connecting to FoldIndex server [10]
    fisleep=X   : Number of seconds to sleep between attempts [2]
    iuchdir=T/F : Whether to change to IUPred directory and run (True) or rely on IUPred_PATH env variable [False]

Uses general modules: copy, os, string, sys, time, urllib2
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_ensembl ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_ensembl.py] ~~~ ###

Module:       rje_ensembl
Description:  EnsEMBL Processing/Manipulation Module 
Version:      2.15.2
Last Edit:    20/04/15
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for processing EnsEMBL data for the rje_dbase module. The main class is an EnsEMBL class, which stores
    information on EnsEMBL proteins in terms of their gene IDs, loci and descriptions. This generates the "EnsLoci"
    dataset for each genome, consisting of the "best" peptide for a given locus. For known genes, UniProt accession
    numbers will be used in place of the EnsEMBL accession number. If the EnsEMBL sequence maps to a SwissProt sequence
    but is of really low quality (20+ consecutive Xs with less non-X residues than the SwissProt sequence) then the
    SwissProt sequence itself will replace the EnsEMBL sequence. This is the only time that the relationship between
    EnsEMBL peptide ID and sequence will break down.

    Version 1.7 introduced a new "EnsGO" function for making GO datasets for the species codes listed. This mode will
    need, for each SPECIES, the EnsLoci file enspath/ens_SPECIES.loci.fas, the GO mapping enspath/ens_SPECIES.GO.tdt
    and the GO ID file [GO.terms_ids_obs]. GO mapping files can be created for the relevant species using EnsEMBL's
    BioMart tool (http://www.ensembl.org/biomart/martview/), while the ID file can be downloaded from GO
    (http://www.geneontology.org/doc/GO.terms_ids_obs). From BioMart, the following columns should be downloaded:
    "Ensembl Gene ID","Ensembl Transcript ID","Ensembl Peptide ID","GO ID","GO description","GO evidence code",
    "EntrezGene ID","HGNC Symbol". Other fields can also be downloaded if desired. This function has been further
    updated in version 1.8 & 1.9. From Version 2.8, the columns should be: "Ensembl Gene ID", "Ensembl Transcript ID",
    "Ensembl Protein ID", "GO Term Accession", "GO Term Evidence Code", "EntrezGene ID", "HGNC symbol"

    Version 2.0 introduced a new "EnsDat" function for generating fake UniProt format entries for EnsLoci data using
    PFam HMM domain prediction, TMHMM transmembrane topology prediction, SIGNALP signal peptide prediction and IUPRED
    disorder prediction. Assumes that the EnsLoci files have been created. (Use download=T ensloci=T if not!) Sequences
    should be extracted from the file created by this method using Accession Numbers only.

    Version 2.11 is the start of a major reworking in preparation for V3.0. Species codes are now read in automatically
    and Ensembl species alone downloaded from Uniprot for EnsLoci processing. (This can be quite slow depending on
    connection etc.) This avoids the need for pre-processing Uniprot in order to make EnsLoci sequences. Modified Uniprot
    downloads and data extraction is used for db xref mapping in place of manual biomart tables. Species data is now
    split into subsets according to Ensembl sets (main, metazoa, protists etc.) and EnsLoci files are similarly split
    within an `ensloci/` subdirectory of `enspath/`.

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Primary Module Functions ###
    download=T/F    : Download EnsEMBL databases [False]
    makeuniprot=T/F : Whether to generate an Ensembl.dat file of UniProt entries for species [False]
    ensloci=T/F     : Create EnsEMBL datasets "reduced by loci" [False]
    enspep=T/F      : Create full gnspacc EnsEMBL peptide datasets [False]
    hgncmap=FILE    : File to be used for HGNC ID mapping []
    resume=X        : Species or species code to pickup run from [None]
    sections=LIST   : List of Ensembl sections to use for run (else All) []
    speclist=LIST   : List of species to use for run (else All) []
    chromspec=LIST  : List of species codes to download chromosomes for [HUMAN,DROME,CAEEL,YEAST,MOUSE,DANRE,CHICK,XENTR]
    speedskip=T/F   : Whether to assume download is fine if pep.all/cdna.all/dna.toplevel file found [True]
    ### Advanced UniProt Mapping Options ###
    mapstat=X       : GABLAM Stat to use for mapping assessment (ID/Sim/Len) [ID]
    automap=X       : Minimum value of mapstat for mapping to occur [80.0]
    unispec=FILE    : Alternative UniProt species file [None]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### EnsGO Options ###    
    ensgo=LIST      : List of species codes to make EnsGO Datasets for []
    mingo=X         : Minumum number of genes to output GO category [0]
    obsgo=T/F       : Whether to include obselete terms [False]
    splicego=T/F    : Whether to include all splice variants (EnsEMBL peptides) in GO datasets [False]
    goids=FILE      : File containing GO IDs [GO.terms_ids_obs]
    goevidence=LIST : List of acceptable GO evidence codes. (Will use all if blank.) []
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### EnsDat Options ###
    ensdat=LIST     : Perform EnsDat construction of predicted UniProt data for the species listed []
    tmhmm=FILE      : Path to TMHMM program [/home/richard/Bioware/TMHMM2.0c/bin/tmhmm]
    signalp=FILE    : Path to SIGNALP program [/home/richard/Bioware/signalp-3.0/signalp]
    hmmerpath=PATH  : Path for hmmer files [/home/richard/Bioware/hmmer-2.3.2/src/]
    pfam=FILE       : Path to PFam LS file [/home/richard/Databases/PFam/Pfam_ls]
    datpickup=FILE  : Text file containing names of proteins already processed (skip and append) [ensdat.txt]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### System Parameters ###
    enspath=PATH    : Path to EnsEMBL file [EnsEMBL/]
    unipath=PATH    : Path to UniProt files [enspath=PATH/uniprot/]
    specsleep=X     : Sleep for X seconds between species downloads [60]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

Uses general modules: copy, glob, urllib, os, string, sys, time
Uses RJE modules: rje, rje_uniprot
Other modules needed: rje_sequence

### ~~~ Module rje_forker ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_forker.py] ~~~ ###

Module:       rje_forker
Description:  Generic RJE Forking Module
Version:      0.0
Last Edit:    15/08/13
Copyright (C) 2013  Richard J. Edwards - See source code for GNU License Notice

Function:
    The primary function of this module is to provide a generic Forker class that can be used by other objects. This is
    loosely based on the IRIDIS Class of rje_iridis but with the difference that it does not fork out processes to other
    host nodes.

Commandline:

    ### ~ FORK CONTROL OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    forkdir=PATH    : Alternative directory for forking output. [`./`]
    tofork=LIST     : List of system commands to fork out (standalone Forker only) []
    iolimit=X       : Limit of number of IOErrors before termination [50]
    memfree=X       : Min. proportion of node memory to be free before spawning new fork [0.1]
    noforks=T/F     : Whether to avoid forks [False]
    forks=X         : Number of parallel sequences to process at once [0]
    killforks=X     : Number of seconds of no activity before killing all remaining forks. [36000]
    forksleep=X     : Sleep time (seconds) between cycles of forking out more process [0]
    rjepy=T/F       : Whether forked commands are rje Python commands [False]
    logfork=T/F     : Whether to log forking in main log [True]
    resfile=LIST    : List of results files (BASEFILE.X) that will need transferring []
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~ Module rje_genbank ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_genbank.py] ~~~ ###

Module:       rje_genbank
Description:  RJE GenBank Module
Version:      1.5.3
Last Edit:    18/12/17
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for parsing information out of GenBank files and converting them to other formats.

Input Options:
    seqin=FILE      : Input Genbank file []
    fetchuid=LIST   : Genbank retrieval to of a list of nucleotide entries to generate seqin=FILE []
    spcode=X        : Overwrite species read from file (if any!) with X [None]
    taxdir=PATH     : Path to taxonomy files for species code extraction. (Will not use if blank or None) [./SourceData/]
    addtags=T/F     : Add locus_tag identifiers if missing - needed for gene/cds/prot fasta output [False]

Output Options:
    basefile=FILE   : Root of output file names (same as input file by default) []
    tabout=T/F      : Delimited table output of features [False]
    features=LIST   : Subset of features to extract from Genbank file (blank for all) []
    details=LIST    : List of feature details to extract into own columns []
    detailskip=LIST : Subset of feature details to exclude from extraction [translation]
    fasout=LIST     : Types of sequences to output into files (full/gene/cds/prot) as *.*.fas []
    geneacc=X       : Feature detail to use for gene sequence accession number (added to details) [locus_tag]
    protacc=X       : Feature detail to use for protein sequence accession number (added to details) [protein_id]
    locusout=T/F    : Whether to generate output by locus (True, locus as basefile) or combined (False) [False]
    locusdir=PATH   : Directory in which to generate output by locus [./]
    
See also rje.py generic commandline options.

### ~~~ Module rje_genecards ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_genecards.py] ~~~ ###

Module:       rje_genecards
Description:  RJE Genecards Parsing Module 
Version:      0.4
Last Edit:    28/03/08
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

NOTE:
    This module has now been superceded somewhat by the rje_genemap module but is still used with rje_hprd to compile
    links from HPRD. This module may also still be of use for smaller sets of genes that need to me mapped to HGNC, e.g.
    manually compiled lists from experiments.

Function:
    This is a prototype module, which aims to take in a list of Gene Symbols and/or Aliases, find the relevant GeneCard
    entry, download it and extract the relevant protein gene/protein links into a table.

    The ultimate goal is to generate a table pulling in identifiers from EnsEMBL, GeneCards and HPRD to allow easy
    cross-referencing across datasets and compilation of data from different sources. When using the altsource=LIST
    option, subsequent files will overwrite the data read from files earlier in the list. If update=T and the cardout
    file exists, this will be appended to the altsource list.

    To save time, a full download of HGNC symbols can be downloaded from HGNC (http://www.genenames.org/index.html)
    and imported using the hgncdata=FILE option. This file should be delimited and contain the following fields (others
    are allowed):
    - HGNC ID, Approved Symbol, Approved Name, Previous Symbols, Aliases, Entrez Gene ID, RefSeq IDs,
    Entrez Gene ID (mapped data), OMIM, UniProt ID (mapped data), Ensembl ID (mapped data)

Commandline:
    ### Input Options ###
    genes=LIST      : List of gene symbols/aliases to download []
    update=T/F      : Whether to read in any data from cardout file (if present) and add to it [True]
    skiplist=LIST   : Skip genes matching LIST (e.g. XP_*) []
    useweb=T/F      : Whether to try and extract missing data from GeneCards website [True]
    altsource=LIST  : List of alternative sources of data (Delimited files with appropriate headers) []
    hgncdata=FILE   : HGNC download file containing data []

    ### Output Options ###
    species=X       : Species to output in table [Human]
    cardout=FILE    : File for output of genecard data [genecards.tdt]
    ensloci=FILE    : File of EnsLoci genome to incorporate [/home/richard/Databases/EnsEMBL/ens_HUMAN.loci.fas]
    restrict=T/F    : Whether to only output lines for gene in the original gene=LIST [False]
    purify=T/F      : Only output lines where the Alias and the Symbol are the same [False]

    ### Special execution options ###
    fullens=T/F     : Incorporate all EnsLoci EnsEMBL genes into cardout file (long run!) [False]
    fullhgnc=T/F    : Output all HGNC codes and unambiguous aliases into file [False]

Uses general modules: copy, glob, os, string, sys, time, urllib2
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_genemap ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_genemap.py] ~~~ ###

Module:       rje_genemap
Description:  RJE Gene & Database ID Mapping Module
Version:      1.5
Last Edit:    16/12/13
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to replace rje_genecards, which has become a bit unwieldy since its conception. Some of the
    original functions of rje_genecards will still be maintained by (hopefully) in a simplified format. Some of the
    additional mapping functions of Pingu will be added to this module for easier implementation across packages.

    The main functions of this module are:
    1. To map, store and retrieve database cross-references for a key dataset of gene IDs, usually HGNC symbols.
    2. To store a number of aliases for the key gene IDs, including old versions of accession numbers etc.
    3. To retrieve sequences from given datasets for stored aliases/genes.

    The main processing pipeline is as follows:
    1. Read in key data and generate data structure OR load pickle.
    2. Repeat 1 until all data/pickles integrated.
    3. Save pickle and output new data flat files, if desired.

    This is the limit of the standalone functionality of the program. However, the GeneMap class with have a number of
    additional methods for data retrieval by other programs that use it.

GeneMap Class:
    The GeneMap Class stores two main data dictionaries:
    1. A dictionary for the key Gene IDs that contains mappings to other databases.
    2. A dictionary that maps aliases onto other Gene IDs.

    In addition, sequence files may be loaded and used to map IDs onto Sequence objects.    

Key Input:
    There are four primary input files that are processed into the mapping:
    1. Designed for human data, an HGNC download file is one of the key input files. Headers will be converted into those
    from the original rje_genecards files, which are now replaced by sourcedata=FILE.
    2. Source Data files are delimited text files containing mapping to various databases. In each case, the first column
    should be unique for each line. This will be treated as an Alias. If a Symbol column is found, this will be treated
    as a key identifier (unless keyid=X has been changed).
    3. Alias files containing simple lists of ID:Alias to populate Alias dictionary. The first column can have any header
    but must be the identifier to map *to*. Another column must have the header "Aliases" and be a comma-separated list
    of aliases.
    4. Pickle data containing a pickled GeneMap object.

    In addition, sequence files may be loaded that have additional links and can be used to map to sequences. These are:
    1. EnsLoci = This is used to add additional mapping of genes to proteins and to EnsLoci protein IDs.

Input Processing:
    As data is loaded, either from a pickle or a text file, its data is integrated. NOTE: These commands can be repeated
    several times and, unlike normal, subsequent commands will not replace earlier ones but simply add to the list. If
    there is danger of additional unwanted commands in the command argument list, then the loadData() method should be
    called with a specified list of commandline options rather than using the default system arguments.

    If a given set of data has a "Symbol" (KeyID) Header then this is added to the main Data dictionary as a key and all
    column headers as stored data. (These column headers are stored in the "Header" list.) The Alias - the original key
    of the dictionary - is added to self.dict['Alias']. Note that each Alias can be involved in many-to-many and circular
    referencing, which will need to be dealt with by the class when mapping. And headers that are in the "XRef" list of
    database cross-references will also be added to the Alias dictionary. If there is no KeyID, the data is stored in a
    "TempData" dictionary, and XRef headers are aliased to the Alias rather than the KeyID.

    If a KeyID already exists in the Data dictionary, then any blank entries will be overwritten but data loaded from a
    previous file will not be. All aliases will be mapped to the KeyID, however, even if they do not end up in the Data
    dictionary itself. If a KeyID is missing from the Data dictionary but present in the TempData dictionary, it will be
    overwritten in the same way and moved to the Data dictionary. If an Alias without a KeyID is already present in the
    TempData dictionary, the same will happen without any transferral.

    Sequence data will be processed according to the specifics of the type of sequence file it is. EnsLoci sequences
    will be converted into an EnsLoci dictionary of {ID:Sequence} but also key gene-protein mappings will be extracted.
    The protein to gene aliases will be added to the Alias dictionary, while the EnsLoci ID will be added as an XRef to
    the appropriate Data or TempData dictionary element.

    Once all data has been read in, each TempData entry will be assessed using the Alias mappings to see if it, or any of
    its XRef entries, is an alias for a KeyID. If so, its data will be combined with that in Data and it will be removed
    from TempData. If not, it will be assessed for being an alias of another TempData entry and will be combined if so.
    After this final stage of processing, any entries still in TempData will be promoted to KeyIDs and added to the main
    dictionary, though they will not appear in the "KeyID" list.

Commandline:
    ### ~ Input Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    hgncdata=FILE   : Download file containing HGNC data. []
    mgidata=FILE    : Download file containing MGI data (ftp://ftp.informatics.jax.org/pub/reports/MGI_MouseHumanSequence.rpt) []
    sourcedata=FILE : File containing data in order of preference regarding conflicting data. []
    aliases=FILE    : Files containing aliases only. []
    pickledata=FILE : Genemap pickle to import and use. []
    ensloci=FILE    : File of EnsLoci genome to incorporate [None]
    genepickle=FILE : Use pickle of GeneMap data without additional loading/processing etc. [None]
    pfamdata=FILE   : Delimited files containing domain organisation of sequences [None]
    approved=LIST   : Approved HGNC gene symbols to avoid over-zealous alias mapping (will add to from HGNC) []

    ### ~ Processing Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    keyid=X         : Key field header to be used in main Data dictionary - aliases map to this [Symbol]
    xref=LIST       : Headers in Data dictionaries that are used for aliases [EnsEMBL,Entrez,HGNC,HPRD,UniProt]
    useweb=T/F      : Whether to try and extract missing data from GeneCards website [False]
    skiplist=LIST   : Skip genes matching LIST when using GeneCards website (e.g. XP_*) ['HPRD*']
    
    ### ~ Output Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : Root for output files [genemap]
    flatout=T/F     : Whether to output flatfiles (*.data.tdt & *.aliases.tdt) [False]
    pickleout=T/F   : Whether to output pickle (*.pickle.gz) [False]

Uses general modules: copy, glob, pickle, os, string, sys, time, urllib2
Uses RJE modules: rje, rje_seq, rje_zen
Other modules needed: None

### ~~~~~ Module rje_go ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_go.py] ~~~~~ ###

Module:       rje_go
Description:  Gene Ontology Parsing/Manipulation Module
Version:      1.2
Last Edit:    05/05/10
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    Parse the OBO V1.2 text download from GO into a data structure for querying by other programs, making GO Slims and
    mapping parent terms etc. Download input file from http://www.geneontology.org/GO.downloads.ontology.shtml. If no
    file is given, will attempt to download and parse http://www.geneontology.org/ontology/gene_ontology_edit.obo.

Commandline:
    obofile=FILE    : Input GO OBO V1.2 download [None]
    webobo=T/F      : Whether to download from GO website if file not given [True]
    goslim=LIST     : List of GO IDs to form basis of GO Slim []
    parentterms=LIST: Terms relating to parent relationships ['is_a','part_of']
    ensgopath=PATH  : Path to EnsGO files   (!!! Restricted to Humans Currently !!!)

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~~ Module rje_haq ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_haq.py] ~~~~ ###

Module:       rje_haq
Description:  Homologue Alignment Quality module
Version:      1.3
Last Edit:    21/02/07
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    HAQ of HAQESAC: SAQ and PAQ methods from haqesac.

    NB. The classes in this module are designed to take sequence objects and perform analyses, not create sequence objects
    themselves if none are given. The module haqesac.py will do this.

Commandline:
    noquery=T/F : No Query for SAQ, Random Query for PAQ
    saqc=X      : Min no. seqs to share residue in SAQ. [2]
    saqb=X      : SAQ Block length. [10]
    saqm=X      : No. residues to match in SAQ Block. [7]
    saqks=X     : Relative Weighting of keeping Sequences in SAQ. [3]
    saqkl=X     : Relative Weighting of keeping Length in SAQ. [1]
    mansaq=T/F  : Manual over-ride of sequence rejection decisions in SAQ [False]
    paqb=X      : PAQ Block length. [7]
    paqm=X      : No. residues to match in PAQ Block. [3]
    paqks=X     : Relative Weighting of keeping Sequences in PAQ. [3]
    paqkl=X     : Relative Weighting of keeping Length in PAQ. [1]
    manpaq=T/F  : Manual over-ride of sequence rejection decisions in PAQ [False]
    anchors=T/F	: Whether to use conserved 'anchors' to extend well-aligned regions in PAQ	[True]

Uses general modules: copy, os, random, sys, time
Uses RJE modules: rje

### ~~~ Module rje_hmm_V1 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_hmm_V1.py] ~~~ ###

Module:       rje_hmm
Description:  HMMer Control Module
Version:      1.3
Last Edit:    25/11/08
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to perform basic HMM functions using the HMMer program. Currently, there are three functions
    that may be performed, separately or consecutively:
    * 1. Use hmmbuild to construct HMMs from input sequence files
    * 2. Search a sequence database with HMMs files
    * 3. Convert HMMer output into a delimited text file of results.

Commandline:
    ## Build Options ##
    makehmm=LIST        : Sequence file(s). Can include wildcards [None]
    hmmcalibrate=T/F    : Whether to calibrate HMM files once made [True]

    ## Search Options ##    
    hmm=LIST        : HMM file(s). Can include wildcards. [*.hmm]
    searchdb=FILE   : Fasta file to search with HMMs [None]
    hmmoptions=LIST : List or file of additional HMMer search options (joined by whitespace) []
    hmmpfam=T/F     : Performs standard HMMer PFam search (--cut_ga) (or processes if present) [False]
    hmmout=FILE     : Pipe results of HMM searches into FILE [None]
    hmmres=LIST     : List of HMM search results files to convert (wildcards allowed) []
    hmmtab=FILE     : Delimited table of results ('None' to skip) [searchdb.tdt]
    cleanres=T/F    : Option to reduce size of HMM results file by removing no-hit sequences [True]

    ## System Parameters ##
    hmmerpath=PATH  : Path for hmmer files [/home/richard/Bioware/hmmer-2.3.2/src/] 
    force=T/F       : Whether to force regeneration of new HMMer results if already existing [False]
    gzip=T/F        : Whether to gzip (and gunzip) HMMer results files (not Windows) [True]
    
Classes:
    HMMRun Object = Full HMM run
    HMMSearch Object = Information for a single Query search within a BLASTRun
    HMMHit Object = Detailed Information for a single Query-Hit pair within BLASTRun
    rje_blast.PWAln Object = Detailed Information for each aligned section of a Query-Hit Pair

Uses general modules: glob, os, re, string, sys, time
Uses RJE modules: rje, rje_blast

### ~~~~ Module rje_hpc ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_hpc.py] ~~~~ ###

Module:       rje_hpc
Description:  High Performance Computing job farming
Version:      1.1
Last Edit:    17/06/14
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the generic code for farming jobs out over multiple processors and/or nodes. It is based on
    rje_iridis Version 1.10, updated to the new RJE_Object structure and made more generic. The recommended tool for
    actual job farming is SLiMFarmer, which inherits the rje_hpc.JobFarmer object and adds extra bells and whistles.

Commandline:
    ### ~ Generic HPC Job Farming pptions ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    subsleep=X      : Sleep time (seconds) between cycles of subbing out jobs to hosts [1]
    subjobs=LIST    : List of subjobs to farm out to HPC cluster []
    hpcmode=X       : Mode to be used for farming jobs between nodes (rsh/fork) [fork]
    iolimit=X       : Limit of number of IOErrors before termination [50]
    memfree=X       : Min. proportion of node memory to be free before spawning job [0.0]
    keepfree=X      : Number of processors to keep free on head node [1]

    ### ~ SLiMSuite and SeqSuite SeqBySeq program options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqbyseq=T/F    : Activate seqbyseq mode - assumes basefile=X option used for output [False]
    farm=X          : Program to farm out using seqbyseq mode. []
    seqin=FILE      : Input sequence file to farm out [None]
    basefile=X      : Base for output files - compiled from individual run results [None]
    outlist=LIST    : List of extensions of outputs to add to basefile for output (basefile.*) []
    pickhead=X      : Header to extract from OutList file and used to populate AccNum to skip []
    startfrom=X     : Sequence ID at which to begin the SeqBySeq farming [None]
    rjepy=T/F       : Whether program is an RJE *.py script (adds log processing) [True]
    jobini=FILE     : Ini file to pass to the called program [None]
    pypath=PATH     : Path to python modules ['/home/re1u06/Serpentry/']

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~ Module rje_hprd ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_hprd.py] ~~~ ###

Module:       rje_hprd
Description:  HPRD Database processing module
Version:      1.2.1
Last Edit:    16/05/16
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed for specific PPI Database manipulations:

    1. Parsing HPRD Flat Files. [This is the default and is run if no other option is selected.]
    Upon downloading the HPRD FLAT_FILES, this module can be run to parse out binary protein interactions and make
    sequence-specific interaction datasets. Currently, it is not clear how (if at all) the "isoform" data is used in
    HPRD for distinguishing interactions, so all isoform_1 sequences will be used for Fasta datasets. Sequences will be
    reformatted into:

    >Gene_HUMAN__AccNum Description [Gene:WWWW HPRD:XXXX; gb:YYYY; sp:ZZZZ]

    The Gene will be the HUGO gene name (as parsed from HPRD) where available, else it will be the HPRD ID. This will be
    unique for each protein and will correspond to a dataset of the same name: HPRD_Datasets/Gene_hprd.fas. All proteins
    will also be saved in a file hprd.fas. The AccNum will be UniProt if possible, else GenBank. If the option alliso=T
    is used, then all isoforms will be included and the AccNum will be X-Y where X is the HPRD ID and Y is the isoform.

    2. Converting a table of interactions into a distance matrix.
    This table should be a plain text file in which the first column is the interacting protein name and the subsequent
    columns are for the proteins (hubs) to be clustered. The first row contains their name. The rows for each spoke
    protein should be empty (or value 0) if there is no interaction and have a non-zero value if there is an interaction:
    Gene	Beta	Epsilon	Eta	Gamma	Sigma	Theta	Zeta
    AANAT							1
    
    A distance matrix is then produced (outfile=FILE => FILE.ppi_dis.txt) consisting of the number of unique interactors
    for each pairwise comparison. (The format is set by outmatrix=X : text / mysql / phylip)

    3. Incorporation of data from the GeneCards website (and Human EnsLoci) using rje_genecards. This will create a file
    called HPRD.genecards.tdt by default but this can be over-ridden using cardout=FILE. EnsLoci data will also be looked
    for in /home/richard/Databases/EnsEMBL/ens_HUMAN.loci.fas but this can be over-ridden with ensloci=FILE.

Commandline:
    ### HRPD Options ###
    hprdpath=PATH   : Path to HPRD Flat Files [./]
    genecards=T/F   : Make the HRPD.genecards.tdt file using rje_genecards (and its options) [False]
    hprdfas=T/F     : Whether to generate HPRD fasta files [False]
    alliso=T/F      : Whether to include all isoforms in the output [False]
    ppitype=LIST    : List of acceptable interaction types to parse out [in vitro;in vivo;yeast 2-hybrid]
    badtype=LIST    : List of bad interaction types, to exclude []
    domainfas=T/F   : Whether to output Domain fasta files [False]
    complexfas=T/F  : Whether to output Protein Complex fasta files [False]
    outdir=PATH     : The output directory for the files produced [./]

    ### Distance Matrix Options ###    
    ppitab=FILE     : File containing PPI data (see 2 above)
    scaled=T/F      : Whether distance matrix is to be scaled by total number of interactors in pairwise comparison [F]
    
Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_genecards, rje_ppi, rje_seq, rje_dismatrix
Other modules needed: None

### ~~~ Module rje_html ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_html.py] ~~~ ###

Module:       RJE_HTML
Description:  Module for generating HTML 
Version:      0.2.1
Last Edit:    28/01/15
Copyright (C) 2010  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is primarily for general methods for making HTML pages for other modules. 

Commandline:
    stylesheets=LIST    : List of CSS files to use ['http://www.slimsuite.unsw.edu.au/stylesheets/slimhtml.css']
    tabber=FILE         : Tabber javascript file location ['tabber.js']
    border=X            : Border setting for tables [0]
    nobots=T/F          : Whether to add code to avoid Google Bots [True]
    analytics=X         : Google Analytics code to use with pages []
    javascript=PATH     : Path to javascript files for tabs etc. ['http://www.southampton.ac.uk/~re1u06/javascript/']
    keywords=LIST       : List of keywords to put in page metadata []
    title=X             : Default title for HTML page []
    copyright=X         : Copyright statement for page ['RJ Edwards 2015']

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_slim, rje_uniprot, rje_zen
Other modules needed: None

### ~~~ Module rje_iridis ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_iridis.py] ~~~ ###

Module:       RJE_IRIDIS
Description:  Parallel processing on IRIDIS
Version:      1.10.2
Last Edit:    29/11/15
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to control and execute parallel processing jobs on the IRIDIS cluster based on the script
    written by Ivan Wolton. Initially, it will call other programs but, in time, it is envisaged that other programs will
    make use of this module and have parallelisation built-in.

    In SeqBySeq mode, the program assumes that seqin=FILE and basefile=X are given and irun states the program to be run.
    Seqin will then be worked through in turn and each sequence farmed out to the irun program. Outputs given by OutList
    are then compiled, as is the Log, into the correct basefile=X given. In the case of *.csv and *.tdt files, the header
    row is copied for the first file and then excluded for all subsequent files. For all other files extensions, the
    whole output is copied.

Commandline:
    ### ~ STANDARD RUN OPTIONS ~ ###
    irun=X          : Exectute a special iRun analysis on Iridis (gopher/slimfinder/qslimfinder/slimsearch/unifake) []
    iini=FILE       : Ini file to pass to the called program [None]
    pypath=PATH     : Path to python modules ['/home/re1u06/Serpentry/']
    rjepy=T/F       : Whether program is an RJE *.py script (adds log processing) [True]
    subsleep=X      : Sleep time (seconds) between cycles of subbing out jobs to hosts [1]
    subjobs=LIST    : List of subjobs to farm out to IRIDIS cluster []
    iolimit=X       : Limit of number of IOErrors before termination [50]
    memfree=X       : Min. proportion of node memory to be free before spawning job [0.0]
    test=T/F        : Whether to produce extra output in "test" mode [False]
    keepfree=X      : Number of processors to keep free on head node [1]
    rsh=T/F         : Whether to use rsh to run jobs on other nodes [True]

    ### ~ SEQBYSEQ OPTIONS ~ ###
    seqbyseq=T/F    : Activate seqbyseq mode - assumes basefile=X option used for output [False]
    seqin=FILE      : Input sequence file to farm out [None]
    basefile=X      : Base for output files - compiled from individual run results [None]
    outlist=LIST    : List of extensions of outputs to add to basefile for output (basefile.*) []
    pickup=X        : Header to extract from OutList file and used to populate AccNum to skip []

    ### ~ SPECIAL iRUN OPTIONS ~ ###
    runid=X         : Text identifier for iX run [None]
    resfile=FILE    : Main output file for iX run [islimfinder.csv]
    sortrun=T/F     : Whether to sort input files by size and run big -> small to avoid hang at end [True]
    loadbalance=T/F : Whether to split SortRun jobs equally between large & small to avoid memory issues [True]
    
Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_markov ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_markov.py] ~~~ ###

Module:       rje_markov
Description:  RJE Module for protein sequence Markov Chain related gubbins
Version:      2.2
Last Edit:    17/01/13
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to handle a number of things to do with amino acid frequencies and the like, including:
    - Observed and expected 1mer .. Xmer frequencies

    Some old functions including 1mer .. Xmer counts per protein are now only found in V1.2.

Commandline:
    ## General ##
    seqin=FILE          : File with input sequences [None]
    alphabet=X,Y,..Z    : List of letters in alphabet of interest
    split=X             : Splits file into numbered files of X sequences and recombines at end.
    autoload=T/F        : Whether to load sequences automatically. If False, will try memory-efficient seqlist handling [False]
    aafreq=FILE         : Generate expected 1mer frequencies from FILE of aafreq [None]
    xmerfile=FILE       : File from which to load Xmer counts and build suffix tree [None]
    direction=X         : Direction to read chains (fwd/bwd/both) [fwd]
    sorted=T/F          : Whether to use sorted xmer fragments to reduce memory requirements [False] 
    scap=T/F            : Whether to use special SCAP sorting of xmers (sorts all but last aa) [False]
    negvpos=X           : Perform Negatives versus Positives analysis on X.* files [None]

    ## Output ##
    markov=T/F      : Whether to perform Markov Chain Analysis of observed vs Expected for Xmers [True]
    xmers=X[,Y]     : Deal with Xmers [upto Ymers] [1]

Uses general modules: copy, os, re, string, sys, time
Uses RJE modules: rje, rje_seq
Other modules needed: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_uniprot

### ~~~ Module rje_mascot ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_mascot.py] ~~~ ###

Module:       rje_mascot
Description:  Module for loading, manipulating and saving MASCOT data for BUDAPEST and PICSI
Version:      1.2
Last Edit:    27/02/13
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    The main function of this module is to read in CSV-exported data from MASCOT and then save it in formats that can be
    more readily utilised and interrogated by other programs.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    mascot=FILE     : Name of MASCOT csv file [None]
    itraq=T/F       : Whether data is from an iTRAQ experiment [False]
    empai=T/F       : Whether emPAI data is present in MASCOT file [True]
    samples=LIST    : List of X:Y, where X is an iTRAQ isotag and Y is a sample []
    
See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_obj, rje_zen
Other modules needed: None

### ~~~ Module rje_menu ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_menu.py] ~~~ ###

Module:       rje_menu
Description:  Generic Menu Methods Module
Version:      0.5.0
Last Edit:    08/11/16
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to contain generic menu methods for use with any RJE Object. At least, that's the plan...

Commandline:
    This module is not for standalone running and has no commandline options (including 'help'). All options are handled
    by the parent module.

Uses general modules: os, string, sys
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_mitab ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_mitab.py] ~~~ ###

Module:       rje_mitab
Description:  RJE MITAB File Parser
Version:      0.2.1
Last Edit:    17/05/16
Copyright (C) 2015  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is for parsing PPI data from a MITAB file into a pairwise PPI file, using rje_xref.XRef to map protein
    sequences onto the KeyID of XRef and a Uniprot field set by unifield=X. It is best to use an XRef table generated by
    PINGU as this includes a "Secondary" field of Uniprot secondary accession numbers.

    The complex=LIST setting is used to read in iRef-style MITAB files that have complex:XXX identifiers as one of the
    interactors. These will be kept during the initial parsing of the file and then replaced with ALL members to generate
    a complete set of all-by-all pairwise PPI for the complex.

Commandline:
    ### ~ Input Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    mitab=FILE          : MITAB interaction file []
    unifield=X          : Uniprot accession number field identifier for xrefdata ['Uniprot']
    dbsource=X          : Source database for evidence field ['mitab']
    idfield=LIST        : Gene/protein identifier fields to look for []
    mapdb=LIST          : Restricted list of database identifier types to try mapping to xref KeyID/MapFields []
    taxafield=LIST      : Taxon identifier fields to look for []
    methodfield=LIST    : PPI detection method fields to look for []
    typefield=LIST      : PPI type fields to look for []
    ### ~ Parsing Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    taxid=LIST          : List of NCBI Taxa IDs to use [9606]
    symmetry=T/F        : Whether to impose hub/spoke symmetry on parsed PPI [True]
    complex=LIST        : Complex identifier prefixes to expand from mapped PPI [rigid]
    splicevar=T/F       : Whether to allow splice variants in parsed Uniprot identifiers [False]
    unionly=T/F         : Whether to restrict PPI to pairwise with UniProt IDs [False]
    adduni=T/F          : Whether to add Uniprot IDs that are not returned from mapping file [True]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~ Module rje_motif_V3 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_motif_V3.py] ~~~ ###

Module:       rje_motif
Description:  Motif Class and Methods Module
Version:      3.1
Last Edit:    04/06/14
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the Motif class for use with both Slim Pickings and PRESTO, and associated methods. This basic
    Motif class stores its pattern in several forms:
    - info['Sequence'] stores the original pattern given to the Motif object
    - list['PRESTO'] stores the pattern in a list of PRESTO format elements, where each element is a discrete part of
      the motif pattern
    - list['Variants'] stores simple strings of all the basic variants - length and ambiguity - for indentifying the "best"
      variant for any given match
    - dict['Search'] stores the actual regular expression variants used for searching, which has a separate entry for
      each length variant - otherwise Python RegExp gets confused! Keys for this dictionary relate to the number of
      mismatches allowed in each variant.

    The Motif Class is designed for use with the MotifList class. When a motif is added to a MotifList object, the
    Motif.format() command is called, which generates the 'PRESTO' list. After this - assuming it is to be kept -
    Motif.makeVariants() makes the 'Variants' list. If creating a motif object in another module, these method should be
    called before any sequence searching is performed. If mismatches are being used, the Motif.misMatches() method must
    also be called.

Commandline:
    These options should be listed in the docstring of the module using the motif class:
    - alphabet=LIST     : List of letters in alphabet of interest [AAs]
    - ambcut=X          : Cut-off for max number of choices in ambiguous position to be shown as variant (0=All) [10]
    - trimx=T/F         : Trims Xs from the ends of a motif [False]

Uses general modules: copy, math, os, re, string, sys
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_motif_stats ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_motif_stats.py] ~~~ ###

Module:       rje_motif_stats
Description:  Motif Statistics Methods Module
Version:      1.0
Last Edit:    01/02/07
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the Alignment Conservation methods for motifs, as well as other calculations needing occurrence
    data. This module is designed to be used by the MotifList class, which contains the relevant commandline options.

Commandline:
    This module is not for standalone running and has no commandline options (including 'help'). All options are handled
    by the parent module.

Uses general modules: copy, os, string, sys
Uses RJE modules: gopher_V2, rje, rje_blast, rje_disorder, rje_motif_V3, rje_seq, rje_sequence
Other modules needed: rje_seq modules

### ~~~ Module rje_motiflist ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_motiflist.py] ~~~ ###

Module:       rje_motiflist
Description:  RJE Motif List Module
Version:      1.0
Last Edit:    03/04/07
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the MotifList Class, which is designed to replace many of the functions that previously formed
    part of the Presto Class. This class will then be used by PRESTO, SLiMPickings and CompariMotif (and others?) to
    control Motif loading, redundancy and storage. MotifOcc objects will replace the previous PrestoSeqHit objects and
    contain improved data commenting and retrieval methods. The MotifList class will contain methods for filtering motifs
    according to individual or combined MotifOcc data.

    The options below should be read in by the MotifList object when it is instanced with a cmd_list and therefore do not
    need to be part of any class that makes use of this object unless it has conflicting settings.

    The Motif Stats options are used by MotifList to calculate statistics for motif occurrences, though this data will
    actually be stored in the MotifOcc objects themselves. This includes conservation statistics.

    Note. Additional output parameters, such as motifaln and proteinaln settings, and stat filtering/novel scores are not
    stored in this object, as they will be largely dependent on the main programs using the class, and the output from
    those programs. (This also enables statfilters etc. to be used with stats not related to motifs and their occurrences
    if desired.)

MotifList Commands:
    ## Basic Motif Input/Formatting Parameters ##
    motifs=FILE     : File of input motifs/peptides [None]
                      Single line per motif format = 'Name Sequence #Comments' (Comments are optional and ignored)
                      Alternative formats include fasta, SLiMDisc output and raw motif lists.
    minpep=X        : Min length of motif/peptide X aa [2]
    minfix=X        : Min number of fixed positions for a motif to contain [0]
    minic=X         : Min information content for a motif (1 fixed position = 1.0) [2.0]
    trimx=T/F       : Trims Xs from the ends of a motif [False]
    nrmotif=T/F     : Whether to remove redundancy in input motifs [False]
    minimotif=T/F   : Input file is in minimotif format and will be reformatted (PRESTO File format only) [False]
    goodmotif=LIST  : List of text to match in Motif names to keep (can have wildcards) []
    ambcut=X        : Cut-off for max number of choices in ambiguous position to be shown as variant [10]
    reverse=T/F     : Reverse the motifs - good for generating a test comparison data set [False]
    msms=T/F        : Whether to include MSMS ambiguities when formatting motifs [False]

    ## Motif Occurrence Statistics Options ##
    winsa=X         : Number of aa to extend Surface Accessibility calculation either side of motif [0]
    winhyd=X        : Number of aa to extend Eisenberg Hydrophobicity calculation either side of motif [0]
    windis=X        : Extend disorder statistic X aa either side of motif (use flanks *only* if negative) [0]
    winchg=X        : Extend charge calculations (if any) to X aa either side of motif [0]
    winsize=X       : Sets all of the above window sizes (use flanks *only* if negative) [0]
    slimchg=T/F     : Calculate Asolute, Net and Balance charge statistics (above) for occurrences [False]
    iupred=T/F      : Run IUPred disorder prediction [False]
    foldindex=T/F   : Run FoldIndex disorder prediction [False]
    iucut=X         : Cut-off for IUPred results (0.0 will report mean IUPred score) [0.0]
    iumethod=X      : IUPred method to use (long/short) [short]
    domfilter=FILE  : Use the DomFilter options, reading domains from FILE [None] ?? Check how this works ??
    ftout=T/F       : Make a file of UniProt features for extracted parent proteins, where possible, incoroprating SLIMs [*.features.tdt]
    percentile=X    : Percentile steps to return in addition to mean [0]

    ## Conservation Parameters ##   ??? Add separate SlimCons option ???
    usealn=T/F      : Whether to search for and use alignemnts where present. [False]
    gopher=T/F      : Use GOPHER to generate missing orthologue alignments in alndir - see gopher.py options [False]
    alndir=PATH     : Path to alignments of proteins containing motifs [./] * Use forward slashes (/)
    alnext=X        : File extension of alignment files, accnum.X [aln.fas]
    alngap=T/F      : Whether to count proteins in alignments that have 100% gaps over motif (True) or (False) ignore
                      as putative sequence fragments [False]  (NB. All X regions are ignored as sequence errors.)
    conspec=LIST    : List of species codes for conservation analysis. Can be name of file containing list. [None]
    conscore=X      : Type of conservation score used:  [pos]
                        - abs = absolute conservation of motif using RegExp over matched region
                        - pos = positional conservation: each position treated independently 
                        - prop = conservation of amino acid properties
                        - all = all three methods for comparison purposes
    consamb=T/F     : Whether to calculate conservation allowing for degeneracy of motif (True) or of fixed variant (False) [True]
    consinfo=T/F    : Weight positions by information content (does nothing for conscore=abs) [True]
    consweight=X    : Weight given to global percentage identity for conservation, given more weight to closer sequences [0]
                        - 0 gives equal weighting to all. Negative values will upweight distant sequences.
    posmatrix=FILE  : Score matrix for amino acid combinations used in pos weighting. (conscore=pos builds from propmatrix) [None]
    aaprop=FILE     : Amino Acid property matrix file. [aaprop.txt]

    ## Alignment Settings ##
    protalndir=PATH : Output path for Protein Alignments [ProteinAln/]
    motalndir=PATH  : Output path for Motif Alignments []
    flanksize=X     : Size of sequence flanks for motifs [30]
    xdivide=X       : Size of dividing Xs between motifs [10]

    ## System Settings ##
    iupath=PATH     : The full path to the IUPred exectuable [c:/bioware/iupred/iupred.exe]
    ?? memsaver=T/F    : Whether to store all results in Objects (False) or clear as search proceeds (True) [True] ??
    ?- should this be controlled purely by the calling program? Probably!
    fullforce=T/F   : Whether to force regeneration of alignments using GOPHER

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_aaprop, rje_disorder, rje_motif_V3, rje_motif_cons, rje_scoring, rje_seq, rje_sequence,
    rje_blast, rje_uniprot
Other modules needed: rje_dismatrix, 

### ~~~ Module rje_motifocc ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_motifocc.py] ~~~ ###

Module:       rje_motifocc
Description:  Motif Occurrence Module
Version:      0.0
Last Edit:    29/01/07
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the MotifOcc class. This class if for storing methods and attributes pertinent to an individual
    occurrence of a motif, i.e. one Motif instance in one sequence at one position. This class is loosely based on (and
    should replace) the old PRESTO PrestoHit object. (And, to some extent, the PrestoSeqHit object.) This class is
    designed to be flexible for use with PRESTO, SLiMPickings and CompariMotif, among others.

    In addition to storing the standard info and stat dictionaries, this object will store a "Data" dictionary, which
    contains the (program-specific) data to be output for a given motif. All data will be in string format. The
    getData() and getStat() methods will automatically convert from string to numerics as needed.

Commandline:
    This module has no standalone functionality and should not be called from the commandline.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_seq, rje_sequence
Other modules needed: None

### ~~~~ Module rje_obj ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_obj.py] ~~~~ ###

Module:       rje_obj
Description:  Contains revised General Object templates for Rich Edwards scripts and bioinformatics programs
Version:      2.2.2
Last Edit:    19/01/17
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains updated Classes for use by all RJE scripts and bioinformatics programs. With time, all
    programs should be migrated over to the new objects. Miscellaneous methods will, for the most part, be kept in
    the rje module.

    Commandline options are all in the form X=Y. Where Y is to include spaces, use X="Y". Commands in the form "-X Y"
    will also be recognised.

General Commandline:
    v=X             : Sets verbosity (-1 for silent, 0 for no progress counters) [1]
    i=X             : Sets interactivity (-1 for full auto) [0]
    log=FILE        : Redirect log to FILE [Default = calling_program.log]
    newlog=T/F      : Create new log file. [Default = False: append log file]
    silent=T/F      : If set to True will not write to screen or log. [False]
    errorlog=FILE   : If given, will write errors to an additional error file. [None]
    help            : Print help to screen

Program-Specific Commands: (Some programs only)
    basefile=FILE   : This will set the 'root' filename for output files (FILE.*), including the log
    outfile=FILE    : This will set the 'root' filename for output files (FILE.*), excluding the log
    delimit=X       : Sets standard delimiter for results output files [\t]
    mysql=T/F       : MySQL output
    append=T/F      : Append to results files rather than overwrite [False]
    force=T/F       : Force to regenerate data rather than keep old results [False]
    backups=T/F     : Whether to generate backup files (True) or just overwrite without asking (False) [True]
    maxbin=X        : Maximum number of trials for using binomial (else use Poisson) [-]
    memsaver=T/F    : Some modules will have a memsaver option to save memory usage [False]

System Commandline:
    win32=T/F       : Run in Win32 Mode [False]
    osx=T/F         : Run in MacOSX Mode [False]
    pwin            : Run in PythonWin (** Must be 'commandline', not in ini file! **)
    runpath=PATH    : Run program from given path (log files and some programs only) [path called from]
    rpath=PATH      : Path to installation of R ['R']
    webserver=T/F   : Trigger webserver run and output [False]
    soaplab=T/F     : Implement special options/defaults for SoapLab implementations [False]
    rest=X          : Variable that sets the output to be returned by REST services [None]
    screenwrap=X    : Maximum width for some screen outputs [200]

Forking Commandline:
    noforks=T/F     : Whether to avoid forks [False]
    forks=X         : Number of parallel sequences to process at once [0]
    killforks=X     : Number of seconds of no activity before killing all remaining forks. [36000]

Development Commandline:
    debug=T/F       : Turn on additional debugging prints and prompts [False]
    warn=T/F        : Turn on program integrity check warnings (unless silent) [True]
    test=T/F        : Run additional testing methods and/or produce additional test outputs [False]
    dev=T/F         : Run development-specific code. (Added to keep main coding working during dev) [False]

Classes:
    RJE_Object(log=None,cmd_list=[]):
        - Metclass for inheritance by other classes.
        >> log:Log = rje.Log object
        >> cmd_list:List = List of commandline variables
        On intiation, this object:
        - sets the Log object (if any)
        - sets verbosity and interactive attributes
        - calls the _setAttributes() method to setup class attributes
        - calls the _cmdList() method to process relevant Commandline Parameters
    Log(itime=time.time(),cmd_list=[]):
        - Handles log output; printing to log file and error reporting
        >> itime:float = initiation time
        >> cmd_list:list of commandline variables

Uses general modules: glob, math, os, random, re, string, sys, time, traceback

### ~~~~ Module rje_pam ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_pam.py] ~~~~ ###

Module:       rje_pam
Description:  Contains Objects for PAM matrices
Version:      1.2.1
Last Edit:    27/11/15
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module handles functions associated with PAM matrices. A PAM1 matrix is read from the given input file and
    multiplied by itself to give PAM matrices corresponding to greater evolutionary distance. (PAM1 equates to one amino acid
    substitition per 100aa of sequence.) 

Commandline:
    pamfile=X   : Sets PAM1 input file [jones.pam]
    pammax=X    : Initial maximum PAM matrix to generate [100]
    pamcut=X    : Absolute maximum PAM matrix [1000]

Alternative PAM matrix commands:
    altpam=FILE : Alternative to PAM file input = matrix needing scaling by aafreq [None]
    seqin=FILE  : Sequence file from which to calculate AA freq for scaling [None]
    pamout=FILE : Name for rescaled PAM matrix output [*.pam named after altpam=FILE]

Classes:
    PamCtrl(rje.RJE_Object):
        - Controls a set of PAM matrices.
    PAM(pam,rawpamp,alpha):
        - Individual PAM matrix.
        >> pam:int = PAM distance
        >> rawpamp:dictionary of substitution probabilities
        >> alpha:list of amino acids (alphabet)

Uses general modules: os, string, sys, time
Uses RJE modules: rje

### ~~~~ Module rje_ppi ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_ppi.py] ~~~~ ###

Module:       RJE_PPI
Description:  RJE Protein-Protein Interaction Module
Version:      2.8.1
Last Edit:    07/01/15
Copyright (C) 2010  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is contains methods for manipulating protein-protein interaction (PPI) dictionaries. Database-specific
    classes and methods can be found in rje_hprd and rje_biogrid, while broader-scale functionality is found in the
    PINGU application. With time, generic PINGU functions should be migrated to this module and used by PINGU.
    
    The main purpose of rje_ppi is for use within other applications but there is also some standalone functionality
    for reading in a pairwise PPI file of delimited data with "Hub", "Spoke" and "Evidence" columns.
    
    Data is stored as an unannotated graph {Hub:[Spokes]} or an annotated graph {Hub:{Spoke:Evidence}} and rje_db 
    Database tables that store additional Node and Edge data. Edge data is read directly from PPI Pairwise input, with
    'Hub' and 'Edge' fields that combine to make the key. The Edge table will have an "Evidence" field and optional 
    additional fields. The Node table will have a 'Node' field as the key (that matches the 'Hub' or 'Spoke' from the
    Edge table). 

Commandline:
    ### Input Options ###
    pairwise=FILE   : Input PPI pairwise file, containing Hub, Spoke and optional Evidence columns []
    nodelist=LIST   : Reduce input PPI to given Node list []
    nodemap=X       : Try to map Nodes first using Node table field X []
    expandppi=X     : Expand reduced Node list by X PPI levels [0]
    combineppi=LIST : List of pairwise PPI files (using same ID set) to import. Uses Evidence field or filename []
    nodefields=LIST : List of alternative A/B fields to replace Hub and Spoke fields [SYMBOL_,Gene_]
    ppisym=T/F      : Whether to enforce Hub/Spoke symmetry [True]

    ### Output/Processing Options ###
    tabout=T/F      : Output PPI data as Node and Edge tables [False]
    fragment=T/F    : Perform PPI fragmentation [False]
    fragsize=X      : Combine smaller fragments upto fragsize [200]
    minfrag=X       : Minimum fragment size to keep [3]
    mcode=T/F       : Perform MCODE clustering [False]
    xgmml=T/F       : Output network in XGMML style [False]
    xdir=PATH       : Directory for XGMML output [./XGMML]

    ### Layout Options ###
    layout=X        : Layout to be used for XGMML output [spring]
    walltime=X      : Walltime (hours) for layouts [0.02]
    damping=X       : Force Directed Layout, damping parameter [0.9]
    colbydeg=T/F    : Whether to colour PNG output by node degree [False]
    nudgecyc=X      : Number of cycles between node nudges (try to bump out of unstable equilibria) [1000]

    ### MCODE Options ###
    haircut=T/F     : Whether to perform "haircut" on MCODE complexes [False]
    multicut=T/F    : Whether to perform "haircut" on MCODE complexes for the purposes of looking at nodes 2+ times [True]
    fluff=X         : MCODE "fluff" threshold. <0 = No Fluff [0.5]
    vwp=X           : MCODE vertex weighting percentage [0.2]
    mink=X          : MCODE min k-core values [2]
    mindeg=X        : MCODE min degree for node scoring [2]

See also rje.py generic commandline options.

Uses general modules: copy, glob, math, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_xgmml, rje_zen
Other modules needed: None

### ~~~ Module rje_qsub ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_qsub.py] ~~~ ###

Module:       rje_qsub
Description:  QSub Generating module
Version:      1.9.1
Last Edit:    02/11/17
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to make a job file and call it with qsub. Walltime and depend commands are included. 

Commandline:
    - program=X     : Program call for Qsub (and options) [None]
    - job=X         : Name of job file (.job added) [qsub]
    - qpath=PATH    : Path to change directory too [current path]
    - rjepy=T/F     : Whether program is an RJE *.py script (adds python PyPath/) [True]
    - pypath=PATH   : Path for RJE Python scripts [/rhome/re1u06/Serpentry/]
    - nodes=X       : Number of nodes to run on [4]
    - ppn=X         : Processors per node [12]
    - walltime=X    : Walltime for qsub job (hours) [60]
    - depend=LIST   : List of job ids to wait for before starting job (dependhpc=X added) []
    - pause=X       : Wait X seconds before attempting showstart [5]
    - report=T/F    : Pull out running job IDs and run showstart [False]
    - email=X       : Email address to email job stats to at end ['']
    - mailstart=T/F : Whether to email user at start of run [False]
    - hpc=X         : Name of HPC system for depend ['IRIDIS4']
    - dependhpc=X   : Name of HPC system for depend ['blue30.iridis.soton.ac.uk']
    - vmem=X        : Virtual Memory limit for run (GB) [48]
    - modules=LIST  : List of modules to add in job file []
    - modpurge=T/F  : Whether to purge loaded modules in qsub job file prior to loading [True]
    - precall=LIST  : List of additional commands to run between module loading and program call []

Uses general modules: copy, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_samtools ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_samtools.py] ~~~ ###

Module:       rje_samtools
Description:  RJE SAMtools parser and processor
Version:      1.19.2
Last Edit:    26/11/17
Copyright (C) 2013  Richard J. Edwards - See source code for GNU License Notice

Function:
    The initial function of this program is for calling/assessing genetic changes following MPileup mapping of multiple
    short read datasets against the same reference genome (provided as BAM files). The MPileup files should be generated
    by piping the output of the following into a file (*.mpileup):

    samtools mpileup -BQ0 -d10000000 -f <Ref Genome Fasta> <BAM file>

    Initial parsing and filtering converts the MPileup format into a delimited text file with quality score and read
    depth filtering, converting mapped read data into an allele list in the form: allele1:count|allele2:count|... These
    will be sorted by frequency, so that allele1 is the "major allele". Output of the *.QX.tdt file will have the fields:
    * Locus = Reference locus (contig/chromosome)
    * Pos = Position in reference (1-L)
    * Ref = Reference sequence
    * N = Total read depth at that position
    * QN = Read depth after quality filtering
    * Seq = Mapped (filtered) allele list in the form: allele1:count|allele2:count|...

SNP Frequency Calculations:
    A second function of this tool is to compare the SNP frequencies of two populations/datasets and identify major
    alleles in the "Treatment" that have significantly increased in frequency compared to the "Control". This mode takes
    two pileup files as control=FILE and treatment=FILE. These file names (minus extension) will be output fields. For
    something more user-friendly, use `labels=X,Y` to give them better labels (where X is control and Y is treatment).
    These file names will also be used to set the output files `CONTROL.vs.TREATMENT` unless `basefile=X` is set.

    Parsed pileup files (see above) are read in and combined. Only locus positions with entries in both files (i.e. both
    meet the `qcut=X` and `minqn=X` criteria) are kept and base calls combined. Any alleles failing to meet the minimum
    count criteria (mincut=X) are removed. If `mincut` < 1.0, it is treated as a proportion of all reads, with a minimum
    value set by `absmincut=X`. The exception to this allele removal is the Reference allele, unless `ignoreref=T`. By
    default, `N` alleles are removed and do not contribute to overall read counts for allele frequency calculations. This
    can be changed by setting `ignoren=F`. Filtered SNPs are output to `*.snp.tdt`. Unless `ignoreref=T`, fixed
    differences to the Reference (i.e. 100% frequency in both control and treatment) are also output.

    The focus of analysis is the "major allele" for the treatment population, which is the allele with highest frequency.
    (When tied, alleles will be arbitrarily selected in alphabetical order). The frequency of  the major allele in the
    treatment (`MajFreq`) is output along with the difference in frequency of this allele relative to the control
    (`MajDiff`). A positive value indicates that the allele is at higher frequency in the treatment, whereas a negative
    value means higher frequency in the control. Finally, the probability of the treatment frequency is calculated using
    a binomial distribution, where: p is probability of a read being the major allele based on the control frequency (if
    zero, this is conservatively set to 1/(N+1), i.e. assuming that the next control read would be that allele); k is the
    number of major allele treatment reads and n is the total number of treatment reads.

    The next stage is to filter these SNPs and calculate FDR statistics. By default, only positions with non-reference
    major treatment alleles are considered. This can be switched off with `majmut=F` or reversed with `majmut=F` and
    `majref=T` in combination. Following this filtering, the total
    number of SNPs is used for an FDR correction of the `MajProb` probabilities, using the Benjamini & Hochberg approach,
    where FDR = pN/n. To speed this up and reduce output, a probability cutoff can be applied with `sigcut=X` (0.05 by
    default). To only keep positions where the treatment major allele is different to the control major allele, use
    `majdif=T`. `majcut=X` will add an additional min. frequency criterion. These are applied after FDR correction.
    Output is then further reduced to those SNPs where `MajDiff` > 0 and filtered with a final FDR cutoff (`fdrcut=X`).
    Remaining SNPs are output to the `*.fdr.tdt` table.

    Optionally, a table of SNP annotation (snptable=FILE) can be merged with the `*.fdr.tdt` output. This table must have
    `Locus` and `Pos` fields. If it has other fields that are required to identify unique entries, these should be set
    with `snptabkeys=LIST`.

Read Coverage Analysis:
    Version 1.8.0 added read coverage analysis, which will calculate average depth of coverage for each Locus based on a
    SAM or mpileup file, or a Read ID file (`*.rid.tdt`) previously generated by `rje_samtools`. This file is given by
    `readcheck=FILE`. If the source sequence file is also given using `seqin=FASFILE` then the true sequence lengths are
    used for the calculation. Otherwise, the last position covered by a read in the `readcheck` file is used. Read depths
    are output to `*.coverage.tdt`.

    A table of regions to be checked can be provided using `checkpos=FILE`. This should have `Locus`, `Start` and `End`
    fields for the regions to be checked. Reads from `readcheck` will be scanned to (a) identify reads completely
    spanning the region, and (b) calculate the mean depth of coverage for that region. Reads spanning the region must
    also cover a number of nucleotides flanking the region, as set by `checkflanks=LIST`. Each flanking distance in the
    list will have its own `SpanX` output field. For example, the default settings check 0 flanks (just the region), plus
    flanks of 100, 500 and 1000 nt. This generates output fields `Span0`, `Span100`, `Span500` and `Span1000`.

    Alternative `Locus`, `Start` and `End` fields for `checkpos=FILE` can be given with `checkfields=LIST`. NOTE: This is
    case-sensitive and needs all three elements, even if only one or two fields are being changed.

    If `depthplot=T` then read depth will be calculated across each locus and additional depth statistics (MinX, MaxX,
    MedianX and Coverage) will be added to the coverage file. If `rgraphics=T` (default) and R is installed, plots per
    locus will be generated. If `readlen=T` then additional outputs will be generated of the maximum read length at each
    point in the genome.

    If `dirlen=X` > 0 (default=500), the maximum 5' and 3' read linkage will be calculated every `dirlen=X` nucleotides
    (plus the end position). The examines the reads spanning that position and reports the maximum distance any read
    extends in the 5' (Len5) or 3' (Len3) direction from that position. This can identify misassemblies resulting in
    breaks in read coverage. Note that reads in this context are contiguous SAM/pileup hits and so sequencing reads with
    multiple or split mapping will be treated as multiple reads.

Commandline:
    ### ~ MPileup Parsing Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    batch=FILELIST  : List of MPileup/SAM files to be parsed and filtered (e.g. *.pileup) []
    qcut=X          : Min. quality score for a call to include [30]
    minqn=X         : Min. number of reads meeting qcut (QN) for output [10]
    rid=T/F         : Whether to include Read ID (number) lists for each allele [True]
    snponly=T/F     : Whether to restrict parsing output to SNP positions (will use mincut settings below) [False]
    indels=T/F      : Whether to include indels in "SNP" parsing [True]
    skiploci=LIST   : List of loci to exclude from pileup parsing (e.g. mitochondria) []
    snptableout=T/F : Output filtered alleles to SNP Table [False]
    ### ~ SNP Frequency Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    control=FILE    : MPileup or processed TDT file to be used for control SNP frequencies []
    treatment=FILE  : MPileup or processed TDT file to be used for treatment SNP frequencies []
    labels=X,Y      : Optional labels for Control and Treatment fields in output (other file basename used) []
    mincut=X        : Minimum read count for minor allele (proportion if <1) [1]
    absmincut=X     : Absolute minimum read count for minor allele (used if mincut<1) [2]
    biallelic=T/F   : Whether to restrict SNPs to pure biallelic SNPs (two alleles meeting mincut) [False]
    ignoren=T/F     : Whether to exclude "N" calls from alleles [True]
    ignoreref=T/F   : If False will always keep Reference allele and call fixed change as SNP [False]
    basefile=X      : Basename for frequency comparison output [<CONTROLBASE>.v.<TREATMENTBASE>]
    majfocus=T/F    : Whether the focus is on Major Alleles (True) or Mutant/Reference Alleles (False) [True]
    majdif=T/F      : Whether to restrict output and stats to positions with Major Allele differences in sample [False]
    majmut=T/F      : Whether to restrict output and stats to positions with non-reference Major Allele [True]
    majref=T/F      : Whether to restrict output and stats to positions with reference Major Allele (if majmut=F) [False]
    majcut=X        : Frequency cutoff for Major allele [0.0]
    sigcut=X        : Significance cutoff for enriched treatment SNPs [0.05]
    fdrcut=X        : Additional FDR cutoff for enriched treatment SNPs [1.0]
    snptable=FILE   : Table of SNPs of cross-reference with FDR SNP output []
    snptabkeys=LIST : Fields that make unique key entries for snptable (with Locus, Pos) []
    snptabmap=X,Y   : Optional SNPTable fields to replace for mapping onto FDR Locus,Pos fields (Locus,Pos) []
    rgraphics=T/F   : Whether to generate snpfreq multichromosome plots [True]
    ### ~ Double Genome Analysis ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    altcontrol=FILE     : MPileup or processed TDT file to be used for control SNP frequencies in Alt genome []
    alttreatment=FILE   : MPileup or processed TDT file to be used for treatment SNP frequencies in Alt genome []
    ### ~ Read Coverage Analysis ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    readcheck=FILE      : SAM/Pileup/RID file with read mappings [None]
    seqin=FASFILE       : Sequence file for loci in MPileup/SAM files (e.g. matching all relevant RID files) [None]
    depthplot=T/F       : Whether to generate Xdepth plot data for the readcheck FILE. (May be slow!) [False]
    fullcut=X           : Proportion of read to be mapped to count as full-length [0.9]
    readlen=T/F         : Include read length data for the readcheck file (if depthplot=T) [True]
    dirnlen=X           : Include directional read length data at X bp intervals (readlen=T; 0=OFF) [500]
    depthsmooth=X       : Smooth out any read plateaus < X nucleotides in length [200]
    peaksmooth=X        : Smooth out Xcoverage peaks < X depth difference to flanks (<1 = %Median) [0.05]
    rgraphics=T/F       : Whether to generate PNG graphics using R. (Needs R installed and setup) [True]
    checkpos=TDTFILE    : File of Locus, Start, End positions for read coverage checking [None]
    checkfields=LIST    : Fields in checkpos file to give Locus, Start and End for checking [Locus,Start,End]
    checkflanks=LIST    : List of lengths flanking check regions that must also be spanned by reads [0,100,500,1000]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_obj, rje_zen
Other modules needed: None

### ~~~ Module rje_samtools_V0 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_samtools_V0.py] ~~~ ###

Module:       rje_samtools_V0
Description:  RJE SAMtools parser and processor
Version:      0.2.0
Last Edit:    21/12/15
Copyright (C) 2013  Richard J. Edwards - See source code for GNU License Notice

Function:
    The initial function of this program is for calling/assessing genetic changes following MPileup mapping of a
    wildtype (e.g. ancestor) and mutant (BAM file) against the same reference genome. The MPileup files should be
    generated by piping the output of the following into a file:

    samtools mpileup -BQ0 -d10000000 -f <Ref Genome Fasta> <BAM file>

    NOTE: This version has been "frozen" as V0. Development continuing with V1.0.0.

Commandline:
    wtpileup=FILE   : MPileup results for wildtype genome resequencing [None]
    mutpileup=FILE  : MPileup results for mutant genome resequencing [None]
    qcut=X          : Min. quality score for a call to include [40]
    ignoren=T/F     : Whether to exclude "N" calls for major/minor alleles [True]
    minfreq=X       : Minor allele(s) frequency correction for zero counts (e.g. Sequencing error) [0.001]
    majdif=T/F      : Whether to restrict output and stats to positions with Major Allele differences [True]
    majmut=T/F      : Whether to restrict output and stats to positions with non-reference Major Allele [False]
    snptables=LIST  : Existing SNPs of interest to be cross-referenced with the WT and Mutant SNPs []

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_obj, rje_zen
Other modules needed: None

### ~~~ Module rje_scoring ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_scoring.py] ~~~ ###

Module:       rje_scoring
Description:  Scoring and Ranking Methods for RJE Python Modules
Version:      0.0
Last Edit:    22/01/07
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains methods only for ranking, filtering and generating new scores from python dictionaries. At its
    conception, this is for unifying and clarifying the new scoring and filtering options used by PRESTO & SLiMPickings,
    though it is conceived that the methods will also be suitable for use in other/future programs.

    The general format of expected data is a list of column headers, on which data may be filtered/ranked etc. or
    combined to make new scores, and a dictionary containing the data for a given entry. The keys for the dictionary
    should match the headers in a *case-insensitive* fashion. (The keys and headers will not be changed but will match
    without using case, so do not have two case-sensitive variables, such as "A" and "a" unless they have the same
    values.) !NB! For some methods, the case should have been matched.

    Methods in this module will either return the input dictionary or list with additional elements (if calculating new
    scores) or take a list of data dictionaries and return a ranked or filtered list.

    Methods in this module:
    * setupStatFilter(callobj,statlist,filterlist) = Makes StatFilter dictionary from statlist and filterlist
    * statFilter(callobj,data,statfilter) = Filters data dictionary according to statfilter dictionary.
    * setupCustomScores(callobj,statlist,scorelist,scoredict) = Checks and returns Custom Scores and related lists
    
Commandline:
    This module is not for standalone running and has no commandline options (including 'help'). All options are handled
    by the parent modules.

Uses general modules: copy, os, string, sys
Uses RJE modules: rje
Other modules needed: None

### ~~~~ Module rje_seq ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_seq.py] ~~~~ ###

Program:      RJE_SEQ
Description:  DNA/Protein sequence list module
Version:      3.24.0
Last Edit:    02/12/15
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Contains Classes and methods for sets of DNA and protein sequences. 

Sequence Input/Output Options: 
    seqin=FILE      : Loads sequences from FILE (fasta,phylip,aln,uniprot or fastacmd names from fasdb) [None]
    query=X         : Selects query sequence by name [None]
    acclist=LIST    : Extract only AccNums in list. LIST can be FILE or list of AccNums X,Y,.. [None]
    fasdb=FILE      : Fasta format database to extract sequences from [None]
    mapseq=FILE     : Maps sequences from FILE to sequences of same name [None]
    mapdna=FILE     : Map DNA sequences from FILE onto sequences of same name in protein alignment [None]
    seqout=FILE     : Saves 'tidied' sequences to FILE after loading and manipulations [None]
    reformat=X      : Outputs sequence in a particular format, where X is:
                     - fasta/fas/phylip/scanseq/acclist/speclist/acc/idlist/fastacmd/teiresias/mysql/nexus/3rf/6rf/est6rf     [None]
                     - if no seqout=FILE given, will use input file name as base and add appropriate exension.
    #!# reformat=X may not be fully implemented. Report erroneous behaviour! #!#
    logrem=T/F      : Whether to log removed sequences [True] - suggest False with filtering of large files!

Sequence Loading/Formatting Options:
    gnspacc=T/F     : Convert sequence names into gene_SPECIES__AccNum format wherever possible. [False]
    alphabet=LIST   : Alphabet allowed in sequences [standard 1 letter AA codes]
    replacechar=T/F : Whether to remove numbers and replace characters not found in the given alphabet with 'X' [True]
    autofilter=T/F  : Whether to automatically apply sequence filters etc. upon loading sequence [True]
    autoload=T/F    : Whether to automatically load sequences upon initiating object [True]
    memsaver=T/F    : Minimise memory usage. Input sequences must be fasta. [False]
    degap=T/F       : Degaps each sequence [False]
    tidygap=T/F     : Removes any columns from alignments that are 100% gap [True]
    ntrim=X         : Trims of regions >= X proportion N bases (X residues for protein) [0.0]
    seqtype=X       : Force program to read as DNA, RNA, Protein or Mixed (case insensitive; read=Will work it out) [None]
    dna=T/F         : Alternative identification of sequences as DNA [False]
    mixed=T/F       : Whether to allow auto-identification of mixed sequences types (else uses first seq only) [False]
    align=T/F       : Whether the sequences should be aligned. Will align if unaligned. [False]
    rna2dna=T/F     : Converts RNA to DNA [False]
    trunc=X         : Truncates each sequence to the first X aa. (Last X aa if -ve) (Useful for webservers like SingalP.) [0]
    usecase=T/F     : Whether to output sequences in mixed case rather than converting all to upper case [False]
    case=LIST       : List of positions to switch case, starting with first lower case (e.g case=20,-20 will have ends UC) []
    countspec=T/F   : Generate counts of different species and output in log [False]

Sequence Filtering Options:
    filterout=FILE  : Saves filtered sequences (as fasta) into FILE. *NOTE: File is appended if append=T* [None]
    minlen=X        : Minimum length of sequences [0]
    maxlen=X        : Maximum length of sequences (<=0 = No maximum) [0]
    maxgap=X        : Maximum proportion of sequence that may be gaps (<=0 = No maximum) [0]
    maxx=X          : Maximum proportion of sequence that may be Xs (<=0 = No maximum; >=1 = Absolute no.) [0]
    maxglob=X       : Maximum proportion of sequence predicted to be ordered (<=0 = None; >=1 = Absolute) [0]
    minorf=X        : Minimum ORF length for a DNA/EST translation (reformatting only) [0]
    minpoly=X       : Minimum length of poly-A tail for 3rf / 6rf EST translation (reformatting only) [20]
    gapfilter=T/F   : Whether to filter gappy sequences upon loading [True]
    nosplice=T/F    : If nosplice=T, UniProt splice variants will be filtered out [False]
    dblist=LIST     : List of databases in order of preference (good to bad)
                      [sprot,ipi,uniprot,trembl,ens_known,ens_novel,ens_scan]
    dbonly=T/F      : Whether to only allow sequences from listed databases [False]
    unkspec=T/F     : Whether sequences of unknown species are allowed [True]
    accnr=T/F       : Check for redundant Accession Numbers/Names on loading sequences. [True]
    seqnr=T/F       : Make sequences Non-Redundant [False]
    nrid=X          : %Identity cut-off for Non-Redundancy (GABLAMO) [100.0]
    nrsim=X         : %Similarity cut-off for Non-Redundancy (GABLAMO) [None]      
    nralign=T/F     : Use ALIGN for non-redundancy calculations rather than GABLAMO [False]
    specnr=T/F      : Non-Redundancy within same species only [False]
    querynr=T/F     : Perform Non-Redundancy on Query species (True) or limit to non-Query species (False) [True]
    nrkeepann=T/F   : Append annotation of redundant sequences onto NR sequences [False]
    goodX=LIST      : Filters where only sequences meeting the requirement of LIST are kept.
                      LIST may be a list X,Y,..,Z or a FILE which contains a list [None]
                        - goodacc  = list of accession numbers
                        - goodseq  = list of sequence names
                        - goodspec = list of species codes
                        - gooddb   = list of source databases
                        - gooddesc = list of terms that, at least one of which must be in description line
    badX=LIST       : As goodX but excludes rather than retains filtered sequences

System Info Options:
    * Use forward slashes for paths (/)
    blastpath=PATH  : Path to BLAST programs ['']           
    blast+path=PATH : Path to BLAST+ programs ['']           
    fastapath=PATH  : Path to FASTA programs ['']          
    clustalw=PATH   : Path to CLUSTALW program ['clustalw']
    clustalo=PATH   : Path to CLUSTAL Omega alignment program ['clustalo']
    mafft=PATH      : Path to MAFFT alignment program ['mafft']
    muscle=PATH     : Path to MUSCLE alignment program ['muscle']
    fsa=PATH        : Path to FSA alignment program ['fsa']            
    pagan=PATH      : Path to PAGAN alignment program ['pagan']            
    win32=T/F       : Run in Win32 Mode [False]
    alnprog=X       : Choice of alignment program to use (clustalw/clustalo/muscle/mafft/fsa/pagan) [clustalo]
    
Sequence Manipulation/Function Options:
    pamdis      : Makes an all by all PAM distance matrix
    split=X     : Splits file into numbered files of X sequences. (Useful for webservers like TMHMM.)
    relcons=FILE: Returns a file containing Pos AbsCons RelCons [None]
    relconwin=X : Window size for relative conservation scoring [30]
    makepng=T/F : Whether to make RelCons PNG files [False]
    seqname=X   : Output sequence names for PNG files etc. (short/Name/Number/AccNum/ID) [short]

DisMatrix Options:
    outmatrix=X : Type for output matrix - text / mysql / phylip

Special Options:
    blast2fas=FILE1,FILE2,...,FILEn : Will blast sequences against list of databases and compile a fasta file of results per query
        - use options from rje_blast.py for each individual blast (blastd=FILE will be over-ridden)
        - saves results in AccNum.blast.fas and will append existing files!
    keepblast=T/F   : Whether to keep BLAST results files for blast2fas searches [True]
    haqbat=FILE     : Generate a batch file (FILE) to run HAQESAC on generated BLAST files, with seqin as queries [None]

Classes:
    SeqList(rje.RJE_Object):     
        - Sequence List Class. Holds a list of Sequence Objects and has methods for manipulation etc.
    Sequence(rje_sequence.Sequence):     
        - Individual Sequence Class.    
    DisMatrix(rje_dismatrix.DisMatrix):     
        - Sequence Distance Matrix Class.

### ~~~ Module rje_seqlist ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_seqlist.py] ~~~ ###

Module:       rje_seqlist
Description:  RJE Nucleotide and Protein Sequence List Object (Revised)
Version:      1.25.0
Last Edit:    08/05/17
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to replace rje_seq. The scale of projects has grown substantially, and rje_seq cannot deal
    well with large datasets. An important feature of rje_seqlist.SeqList objects, therefore, is to offer different
    sequence modes for different applications. To simplify matters, rje_seqlist will now only cope with single format
    sequences, which includes a single naming format. 

    This version of the SeqList object therefore has several distinct modes that determine how the sequences are stored.
    - full = Full loading into Sequence Objects.
    - list = Lists of (name,sequence) tuples only.
    - file = List of file positions.
    - index = No loading of sequences. Use index file to find sequences on the fly.
    - db = Store sequence data in database object.

SeqShuffle:
    Version 1.2 introduced the seqshuffle function for randomising input sequences. This generates a set of biologically
    unrealistic sequences by randomly shuffling each input sequence without replacement, such that the output sequences
    have the same primary monomer composition as the input but any dimer/trimer biases etc. are removed. This is executed
    by the shuffleSeq() method, which can also generate sequences shuffled with replacement, i.e. based on frequencies.

Sampler:
    Version 1.5 introduced a sequence sampling function for pulling out a random selection of input sequences into one or
    more output files. This is controlled by `sampler=N(,X)` where the X setting is optional. Random selections of N
    sequences will be output into a file named according to the `seqout=FILE` option (or the input file appended with
    `.nN` if none given). X defines the number of replicate datasets to generate and will be set to 1 if not given.
    If X>1 then the output filenames will be appended with `.rx` for each replicate, where x is 1 to X. If 0.0 < N < 1.0
    then a proportion of the input sequences (rounding to the nearest integer) will be selected.

SortSeq:
    In Version 1.8, the `sizesort=T/F` function is replaced with `sortseq=X` (or `seqsort=X`), where X is a choice of:
    - size = Sort sequences by size small -> big
    - accnum = Alphabetical by accession number
    - name = Alphabetical by name
    - seq[X] = Alphabetical by sequence with option to use first X aa/nt only (to save memory)
    - species = Alphabetical by species code
    - desc = Alphabetical by description
    - invsize = Sort by size big -> small re-output prior to loading/filtering (old sizesort - still sets sortseq)
    - invX / revX (Note adding `inv` or `rev` in front of any selection will reverse sort.)

Edit:
    Version 1.16 introduced an interactive edit mode (`edit=T`) that gives users the options to rearrange, copy, delete,
    split, truncate, rename, join, merge (as consensus) etc. Please contact the author for more details.

    From Version 1.20, a delimited text file can also be given
    as `edit=FILE`, which should contain: Locus, Pos, Edit, Details. Edit is the type of change (INS/DEL/SUB) and Details
    contains the nature of the change (ins/sub sequence or del length). Edits are made in reverse order per locus to
    avoid position conflicts and overlapping edits should be avoided. WARNING: These will not be checked for! An optional
    Notes field will be used if present for annotating changes in the log file.

    From Version 1.22, a delimited file can be given in place of Start,End for region=X. This file should contain Locus,
    Start, End and NewAcc fields. If No NewAcc field is present, the new accession number will be the previous accnum
    (extracted from the sequence name) with '.X-Y' appended.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqin=FILE      : Sequence input file name. [None]
    seqmode=X       : Sequence mode, determining method of sequence storage (full/list/file/index/db/filedb). [file]
    seqdb=FILE      : Sequence file from which to extract sequences (fastacmd/index formats) [None]
    seqindex=T/F    : Whether to save (and load) sequence index file in file mode. [True]
    seqformat=X     : Expected format of sequence file [None]
    seqtype=X       : Sequence type (prot(ein)/dna/rna/mix(ed)) [None]
    mixed=T/F       : Whether to allow auto-identification of mixed sequences types (else uses first seq only) [False]
    dna=T/F         : Alternative option to indicate dealing with nucleotide sequences [False]
    autoload=T/F    : Whether to automatically load sequences upon initialisation. [True]
    autofilter=T/F  : Whether to automatically apply sequence filtering. [True]

    ### ~ SEQUENCE FORMATTING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    reformat=X      : Output format for sequence files (fasta/short/acc/acclist/speclist/index/dna2prot/peptides/(q)region/revcomp) [fasta]
    rename=T/F      : Whether to rename sequences [False]
    spcode=X        : Species code for non-gnspacc format sequences [None]
    newacc=X        : New base for sequence accession numbers - will rename sequences [None]
    newgene=X       : New gene for renamed sequences (if blank will use newacc or 'seq' if none read) [None]
    concatenate=T   : Concenate sequences into single output sequence named after file [False]
    split=X         : String to be inserted between each concatenated sequence [''].
    seqshuffle=T/F  : Randomly shuffle each sequence without replacement (maintains monomer composition) [False]
    region=X,Y      : Alignment/Query region to use for reformat=peptides/(q)region reformatting of fasta alignment (1-L) [1,-1]
    edit=T/F/FILE   : Enter sequence edit mode upon loading (will switch seqmode=list) (see above) [False]

    ### ~ DNA TRANSLATIONS (reformat=dna2prot) ~~~~~~~~~~~~ ###
    minorf=X        # Min. ORF length for translated sequences output. -1 for single translation inc stop codons [-1]
    terminorf=X     # Min. length for terminal ORFs, only if no minorf=X ORFs found (good for short sequences) [-1]
    orfmet=T/F      # Whether ORFs must start with a methionine (before minorf cutoff) [True]
    rftran=X        # No. reading frames (RF) into which to translate (1,3,6) [1]

    ### ~ FILTERING OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqnr=T/F       : Whether to check for redundancy on loading. (Will remove, save and reload if found) [False]
    revcompnr=T/F   : Whether to check reverse complement for redundancy too [False]
    goodX=LIST      : Inclusive filtering, only retaining sequences matching list []
    badX=LIST       : Exclusive filtering, removing sequences matching list []
    - where X is 'Acc', Accession number; 'Seq', Sequence name; 'Spec', Species code; 'Desc', part of name;
    minlen=X        : Minimum sequence length [0]
    maxlen=X	    : Maximum length of sequences (<=0 = No maximum) [0]

    ### ~ OUTPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqout=FILE     : Whether to output sequences to new file after loading and filtering [None]
    usecase=T/F     : Whether to return sequences in the same case as input (True), or convert to Upper (False) [False]
    sortseq=X       : Whether to sort sequences prior to output (size/invsize/accnum/name/seq/species/desc) [None]
    sampler=N(,X)   : Generate (X) file(s) sampling a random N sequences from input into seqout.N.X.fas [0]
    summarise=T/F   : Generate some summary statistics in log file for sequence data after loading [False]
    splitseq=X      : Split output sequence file according to X (gene/species) [None]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_obj, rje_zen
Other modules needed: None

### ~~~ Module rje_sequence ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_sequence.py] ~~~ ###

Module:       rje_sequence
Description:  DNA/Protein sequence object
Version:      2.6.0
Last Edit:    15/11/17
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the Sequence Object used to store sequence data for all PEAT applications that used DNA or
    protein sequences. It has no standalone functionality.

    This modules contains all the methods for parsing out sequence information, including species and source database,
    based on the format of the input sequences. If using a consistent but custom format for fasta description lines,
    please contact me and I can add it to the list of formats currently recognised.

Uses general modules: copy, os, random, re, sre_constants, string, sys, time
Uses RJE modules: rje, rje_disorder

### ~~~ Module rje_slim ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_slim.py] ~~~ ###

Module:       RJE_SLiM
Description:  Short Linear Motif class module
Version:      1.12.1
Last Edit:    18/01/17
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the new SLiM class, which replaces the old Motif class, for use with both SLiMFinder and
    SLiMSearch. In addition, this module encodes some general motif methods. Note that the new methods are not
    designed with Mass Spec data in mind and so some of the more complicated regexp designations for unknown amino acid
    order etc. have been dropped. Because the SLiM class explicitly deals with *short* linear motifs, wildcard gaps are
    capped at a max length of 9.

    The basic SLiM class stores its pattern in several forms:
    - info['Sequence'] stores the original pattern given to the Motif object
    - info['Slim'] stores the pattern as a SLiMFinder-style string of defined elements and wildcard spacers
    - dict['MM'] stores lists of Slim strings for each number of mismatches with flexible lengths enumerated. This is
      used for actual searches in SLiMSearch.
    - dict['Search'] stores the actual regular expression variants used for searching, which has a separate entry for
      each length variant - otherwise Python RegExp gets confused! Keys for this dictionary relate to the number of
      mismatches allowed in each variant and match dict['MM'].

    The following were previously used by the Motif class and may be revived for the new SLiM class if needed:    
    - list['Variants'] stores simple strings of all the basic variants - length and ambiguity - for indentifying the
      "best" variant for any given match

    The SLiM class is designed for use with the SLiMList class. When a SLiM is added to a SLiMList object, the
    SLiM.format() command is called, which generates the 'Slim' string. After this - assuming it is to be kept -
    SLiM.makeVariants() makes the 'Variants' list. If creating a motif object in another module, these method should be
    called before any sequence searching is performed. If mismatches are being used, the SLiM.misMatches() method must
    also be called.

    SLiM occurrences are stored in the dict['Occ'] attribute. The keys for this are Sequence objects and values are
    either a simple list of positions (1 to L) or a dictionary of attributes with positions as keys.

Commandline:
    These options should be listed in the docstring of the module using the motif class:
    - alphabet=LIST     : List of letters in alphabet of interest [AAs]
    - ambcut=X          : Cut-off for max number of choices in ambiguous position to be shown as variant (0=All) [10]
    - trimx=T/F         : Trims Xs from the ends of a motif [False]
    - dna=T/F           : Whether motifs should be considered as DNA motifs [False]

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_slimcalc ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_slimcalc.py] ~~~ ###

Module:       rje_slimcalc
Description:  SLiM Attribute Calculation Module
Version:      0.9.3
Last Edit:    27/09/17
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is based on the old rje_motifstats module. It is primarily for calculating empirical attributes of SLiMs
    and their occurrences, such as Conservation, Hydropathy and Disorder. 

Commandline:
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Motif Occurrence Attribute Options ###
    slimcalc=LIST   : List of additional attributes to calculate for occurrences - Cons,SA,Hyd,Fold,IUP,Chg,Comp []
    winsize=X       : Used to define flanking regions for calculations. If negative, will use flanks *only* [0]
    relconwin=X     : Window size for relative conservation scoring [30]
    iupath=PATH     : The full path to the IUPred exectuable [c:/bioware/iupred/iupred.exe]
    iucut=X         : Cut-off for IUPred results (0.0 will report mean IUPred score) [0.0]
    iumethod=X      : IUPred method to use (long/short) [short]
    percentile=X    : Percentile steps to return in addition to mean [0]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Alignment Settings ###
    usealn=T/F      : Whether to search for and use alignments where present. [False]
    alndir=PATH     : Path to pre-made alignment files [./]
    alnext=X        : File extension of alignment files, AccNum.X (checked before Gopher used) [orthaln.fas]
    gopherdir=PATH  : Path from which to look for GOPHER alignments (if not found in alndir) and/or run GOPHER [./] 
    usegopher=T/F   : Use GOPHER to generate orthologue alignments missing from alndir - see gopher.py options [False]
    fullforce=T/F   : Whether to force regeneration of alignments using GOPHER [False]
    orthdb=FILE     : File to use as source of orthologues for GOPHER []
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Conservation Parameters ###   
    conspec=LIST    : List of species codes for conservation analysis. Can be name of file containing list. [None]
    conscore=X      : Type of conservation score used:  [rlc]
                        - abs = absolute conservation of motif using RegExp over matched region
                        - pos = positional conservation: each position treated independently
                        - prob = conservation based on probability from background distribution
                        - prop = conservation of amino acid properties
                        - rlc = relative local conservation
                        - all = all methods for comparison purposes
    consamb=T/F     : Whether to calculate conservation allowing for degeneracy of motif (True) or of fixed variant (False) [True]
    consinfo=T/F    : Weight positions by information content (does nothing for conscore=abs) [True]
    consweight=X    : Weight given to global percentage identity for conservation, given more weight to closer sequences [0]
                        - 0 gives equal weighting to all. Negative values will upweight distant sequences.
    minhom=X        : Minimum number of homologues for making conservation score [1]
    homfilter=T/F   : Whether to filter homologues using seqfilter options [False]
    alngap=T/F      : Whether to count proteins in alignments that have 100% gaps over motif (True) or (False) ignore
                      as putative sequence fragments [False]  (NB. All X regions are ignored as sequence errors.)
    posmatrix=FILE  : Score matrix for amino acid combinations used in pos weighting. (conscore=pos builds from propmatrix) [None]
    aaprop=FILE     : Amino Acid property matrix file. [aaprop.txt]
    masking=T/F     : Whether to use seq.info['MaskSeq'] for Prob cons, if present (else 'Sequence') [True]
    vnematrix=FILE  : BLOSUM matrix file to use for VNE relative conservation []
    relgappen=T/F   : Whether to invoke the "Gap Penalty" during relative conservation calculations [True]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiM/Occ Filtering Options ###
    slimfilter=LIST : List of stats to filter (remove matching) SLiMs on, consisting of X*Y  []
                      - X is an output stat (the column header),
                      - * is an operator in the list >, >=, !=, =, >= ,<    
                      - Y is a value that X must have, assessed using *.
                      This filtering is crude and may behave strangely if X is not a numerical stat!
                      !!! Remember to enclose in "quotes" for <> filtering !!!
    occfilter=LIST  : Same as slimfilter but for individual occurrences []
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

### ~~~ Module rje_slimcore ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_slimcore.py] ~~~ ###

Module:       SLiMCore
Description:  SLiMSuite core module
Version:      2.9.0
Last Edit:    19/10/17
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is primarily to contain core dataset processing methods for both SLiMFinder and SLiMProv to inherit and
    use. This primarily consists of the options and methods for masking datasets and generating UPC. This module can
    therefore be run in standalone mode to generate UPC files for SLiMFinder or SLiMProb. It can also be used to generate
    "MegaSLiM" files of precomputed scores that can be used for subsequent subdata.

    In addition, the secondary MotifSeq and Randomise functions are handled here.
    
Secondary Functions:
    The "MotifSeq" option will output fasta files for a list of X:Y, where X is a motif pattern and Y is the output file.

    The "Randomise" function will take a set of input datasets (as in Batch Mode) and regenerate a set of new datasets
    by shuffling the UPC among datasets. Note that, at this stage, this is quite crude and may result in the final
    datasets having fewer UPC due to common sequences and/or relationships between UPC clusters in different datasets.

    The "EquivMaker" function will read in a BLOSUM matrix and, for a particular score cut-off, generate all equivalence
    groups for which all members have pairwise BLOSUM scores that equal or exceed the cut-off. This equivalence file can
    then be used as input for TEIRESIAS or SLiMFinder.

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ### 
    seqin=FILE      : Sequence file to search. Over-rules batch mode (and uniprotid=LIST). [None]
    batch=LIST      : List of files to search, wildcards allowed. [*.dat,*.fas]
    uniprotid=LIST  : Extract IDs/AccNums in list from Uniprot into BASEFILE.dat and use as seqin=FILE. []
    maxseq=X        : Maximum number of sequences to process [0]
    maxupc=X        : Maximum UPC size of dataset to process [0]
    sizesort=X      : Sorts batch files by size prior to running (+1 small->big; -1 big->small; 0 none) [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [SLiMFinder/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMFinder/]
    force=T/F       : Force re-running of BLAST, UPC generation and SLiMBuild [False]
    dna=T/F         : Whether the sequences files are DNA rather than protein [False]
    alphabet=LIST   : List of characters to include in search (e.g. AAs or NTs) [default AA or NT codes]
    megaslim=FILE   : Make/use precomputed results for a proteome (FILE) in fasta format [None]
    megablam=T/F    : Whether to create and use all-by-all GABLAM results for (gablamdis) UPC generation [False]
    megaslimfix=T/F : Whether to run megaslim in "fix" mode to tidy/repair existing files [False]
    ptmlist=LIST    : List of PTM letters to add to alphabet for analysis and restrict PTM data []
    ptmdata=DSVFILE : File containing PTM data, including AccNum, ModType, ModPos, ModAA, ModCode
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Evolutionary Filtering Options ###
    efilter=T/F     : Whether to use evolutionary filter [True]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    fupc=T/F        : Whether to use experimental "Fragment UPC" approach for UPC of large proteomes [False]
    domtable=FILE   : Domain table containing domain ("Type") and sequence ("Name") pairings for additional UPC [None]
    homcut=X        : Max number of homologues to allow (to reduce large multi-domain families) [0]
    extras=T/F      : Whether to generate additional output files (distance matrices etc.) [True]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Input Masking and AA Frequency Options ###
    masking=T/F     : Master control switch to turn off all masking if False [True]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out (True=EM,DOMAIN,TRANSMEM) []
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    fakemask=T/F    : Whether to invoke UniFake to generate additional features for masking [False]
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    metmask=T/F     : Masks the N-terminal M (can be useful if SLiMFinder termini=T) *Also maskm=T/F* [True]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas *Also maskpos=LIST* [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    logmask=T/F     : Whether to output the log messages for masking of individual sequences to screen [False]
    masktext=X      : Text ID to over-ride automated masking text and identify specific masking settings [None]
    maskpickle=T/F  : Whether to save/load pickle of masked input data, independent of main pickling [False]
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [True]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]    
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    qregion=X,Y     : Mask all but the region of the query from (and including) residue X to residue Y [0,-1]
    megaslimdp=X    : Accuracy (d.p.) for MegaSLiM masking tool raw scores [4]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Advanced Output Options ###
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    pickle=T/F      : Whether to save/use pickles [True]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc, *.pickle (Pickle excluded from tar.gz with this setting)
                        - 2 = Delete all bar *.upc files (Pickle included in tar.gz with this setting)
                        - 3 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)
    iuscoredir=PATH : Path in which to save protein acc.iupred.txt score files for megaslim analysis
    protscores=T/F  : Whether to save individual protein rlc.txt files in alignment directory [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Additional Functions I: MotifSeq ###    
    motifseq=LIST   : Outputs fasta files for a list of X:Y, where X is the pattern and Y is the output file []
    slimbuild=T/F   : Whether to build motifs with SLiMBuild. (For combination with motifseq only.) [True]

    ### Additional Functions II: Randomised datasets ###
    randomise=T/F   : Randomise UPC within batch files and output new datasets [False]
    randir=PATH     : Output path for creation of randomised datasets [Random/]
    randbase=X      : Base for random dataset name [rand]
    randsource=FILE : Source for new sequences for random datasets (replaces UPCs) [None]

    ### Additional Functions III: Equivalence File Maker ###
    blosumfile=FILE : BLOSUM file from which to make equivalence file
    equivout=FILE   : File for equivalence list output [equiv.txt]
    equivcut=X      : BLOSUM score cut-off for equivalence groups [1]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

### ~~~ Module rje_slimhtml ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_slimhtml.py] ~~~ ###

Module:       RJE_SLiMHTML
Description:  Module for generating HTML for Human SLiMFinder study
Version:      0.9
Last Edit:    11/03/11
Copyright (C) 2010  Richard J. Edwards - See source code for GNU License Notice

Function:
    The function of this module will be added here.

Commandline:
    datapath=PATH       : Path to parent data directory [./]
    htmlpath=PATH       : Path of parent html directory [./html]
    stylesheets=LIST    : List of CSS files to use ['../example.css','../redwards.css']
    border=X            : Border setting for tables [0]
    dropfields=LIST     : Fields to exclude from summary tables []
    fakehtml=T/F        : Whether to make UniFake HTML [True]
    unifake=PATH        : Path to UniFake dat file(s) [./unifake/]
    unipath=PATH        : Path to real UniProt dat file(s) [./uniprot/]
    unireal=LIST        : Real UniProt data to add to UniFake output (Empty=None) [AC,GN,RC,RX,CC,DR,PE,KW,FT]
    makepng=T/F         : Whether to (look for and) make PNG files with R [True]
    svg=T/F             : Make SVG files rather than PNG files [True]
    addrand=T/F         : Whether to add pages for random data [True]
    makepages=LIST      : Types of pages to make [front,gene,domain,rand,slim,fake,occaln,interactome,slimaln,go,nested]
    titletext=FILE      : File containing (Page,ID,Title) [titletext.tdt]
    slimdesc=FILE       : File containing descriptions for SLiMs [slimdesc.txt]
    baddset=LIST        : List of Bad Datasets to filter out of main and occ database tables []
    pround=T/F          : Whether to round off occurrence positions for PNGs [True]
    xgcut=X             : Significance cut-off for XGMML [0.01]

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_ppi, rje_slim, rje_uniprot, rje_zen
Other modules needed: None

### ~~~ Module rje_slimlist ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_slimlist.py] ~~~ ###

Module:       rje_slimlist
Description:  SLiM dataset manager
Version:      1.7.3
Last Edit:    12/06/15
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is a replace for the rje_motiflist module and contains the SLiMList class, a replacement for the
    MotifList class. The primary function of this class is to load and store a list of SLiMs and control generic SLiM
    outputs for such programs as SLiMSearch. This class also controls motif filtering according to features of the motifs
    and/or their occurrences.

    Although not actually designed with standalone functionality in mind, as of V1.3 it is possible to load motifs (inc.
    downloading ELM), filter/process motifs and output data using motifout=FILE and motinfo=FILE if desired.

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ###
    motifs=FILE     : File of input motifs/peptides [None]
                      Single line per motif format = 'Name Sequence #Comments' (Comments are optional and ignored)
                      Alternative formats include fasta, SLiMDisc output and raw motif lists.
                      "elm" will download, process and load ELM classes using the ELM API.
    reverse=T/F     : Reverse the motifs - good for generating a test comparison data set [False]
    wildscram=T/F   : Perform a wildcard spacer scrambling - good for generating a test comparison data set [False]
    motifout=FILE   : Name of output file for reformatted/filtered SLiMs (PRESTO format) [None]
    ftout=T/F       : Make a file of UniProt features for extracted parent proteins, where possible, incoroprating SLIMs [*.features.tdt]
    mismatch=LIST   : List of X:Y pairs for mismatch dictionary, where X mismatches allowed for Y+ defined positions []    
    motinfo=FILE    : Filename for output of motif summary table (if desired) [None]
    dna=T/F         : Whether motifs should be considered as DNA motifs [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Advanced Input I: Motif Filtering Options ###
    minpos=X        : Min number of defined positions [0]
    minfix=X        : Min number of fixed positions for a motif to contain [0]
    minic=X         : Min information content for a motif (1 fixed position = 1.0) [0.0]
    varlength=T/F   : Whether to motifs can have flexible-length elements [True]
    goodmotif=LIST  : List of text to match in Motif names to keep (can have wildcards) []
    nrmotif=T/F     : Whether to remove redundancy in input motifs [False]

    ### Advanced Input II: Motif reformatting options ###
    trimx=T/F       : Trims Xs from the ends of a motif [False]
    minimotif=T/F   : Input file is in minimotif format and will be reformatted (PRESTO File format only) [False]
    ambcut=X        : Cut-off for max number of choices in ambiguous position to be shown as variant [10]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Advanced Output I: Motif Occurrence Statistics Options ###
    slimcalc=LIST   : List of additional statistics to calculate for occurrences - Cons,SA,Hyd,Fold,IUP,Chg []
    winsize=X       : Used to define flanking regions for stats. If negative, will use flanks *only* [0]
    iupath=PATH     : The full path to the IUPred exectuable [c:/bioware/iupred/iupred.exe]
    iucut=X         : Cut-off for IUPred results (0.0 will report mean IUPred score) [0.0]
    iumethod=X      : IUPred method to use (long/short) [short]
    percentile=X    : Percentile steps to return in addition to mean [0]
    peptides=T/F    : Whether to output peptide sequences based on motif and winsize [False]

    ### Advanced Output II: Alignment Settings ###
    usealn=T/F      : Whether to search for and use alignemnts where present. [False]
    gopher=T/F      : Use GOPHER to generate missing orthologue alignments in alndir - see gopher.py options [False]
    alndir=PATH     : Path to alignments of proteins containing motifs [./] * Use forward slashes (/)
    alnext=X        : File extension of alignment files, accnum.X [aln.fas]
    protalndir=PATH : Output path for Protein Alignments [ProteinAln/]
    motalndir=PATH  : Output path for Motif Alignments []
    flanksize=X     : Size of sequence flanks for motifs [30]
    xdivide=X       : Size of dividing Xs between motifs [10]
    fullforce=T/F   : Whether to force regeneration of alignments using GOPHER

    ### Advanced Output III: Conservation Parameters ###   
    * see rje_slimcalc.py documentation
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module rje_specificity ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_specificity.py] ~~~ ###

Module:       rje_specificity
Description:  Functional Specificity Methods
Version:      0.1
Last Edit:    18/05/06
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function: Functional Specificity methods for BADASP.

Commandline:
    
Uses general modules: os, string, sys, time
Uses RJE modules: rje, rje_conseq

### ~~~~ Module rje_svg ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_svg.py] ~~~~ ###

Module:       rje_svg
Description:  RJE SVG Module
Version:      0.0
Last Edit:    31/12/10
Copyright (C) 2009  Richard J. Edwards - See source code for GNU License Notice

Function:
    The function of this module will be added here.

Commandline:
    ### ~~~ INPUT ~~~ ###
    col=LIST    : Replace standard colour listing (mixed Hex and RGB) []

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~ Module rje_synteny ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_synteny.py] ~~~ ###

Module:       rje_synteny
Description:  Gene/Protein Annotation Transfer and Synteny Tool
Version:      0.0.3
Last Edit:    26/06/17
Copyright (C) 2016  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is based on the TopHits analysis of PAGSAT. It takes as input:
    - A Reference Genome sequence [refgenome=FILE]
    - A Reference Genome feature table with Gene and CDS features [ftfile=FILE]
    - GABLAM search results (e.g. from PAGSAT) of Genes/Proteins against assembly regions [ftgablam=FILE]




Commandline:
    ### ~ Input/Output options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    refgenome=FILE  : Fasta/Genbank file of reference genome for assessment (also *.gb for full functionality) [None]
    ftfile=FILE     : File of reference genome features [*.Feature.tdt]
    ftgablam=FILE   : GABLAM search results (e.g. from PAGSAT) of Genes/Proteins against assembly regions [None]
    basefile=FILE   : Basename for output files [ftgablam basefile]
    ### ~ Processing options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    proteins=T/F    : Whether input is protein sequences (True) or genes (False) [False]
    tophitbuffer=X  : Percentage identity difference to keep best hits for reference genes/proteins. [1.0]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

### ~~~ Module rje_taxonomy ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_taxonomy.py] ~~~ ###

Module:       rje_taxonomy
Description:  Downloads, reads and converts Uniprot species codes and NCBI Taxa IDs
Version:      1.2.0
Last Edit:    19/05/16
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is designed to download and interconvert between NCBI Taxa IDs and Uniprot species codes and species
    names. It uses two main files: speclist.txt from Uniprot and node.dmp from NCBI taxonomy. These will be downloaded
    if missing and download=T, else can be manually downloaded from:

    - http://www.uniprot.org/docs/speclist.txt
    - ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz

    These will be saved in the directory given by taxdir=PATH/ (default ./SourceData/) and will have the download
    date inserted. This enables several versions to be stored together if desired and selected using the sourcedate=DATE
    option, where DATE is in the form YYYY-MM-DD.

    Alternatively, specfile=FILE, taxmap=FILE and namemap=FILE can be used to give other files of the same format. The
    SpecFile should follow key format features of the organism codes but do not need the headers. The TaxMap
    file simply uses the first three columns of the nodes.dump file (separated by "\t|\t"), which correspond to (1) the
    Taxa ID, (2) the parent Taxa ID, and (3) the rank of the entry. These ranks are used to determine which taxa are
    output if rankonly=T. By default, this is "species" and "subspecies". NameMap uses all four fields.

    To extract and/or convert a set of Taxa IDs, a list of taxa should be given using taxin=LIST, where LIST is either a
    comma separated list of taxa, or a file containing one taxon per line. Taxa may be a mix of NCBI Taxa IDs, Uniprot
    species codes and case-insensitive (but exact match) species names. By default, all taxa will be combined but if
    batchmode=T then each TaxIn element will be processed individually. When batchmode=T, individual list elements can be
    files containing taxa. (This will not work in batchmode=F, unless only a single file is given.)

    Taxa are first mapped on to NCBI Taxa IDs. Unless nodeonly=T, taxa will also be mapped on to all of their child taxa,
    as defined in nodes.dmp. If rankonly=T then only those taxa with a rank matching ranktypes=LIST will be retained. IDs
    can be further restricted by supplying a list with restrictid=LIST, which will limit mapped IDs to those within the
    list given. (Note that this list could itself be created by a previous file of rje_taxonomy and given as a file.)
    Taxa IDs are then mapped on the Uniprot species codes and species names using the SpecFile data. If missing from this
    file, scientific names will be pulled out of the NCBI NameMap file instead. NOTE: Uniprot is used first because NCBI
    has more redundant taxonomy assignments.

    Output is determined by the taxout=LIST option, which is set by default to 'taxid'. Four possible output types are
    permitted:
    - taxid = NCBI Taxa IDs (e.g. 9606 or 7227)
    - spcode = Uniprot species codes (e.g. HUMAN or DROME)
    - name = Scientific name (e.g. Homo sapiens or Drosophila melanogaster)
    - common = Common name (e.g. Human or Fruit fly)

    These will be output as lists to BASE.TYPE.txt files, where BASE is set by basefile=X (using the first taxin=LIST
    element if missing) and TYPE is the taxout type. If batchmode=T then a separate set of files will be made for each
    element of the TaxIn list, using BASE.TAXIN.TYPE.txt file naming.

Commandline:
    ### ~ SOURCE DATA OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    specfile=FILE       : Uniprot species code download. [speclist.txt]
    taxmap=FILE         : NCBI Node Dump File [nodes.dmp]
    namemap=FILE        : NCBI Name mapping file [names.dmp]
    taxdir=PATH/        : Will look in this directory for input files if not found ['./SourceData/']
    sourcedate=DATE     : Source file date (YYYY-MM-DD) to preferentially use [None]
    download=T/F        : Whether to download files directly from websites where possible if missing [True]
    ### ~ TAXONOMY CONVERSION OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    taxin=LIST          : List of Taxa IDs, Uniprot species codes (upper case) and/or common/scientific names []
    batchmode=T/F       : Treat each element of taxin as a separate run (will be used for output basefile) [False]
    taxout=LIST         : List of output formats (taxid/spcode/name/common/all) [taxid]
    nodeonly=T/F        : Whether to limit output to the matched nodes (i.e. no children) [False]
    rankonly=T/F        : Whether to limit output to species-level taxonomic codes [False]
    ranktypes=LIST      : List of Taxon types to include if rankonly=True [species,subspecies,no rank]
    restrictid=LIST     : List of Taxa IDs to restrict output to (i.e. output overlaps with taxin) []
    basefile=X          : Results file prefix. Will use first taxin=LIST term if missing [None]
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
See also rje.py generic commandline options.

### ~~~~~ Module rje_tm ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_tm.py] ~~~~~ ###

Module:       rje_tm    
Description:  Tranmembrane and Signal Peptide Prediction Module
Version:      1.2
Last Edit:    16/08/07
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    Will read in results from tmhmm and/or signalp files as appropriate and append output to:
    - tm.tdt        = TM domain counts and orientation
    - domains.tdt   = Domain table
    - singalp.tdt   = SingalP data (use to add signal peptide domains to domains table using mySQL

Commandline:
    tmhmm=FILE  : TMHMM output file [None]
    signalp=FILE: SignalP output file [None]
    mysql=T/F   : Output results in tdt files for mySQL import [True]

    seqin=FILE      : Sequence file for which predictions have been made [None]
    maskcleave=T/F  : Whether to output sequences with cleaved signal peptides masked. [False]
    source=X        : Source text for mySQL file ['tmhmm']

Uses general modules: os, string, sys, threading, time
Uses RJE modules: rje, rje_seq
Other modules required: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_uniprot

### ~~~ Module rje_tree ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_tree.py] ~~~ ###

Module:       rje_tree
Description:  Phylogenetic Tree Module
Version:      2.15.0
Last Edit:    30/07/17
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    Reads in, edits and outputs phylogenetic trees. Executes duplication and subfamily determination. More details available
    in documentation for HAQESAC, GASP and BADASP at http://www.bioinformatics.rcsi.ie/~redwards/

General Commands:
    nsfin=FILE      : load NSF tree from FILE
    phbin=FILE      : load ClustalW Format *.phb NSF tree from FILE
    seqin=FILE      : load sequence list from FILE (not compatible with useanc)
    disin=FILE      : load distance matrix from FILE (Phylip format for use with distance matrix methods) [None]
    useanc=FILE     : load sequences from ancestral sequence FILE (not compatible with seqin)
    deflen=X        : Default length for branches when no lengths given [0.1] (or 0.1 x longest branch)
    *Note that in the case of conflicts (e.g. seqin=FILE1 useanc=FILE2), the latter will be used.*
    autoload=T/F    : Whether to automatically load sequences upon initiating object [True]

Rooting Commands:
    root=X  : Rooting of tree (rje_tree.py):
        - mid = midpoint root tree.
        - ran = random branch.
        - ranwt = random branch, weighted by branch lengths.
        - man = always ask for rooting options (unless i<0).
        - none = unrooted tree
        - FILE = with seqs in FILE as outgroup. (Any option other than above)
    rootbuffer=X    : Min. distance from node for root placement (percentage of branch length)[0.1]

Grouping/Subfamily Commands:
    bootcut=X   : cut-off percentage of tree bootstraps for grouping.
    mfs=X       : minimum family size [3]
    fam=X       : minimum number of families (If 0, no subfam grouping) [0]
    orphan=T/F  : Whether orphans sequences (not in subfam) allowed. [True]
    allowvar=T/F: Allow variants of same species within a group. [False]
    qryvar=T/F  : Keep variants of query species within a group (over-rides allowvar=F). [False]
    groupspec=X : Species for duplication grouping [None]
    specdup=X   : Minimum number of different species in clade to be identified as a duplication [1]
    group=X     : Grouping of tree
        - man = manual grouping (unless i<0).
        - dup = duplication (all species unless groupspec specified).
        - qry = duplication with species of Query sequence (or Sequence 1) of treeseq
        - one = all sequences in one group
        - None = no group (case sensitive)
        - FILE = load groups from file

Tree Making Commands:
    cwtree=FILE     : Make a ClustalW NJ Tree from FILE (will save *.ph or *.phb) [None]
    kimura=T/F      : Whether to use Kimura correction for multiple hits [True]
    bootstraps=X    : Number of bootstraps [0]
    clustalw=CMD    : Path to CLUSTALW (and including) program [''] * Use forward slashes (/)
    fasttree=PATH   : Path to FastTree (and including) program ['']
    iqtree=FULLPATH : Path IQTree program including program ['iqtree']
    phylip=PATH     : Path to PHYLIP programs [''] * Use forward slashes (/)
    phyoptions=FILE : File containing extra Phylip tree-making options ('batch running') to use [None]
    protdist=FILE   : File containing extra Phylip PROTDIST options ('batch running') to use [None]
    maketree=X      : Program for making tree [None]
        - None = Do not make tree from sequences 
        - clustalw = ClustalW NJ method
        - neighbor = PHYLIP NJ method 
        - upgma    = PHYLIP UPGMA (neighbor) method 
        - fitch    = PHYLIP Fitch method 
        - kitsch   = PHYLIP Kitsch (clock) method 
        - protpars = PHYLIP MP method 
        - proml    = PHYLIP ML method
        - fasttree = Use FastTree
        - iqtree   = Use IQTree
        - PATH     = Alternatively, a path to a different tree program/script can be given. This should accept ClustalW parameters.

Tree Display/Saving Commands
    savetree=FILE   : Save a generated tree as FILE [seqin.maketree.nsf]
    savetype=X      : Format for generated tree file (nsf/nwk/text/r/png/bud/qspec/cairo/te/svg/html) [nwk]
    treeformats=LIST: List of output formats for generated trees [nwk]
    outnames=X      : 'short'/'long' names in output file [short]
    truncnames=X    : Truncate names to X characters (0 for no truncation) [123]
    branchlen=T/F   : Whether to use branch lengths in output tree [True]
    deflen=X        : Default branch length (when none given, also for tree scaling) [0.1]
    textscale=X     : Default scale for text trees (no. of characters per deflen distance) [4]
    seqnum=T/F      : Output sequence numbers (if making tree from sequences) [True]
    
Classes:
    Tree(rje.RJE_Object):     
        - Phylogenetic Tree class.
    Node(rje.RJE_Object): 
        - Individual nodes (internal and leaves) for Tree object.
    Branch(rje.RJE_Object):
        - Individual branches for Tree object.
        
Uses general modules: copy, os, random, re, string, sys, time
Uses RJE modules: rje, rje_ancseq, rje_seq, rje_tree_group
Other module needed: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_uniprot

### ~~~ Module rje_tree_group ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_tree_group.py] ~~~ ###

Module:       rje_tree_group
Description:  Contains all the Grouping Methods for rje_tree.py
Version:      1.2.1
Last Edit:    28/01/15
Copyright (C) 2005  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is a stripped down template for methods only. This is for when a class has too many methods and becomes
    untidy. In this case, methods can be moved into a methods module and 'self' replaced with the relevant object. For
    this module, 'self' becomes '_tree'.

Commandline:
    This module is not for standalone running and has no commandline options (including 'help'). All options are handled
    by the parent module: rje_tree.py

Uses general modules: copy, re, os, string, sys
Uses RJE modules: rje, rje_seq
Other modules needed: rje_blast, rje_dismatrix, rje_pam, rje_sequence, rje_uniprot

### ~~~ Module rje_uniprot ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_uniprot.py] ~~~ ###

Module:       rje_uniprot
Description:  RJE Module to Handle Uniprot Files
Version:      3.24.1
Last Edit:    09/02/17
Copyright (C) 2007 Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains methods for handling UniProt files, primarily in other rje modules but also with some
    standalone functionality. To get the most out of the module with big UniProt files (such as the downloads from EBI),
    first index the UniProt data using the rje_dbase module.

    This module can be used to extract a list of UniProt entries from a larger database and/or to produce summary tables
    from UniProt flat files. Version 3.14 introduced direct querying from the UniProt website if unipath=None or
    unipath=URL.

    In addition to method associated with the classes of this module, there are a number of methods that are called from
    the rje_dbase module (primarily) to download and process the UniProt sequence database.

    Version 3.19 has seen an over-haul of the dbxref extraction. `dblist=LIST` and `dbparse=LIST` can now be used
    largely synonymously. Rather than extract all db xref by default, there is now a default list of databases to parse:
    ['UniProtKB/Swiss-Prot','ensembl','REFSEQ','HGNC','Entrez Gene','FlyBase','Pfam','GO','MGI','ZFIN'].

Input/Output:
    ### ~ Input Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    unipath=PATH    : Path to UniProt Datafile (looks here for DB Index file; url = use Web downloads) [url]
    dbindex=FILE    : Database index file [uniprot.index]
    uniprot=FILE    : Name of UniProt file [None]
    extract=LIST    : Extract IDs/AccNums in list. LIST can be FILE or list of IDs/AccNums X,Y,.. []
    acclist=LIST    : As extract=LIST.
    uniprotid=LIST  : As extract=LIST.
    acctable=FILE   : Delimited text file from which to retrieve a list of accession numbers [None]
    accfield=X      : Accession number field for acctable=FILE extraction [UniProt]
    specdat=LIST    : Make a UniProt DAT file of the listed species from the index (over-rules extract=LIST) []
    proteome=LIST   : Extract complete proteomes for listed Taxa ID (e.g. 9606 for human) []
    taxonomy=LIST   : Extract all entries for listed Taxa ID (e.g. 4751 for fungi) []
    usebeta=T/F     : Whether to use beta.uniprot.org rather than www.uniprot.org [False]
    splicevar=T/F   : Whether to search for AccNum allowing for splice variants (AccNum-X) [True]
    tmconvert=T/F   : Whether to convert TOPO_DOM features, using first description word as Type [False]
    reviewed=T/F    : Whether to restrict input to reviewed entries only [False]
    complete=T/F    : Whether to restrict proteome downloads to "complete proteome" sets [False]
    uniformat=X     : Desired UniProt format for URL download (over-rules normal processing). Append gz to compress. [txt]
                        - html | tab | xls | fasta | gff | txt | xml | rdf | list | rss

    ### ~ Output Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : If set, can use "T" or "True" for other `*out` options. (Will default to `datout` if given) [None]
    datout=T/F/FILE : Name of new (reduced) UniProt DAT file of extracted sequences [None]
    tabout=T/F/FILE : Table of extracted UniProt details [None]
    ftout=T/F/FILE  : Outputs table of features into FILE [None]
    pfamout=T/F/FILE: Whether to output a "long" table of (accnum, pfam, name, num) [False]
    domtable=T/F    : Makes a table of domains from uniprot file [False]
    gotable=T/F     : Makes a table of AccNum:GO mapping [False]
    cc2ft=T/F       : Extra whole-length features added for TISSUE and LOCATION (not in datout) [False]
    xrefout=T/F/FILE: Table of extracted Database xref (Formerly linkout=FILE) [None]
    longlink=T/F    : Whether link table is to be "long" (acc,db,dbacc) or "wide" (acc, dblinks) [True]
    dblist=LIST     : List of databases to extract (extract all if empty or contains 'all') [see above]
    dbsplit=T/F     : Whether to generate a table per dblist database (basefile.dbase.tdt) [False]
    dbdetails=T/F   : Whether to extract full details of DR line rather than parsing DB xref only [False]
    append=T/F      : Append to results files rather than overwrite [False]

    ### ~ UniProt Conversion Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    ucft=X          : Feature to add for UpperCase portions of sequence []
    lcft=X          : Feature to add for LowerCase portions of sequence []
    maskft=LIST     : List of Features to mask out []
    invmask=T/F     : Whether to invert the masking and only retain maskft features [False]
    caseft=LIST     : List of Features to make upper case with rest of sequence lower case []

Specialist Options:
    ### ~ Parsing Options (Programming Only) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    fullref=T/F     : Whether to store full Reference information in UniProt Entry objects [False]
    dbparse=LIST    : List of databases to parse from DR lines in UniProtEntry object [see code]
    uparse=LIST     : Restricted lines to parse to accelerate parsing of large datasets for limited information []

    ### ~ General Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    memsaver=T/F    : Memsaver option to save memory usage - does not retain entries in UniProt object [False]
    cleardata=T/F   : Whether to clear unprocessed Entry data (True) or (False) retain in Entry & Sequence objects [True]
    specsleep=X     : Sleep for X seconds between species downloads [60]

    ### ~ UniProt Download Processing Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    makeindex=T/F   : Generate UniProt index files [False]
    makespec=T/F    : Generate species table [False]
    makefas=T/F     : Generate fasta files [False]
    grepdat=T/F     : Whether to use GREP in attempt to speed up processing [False]

Uses general modules: glob, os, re, string, sys, time
Uses RJE modules: rje, rje_db, rje_sequence

### ~~~ Module rje_xgmml ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_xgmml.py] ~~~ ###

Module:       RJE_XGMML
Description:  RJE XGMLL Module 
Version:      1.0
Last Edit:    26/06/14
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is currently designed to store data for, and then output, an XGMML file for uploading into Cytoscape etc.
    Future versions may incoporate the ability to read and manipulate existing XGMML files.

Commandline:
    At present, all commands are handling by the class populating the XGMML object.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~~ Module rje_xml ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_xml.py] ~~~~ ###

Module:       rje_xml.py
Description:  XML (Text) Parsing Module
Version:      0.2
Last Edit:    06/08/13
Copyright (C) 2006  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module contains the XML class for parsing XML files. These are parsed into a generic set of nested dictionaries
    and XML objects stored in the root XML object. The main attributes of interest in the XML object for retrieving data
    are:
    * info['Name'] = element name
    * info['Content'] = text content of element (if any)
    * dict['Attributes'] = dictionary of element attributes (if any)
    * list['XML'] = list of XML objects containing nested elements (if any)

    For example, the following short XML:    
    < ?xml version="1.0" encoding="ISO-8859-1"? >
    < database name="EnsEMBL" ftproot="ftp://ftp.ensembl.org/pub/" outdir="EnsEMBL" >
          < file path="current_aedes_aegypti/data/fasta/pep/*.gz" >Yellow Fever Mosquito< /file >
          < file path="current_anopheles_gambiae/data/fasta/pep/*.gz" >Malaria Mosquito< /file >
    < /database >

    The following XML objects would be created:
    * XML root object:
        XML.info['Name'] = filename
        XML.list['XML'] = [XML1]
    * XML1: 
        XML1.info['Name'] = 'database'
        XML1.info['Content'] = ''
        XML1.dict['Atrributes'] = {'name':"EnsEMBL",'ftproot':"ftp://ftp.ensembl.org/pub/",'outdir':"EnsEMBL"}
        XML1.list['XML'] = [XML2,XML3]
    * XML2:
        XML2.info['Name'] = 'file'
        XML2.info['Content'] = 'Yellow Fever Mosquito'
        XML2.dict['Atrributes'] = {'path':"current_aedes_aegypti/data/fasta/pep/*.gz"}
        XML2.list['XML'] = []
    * XML3:
        XML3.info['Name'] = 'file'
        XML3.info['Content'] = 'Malaria Mosquito'
        XML3.dict['Atrributes'] = {'path':"current_anopheles_gambiae/data/fasta/pep/*.gz"}
        XML3.list['XML'] = []
    
    The top level XML list (XML.list['XML']) is returned by the parseXML() method of the class, which populates all the
    objects.

Commandline:
    - parse=FILE        : Source file for reading XML file [None]
    - attributes=LIST   : List of Attributes to exclusively extract []
    - elements=LIST     : List of Elements to exclusively extract []

Uses general modules: copy, os, string, sys, time, xml.sax
Uses RJE modules: rje, rje_zen
Other modules needed: None

### ~~~ Module rje_xref ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_xref.py] ~~~ ###

Module:       rje_xref
Description:  Generic identifier cross-referencing 
Version:      1.8.2
Last Edit:    08/01/16
Copyright (C) 2014  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is primarily for use with other programs/modules to handle database cross-referencing. Initially, it is 
    designed to be able to use the output from rje_genemap but might eventually replace this module by generating the 
    xref data from raw data in the first place. Although it is designed and documented with aliases in mind, e.g. mapping
    UniProt IDs to HGNC, it can also be used for more general mapping of, for example, Pfam domains and genes.
    
    This module is designed to work with a main xrefdata table (db table XRef) that contains 1:1:1 values for each
    identifier. Where there are multiple values for a given field, these will be joined with '|' characters (or
    splitchar=X) and likewise split on '|' to generate aliases from the xrefdata table. (These will be sorted if
    sortxref=T. This may slow the program down a little.) Split fields will have whitespace removed. To avoid a field
    being split, use splitskip=LIST.

    If multiple xrefdata files are provided, these will be combined into a single table. The first file becomes the
    master XRef table and any additional input will be added to this table provided that (a) it has a KeyID or AltKeys
    field, or (b) it can be mapped onto an existing KeyID via a given MapField.

    NOTE. If newheaders=LIST is used, it will apply to the FIRST xrefdata table only and the new fields will be used for
    all subsequent processing. All other options should therefore use these new headers. If multiple files need new
    headers, the program may need to be run several times before combining them. If newheaders replaces KeyID then KeyID
    will be updated.

    If fullmap=T then ALL MapFields will be used for mapping, even if this produces multiple mappings. Otherwise, the
    first successful mapping will be used. The KeyID and AltKeys fields are automatically added to the
    front of any MapField list (and thus will be used first for mapping). If an ID List is provided (idlist=LIST) then
    these final XRef data are restricted to the KeyIDs in this list, once all mapping has been done.

    If FileXRef file is given (filexref=FILE) then the XRef data will be mapped onto this file (via MapFields) and
    subsequent output will correspond to the mappings and IDs in this combined file. XRef fields to be added to the file
    can be limited with xrefs=LIST. Sorted (unique) lists of mapped files can be produced by xreflist=LIST. If no
    filexref file is given then output is for the entire XRef data table. Note that the default is to produce this table.

    Some database identifiers are prefixed with the database and a colon. e.g. Entrez gene 10840 may be written
    ENTREZ:10840. This will be recognised and standardised by either removing the prefix or, if the field is in
    dbprefix=LIST, by enforcing the DB:ID format. The database is always taken from the XRef field, so this must match,
    although the case will be switched to uppercase unless in keepcase=LIST.

Commandline:
    ### ~ Input/Field Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    xrefdata=FILES  : List of files with delimited data of identifier cross-referencing (wildcards allowed) []
    newheaders=LIST : List of new Field headers for XRefData (will replace old - must be complete) []
    keepcase=LIST   : Any fields matching keepcase will retain mixed case, otherwise be converted to upper case ['Desc','Description','Name']
    dbprefix=LIST   : List of fields that should have the field added to the ID as a prefix, e.g. HGNC:0001 []
    stripvar=CDICT  : Remove variants using Field:Char list, e.g. Uniprot:-,GenPept:. []
    compress=LIST   : Compress listed fields into lists (using splitchar) to allow 1:many mapping in xrefdata. []
    splitchar=X     : Character on which to split fields for multiple alias processing ['|']
    splitcsv=T/F    : Whether to also split fields based on comma separation [True]
    splitskip=LIST  : List of fields to bypass for field splitting ['Desc','Description','Name']
    sortxref=T/F    : Whether to sort multiple xref data alphabetically [True]
    keyid=X         : Key field header to be used in main Data dictionary - aliases map to this ['Gene']
    comments=LIST   : List of comment line prefixes marking lines to ignore (throughout file) ['//','%']
    xreformat=T/F   : Whether to apply field reformatting to input xrefdata (True) or just xrefs to map (False) [False]
    yeastxref=T/F   : First xrefdata file is a yeast.txt file to convert. (http://www.uniprot.org/docs/yeast.txt)

    ### ~ XRef/Processing Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    altkeys=LIST    : Alternative fields to look for in Alias Data ['Symbol','HGNC symbol']
    onetomany=T/F   : Whether to keep potential one-to-many altkeys IDs [False]
    mapfields=X     : Fields to be used for Alias mapping plus KeyID. (Must be in XRef). []
    maptomany=T/F   : Whether to keep potential one-to-many mapfield IDs [True]
    fullmap=T/F     : Whether to map onto ALL map fields or stop at first hit [False]
    uniquexref=T/F  : Whether to restrict analysis to unique XRef IDs [False]
    mapxref=LIST    : List of identifiers to map to KeyIDs using mapfields []
    filexref=FILE   : File to XRef and expand with xrefs before re-saving []
    badid=LIST      : List of XRef IDs to ignore ['!FAILED!','None','N/A','-']
    aliases=LIST    : Combine XRef fields into single 'Aliases' field (and remove KeyID if found)

    ### ~ Join Method Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    join=LIST       : Run in join mode for list of FILE:key1|...|keyN:JoinField []
    naturaljoin=T/F : Whether to only output entries that join to all tables [False]

    ### ~ Output Options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    basefile=X      : Basefile for output files [Default: filexref or first xrefdata input file w/o path]
    savexref=T/F    : Save the xrefdata table (*.xref.tdt) following compilation of data [True]
    idlist=LIST     : Subset of key IDs to map onto. (All if blank) []
    xrefs=LIST      : List of XRef (or join) fields to keep (blank/* for all) []
    xreflist=LIST   : List of XRef fields to output as sorted (unique) lists (*.*.txt) (* for all) []
    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###

See also rje.py generic commandline options.

### ~~~~ Module rje_zen ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/rje_zen.py] ~~~~ ###

Module:       rje_zen
Description:  Random Zen Wisdom Generator
Version:      1.3.2
Last Edit:    13/07/17
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    Generates random (probably nonsensical) Zen wisdoms. Just for fun.

Commandline:
    wisdoms=X   : Number of Zen Wisdoms to return [10]
    zensleep=X  : Time in seconds to sleep between wisdoms [0]

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None

### ~~~ Module snp_mapper ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/libraries/snp_mapper.py] ~~~ ###

Module:       SNP_Mapper
Description:  SNP consensus sequence to CDS mapping 
Version:      1.2.0
Last Edit:    23/11/17
Copyright (C) 2013  Richard J. Edwards - See source code for GNU License Notice

Function:
    This module is in development. The following documentation should be considered aspirational rather than accurate!

    This module is for mapping SNPs onto a genbank file (or similar feature annotation) and reporting the possible effect
    of SNPs in addition to more detailed output for individual genes and proteins. The main SNP output will be restricted
    to feature reporting and coding changes.

    NOTE: At present, this does not handle indels properly, nor multiple SNPS affecting the same amino acid. It cannot
    deal with introns.

    Primary input for this program is:

    1. Reference genome sequence. This should be a DNA sequence file in which the accession numbers match the `Locus`
    field for the SNP file (below). If a Genbank file is given, the `*.full.fas` file will be used.

    2. A feature table with the headers:
    locus	feature	position	start	end	product	gene_synonym	note	db_xref	locus_tag	details
    If this is not given, a `*.Feature.tdt` file based on `seqin=FILE` will be sought and loaded.

    3. The SNP file (`snpfile=FILE`) should have a `Locus`, `Pos`, `REF` and `ALT` fields, which will be used to map onto
    the features file and uniquely mark variants in the reference genome. If running in (default) altpos=T mode, this
    file will represent the mapping of a single pair of genomes: there will be no multiple-allele entries and the file
    headers should include AltPos and AltLocus for making unique entries. (Reference sequences can map to multiple query
    sequences.) If running on BCF output, alleles will be split on commas but there will be no compiled sequence output.
    (** Not yet implemented! **)

    The default FTBest hierarchy for `*.ftypes.tdt` output is:
        CDS,mRNA,tRNA,rRNA,ncRNA,misc_RNA,gene,mobile_element,LTR,rep_origin,telomere,centromere,misc_feature,intergenic

To be added:
    Sequence output and recognition of BCF files to be added.

Old Function:
    This module reads in an alignment of a coding sequence with a consensus sequence and a list of polymorphic sites
    relative to consensus. Polymorphisms are mapped onto the coding sequence and the desired output produced.

Commandline:
    ### ~ Basic SNP mapping functions ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    seqin=FILE      : Sequence input file with accession numbers matching Locus IDs, or Genbank file.  []
    spcode=X        : Overwrite species read from file (if any!) with X if generating sequence file from genbank [None]
    ftfile=FILE     : Input feature file (locus,feature,position,start,end) [*.Feature.tdt]
    ftskip=LIST     : List of feature types to exclude from analysis [source]
    ftbest=LIST     : List of features to exclude if earlier feature in list overlaps position [(see above)]
    snpbyftype=T/F  : Whether to output mapped SNPs by feature type (before FTBest filtering) [False]
    snpfile=FILE    : Input table of SNPs to map and output (should have locus and pos info, see above) []
    snphead=LIST    : List of SNP file headers []
    snpdrop=LIST    : List of SNP fields to drop []
    altpos=T/F      : Whether SNP file is a single mapping (with AltPos) (False=BCF) [True]
    altft=T/F       : Use AltLocus and AltPos for feature mapping (if altpos=T) [False]
    basefile=FILE   : Root of output file names (same as SNP input file by default) []
    
    ### ~ Old Options (need reviving) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    batch=LIST  : List of alignment files to read X.aln - must have X_polymorphisms.txt too [*.aln]
    screenmatch=LIST: List of genotypes for which to screen out matching SNPs []

    ### ~ Obsolete Options (roll back to pre v0.3.0 if required) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    genotypes=LIST  : List of snpfile headers corresponding to genotypes to map SNPs []
    genbase=FILE    : Basefile for Genbank output. Will use base of seqin if None []
    snpkeys=LIST    : Additional headers to use as keys for SNP file (e.g. if mapping done by chromosome) []

See also rje.py generic commandline options.

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje, rje_db, rje_obj, rje_seqlist, rje_zen
Other modules needed: None




-legacy:

### ~~~~ Module gopher_V2 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/legacy/gopher_V2.py] ~~~~ ###

Program:      GOPHER
Description:  Generation of Orthologous Proteins from High-Throughput Estimation of Relationships
Version:      2.9
Last Edit:    28/11/12
Citation:     Davey, Edwards & Shields (2007), Nucleic Acids Res. 35(Web Server issue):W455-9. [PMID: 17576682]
Copyright (C) 2005 Richard J. Edwards - See source code for GNU License Notice

Function:
    This script is designed to take in two sequences files and generate datasets of orthologous sequence alignments.
    The first [seqin] sequence set is the 'queries' around which orthologous datasets are to be assembled. This is now
    optimised for a dataset consisting of one protein per protein-coding gene, although splice variants should be dealt
    with OK and treated as paralogues. This will only cause problems if the postdup=T option is used, which restricts
    orthologues returned to be within the last post-duplication clade for the sequence.

    The second [orthdb] is the list of proteins from which the orthologues will be extracted. The seqin sequences are
    then BLASTed against the orthdb and processed (see below) to retain putative orthologues using an estimation of the
    phylogenetic relationships based on pairwise sequences similarities.

    NB. As of version 2.0, gopher=FILE has been replaced with seqin=FILE for greater rje python consistency. The allqry
    option has been removed. Please cleanup the input data into a desired non-redundant dataset before running GOPHER.
    (In many ways, GOPHER's strength is it's capacity to be run for a single sequence of interest rather than a whole
    genome, and it is this functionality that has been concentrated on for use with PRESTO and SLiM Pickings etc.) The
    output of statistics for each GOPHER run has also been discontinued for now but may be reintroduced with future
    versions. The phosalign command (to produce a table of potential phosphorylation sites (e.g. S,T,Y) across
    orthologues for special conservation of phosphorylation prediction analyses) has also been discontinued for now.

    Version 2.1 has tightened up on the use of rje_seq parameters that were causing trouble otherwise. It is now the
    responsibility of the user to make sure that the orthologue database meets the desired criteria. Duplicate accession
    numbers will not be tolerated by GOPHER and (arbitrary) duplicates will be deleted if the sequences are the same, or
    renamed otherwise. Renaming may cause problems later. It is highly desirable not to have two proteins with the same
    accession number but different amino acid sequences. The following commands are added to the rje_seq object when input
    is read: accnr=T unkspec=F specnr=F gnspacc=T. Note that unknown species are also not permitted.

    The process for dataset assembly is as follows for each protein :

    1. BLAST against orthdb [orthblast]
        > BLASTs saved in BLAST/AccNum.blast
    2. Work through BLAST hits, indentifying paralogues (query species duplicates) and the closest homologue from each
    other species. This involves a second BLAST of the query versus original BLAST hits (e-value=10, no complexity
    filter). The best sequence from each species is kept, i.e. the one with the best similarity to the query and not part
    of a clade with any paralogue that excludes the query. (If postdup=T, the hit must be in the query's post duplication
    clade.) In addition hits:  [orthfas]
        * Must have minimum identity level with Query
        * Must be one of the 'good species' [goodspec=LIST]
        > Save reduced sequences as ORTH/AccNum.orth.fas
        > Save paralogues identified (and meeting minsim settings) in PARA/AccNum.para.fas
    3. Align sequences with MUSCLE  [orthalign]
        > ALN/AccNum.orthaln.fas
    4. Generate an unrooted tree with (ClustalW or PHYLIP)  [orthtree]
        > TREE/AccNum.orth.nsf

    Optional paralogue/subfamily output:  (These are best not used with Force=T or FullForce=T)
    2a. Alignment of query protein and any paralogues >minsim threshold (paralign=T/F). The parasplice=T/F controls
    whether splice variants are in these paralogue alignments (where identified using AccNum-X notation).
        > PARALN/AccNum.paraln.fas
    2b. Pairwise combinations of paralogues and their orthologues aligned, with "common" orthologues removed from the
    dataset, with a rooted tree and group data for BADASP analysis etc. (parafam=T)
        > PARAFAM/AccNum+ParaAccNum.parafam.fas
        > PARAFAM/AccNum+ParaAccNum.parafam.nsf
        > PARAFAM/AccNum+ParaAccNum.parafam.grp
    2c. Combined protein families consisting of a protein, all the paralogues > minsim and all orthologues for each in a
    single dataset. Unaligned. (gopherfam=T)
        > SUBFAM/AccNum.subfam.fas
    *NB.* The subfamily outputs involve Gopher calling itself to ensure the paralogues have gone through the Gopher
    process themselves. This could potentially cause conflict if forking is used.

Commandline:
    ### Basic Input/Output ###
    seqin=FILE      : Fasta file of 'query' sequences for orthology discovery []
    orthdb=FILE     : Fasta file with pool of sequences for orthology discovery []. Should contain query sequences.
    startfrom=X     : Accession Number / ID to start from. (Enables restart after crash.) [None]
    dna=T/F         : Whether to analyse DNA sequences (not optimised) [False]

    ### GOPHER run control parameters ###
    orthblast   : Run to blasting versus orthdb (Stage 1).
    orthfas     : Run to output of orthologues (Stage 2). 
    orthalign   : Run to alignment of orthologues (Stage 3).
    orthtree    : Run to tree-generation (Stage 4). [default!]

    ### GOPHER Orthologue identifcation Parameters ###
    postdup=T/F     : Whether to align only post-duplication sequences [False]
    reciprocal=T/F  : Use Reciprocal Best Hit method instead of standard GOPHER approach [False]
    minsim=X        : Minimum %similarity of Query for each "orthologue" [40.0]
    simfocus=X      : Style of similarity comparison used for MinSim and "Best" sequence identification [query]
        - query = %query must > minsim (Best if query is ultimate focus and maximises closeness of returned orthologues)
        - hit = %hit must > minsim (Best if lots of sequence fragments are in searchdb and should be retained)
        - either = %query > minsim OR %hit > minsim (Best if both above conditions are true)
        - both = %query > minsim AND %hit > minsim (Most stringent setting)
    gablamo=X       : GABLAMO measure to use for similarity measures [Sim]
        - ID = %Identity (from BLAST)
        - Sim = %Similarity (from BLAST)
        - Len = %Coverage (from BLAST)
        - Score = BLAST BitScore (Reciprocal Match only)
    goodX=LIST      : Filters where only sequences meeting the requirement of LIST are kept.
                      LIST may be a list X,Y,..,Z or a FILE which contains a list [None]
                        - goodacc  = list of accession numbers
                        - goodseq  = list of sequence names
                        - goodspec = list of species codes
                        - gooddb   = list of source databases
                        - gooddesc = list of terms that, at least one of which must be in description line
    badX=LIST       : As goodX but excludes rather than retains filtered sequences

    ### Additional run control options ###
    repair=T/F      : Repair mode - replace previous files if date mismatches or files missing.
                      (Skip missing files if False) [True]
    force=T/F       : Whether to force execution at current level even if results are new enough [False]
    fullforce=T/F   : Whether to force current and previous execution even if results are new enough [False]
    dropout=T/F     : Whether to "drop out" at earlier phases, or continue with single sequence [False]
    ignoredate=T/F  : Ignores the age of files and only replaces if missing [False]
    savespace=T/F   : Save space by deleting intermediate blast files during orthfas [True]
    maxpara=X       : Maximum number of paralogues to consider (large gene families can cause problems) [50]

    ### Additional Output Options ###
    runpath=PATH    : Specify parent directory in which to output files [./]
    paralign=T/F    : Whether to produce paralogue alignments (>minsim) in PARALN/ (assuming run to orthfas+) [False]
    parasplice=T/F  : Whether splice variants (where identified) are counted as paralogues [False]
    parafam=T/F     : Whether to paralogue paired subfamily alignments (>minsim) (assuming run to orthfas+) [False]
    gopherfam=T/F   : Whether to combined paralogous gopher orthologues into protein families (>minsim) (assuming run to orthfas+) [False]
    sticky=T/F      : Switch on "Sticky Orthologous Group generation" [False] *** Experimental ***
    stiggid=X       : Base for Stigg ID numbers [STIGG]

Uses general modules: copy, gc, glob, os, string, sys, threading, time
Uses RJE modules: rje, rje_blast, rje_dismatrix, rje_seq, rje_tree
Other modules needed: rje_ancseq, rje_pam, rje_sequence, rje_tree_group, rje_uniprot

### ~~~ Module qslimfinder_V1.9 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/legacy/qslimfinder_V1.9.py] ~~~ ###

Program:      QSLiMFinder
Description:  Query Short Linear Motif Finder
Version:      1.9
Last Edit:    11/07/14
Citation:     Edwards, Davey & Shields (2007), PLoS ONE 2(10): e967. [PMID: 17912346]
Copyright (C) 2008  Richard J. Edwards - See source code for GNU License Notice

Function:
    QSLiMFinder is a modification of the basic SLiMFinder tool to specifically look for SLiMs shared by a query sequence
    and one or more additional sequences. To do this, SLiMBuild first identifies all motifs that are present in the query
    sequences before removing it (and its UPC) from the dataset. The rest of the search and stats takes place using the
    remainder of the dataset but only using motifs found in the query. The final correction for multiple testing is made
    using a motif space defined by the original query sequence, rather than the full potential motif space used by the
    original SLiMFinder. This is offset against the increased probability of the observed motif support values due to the
    reduction of support that results from removing the query sequence but could potentially still identify SLiMs will
    increased significance.

    Note that minocc and ambocc values *include* the query sequence, e.g. minocc=2 specifies the query and ONE other UPC.    
    
Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ### 
    seqin=FILE      : Sequence file to search [None]
    batch=LIST      : List of files to search, wildcards allowed. (Over-ruled by seqin=FILE.) [*.dat,*.fas]
    query=LIST      : Return only SLiMs that occur in 1+ Query sequences (Name/AccNum/Seq Number) [1]
    addquery=FILE   : Adds query sequence(s) to batch jobs from FILE [None]
    maxseq=X        : Maximum number of sequences to process [500]
    maxupc=X        : Maximum UPC size of dataset to process [0]
    sizesort=X      : Sorts batch files by size prior to running (+1 small->big; -1 big->small; 0 none) [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main QSLiMFinder results table [qslimfinder.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [QSLiMFinder/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMFinder/]
    force=T/F       : Force re-running of BLAST, UPC generation and SLiMBuild [False]
    pickup=T/F      : Pick-up from aborted batch run by identifying datasets in resfile using RunID [False]
    dna=T/F         : Whether the sequences files are DNA rather than protein [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMBuild Options I: Evolutionary Filtering  ###
    efilter=T/F     : Whether to use evolutionary filter [True]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    homcut=X        : Max number of homologues to allow (to reduce large multi-domain families) [0]

    ### SLiMBuild Options II: Input Masking ###
    masking=T/F     : Master control switch to turn off all masking if False [True]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out [EM]
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M (can be useful if termini=T) [True]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []
    qregion=X,Y     : Mask all but the region of the query from (and including) residue X to residue Y [0,-1]
    
    ### SLiMBuild Options III: Basic Motif Construction ###
    termini=T/F     : Whether to add termini characters (^ & $) to search sequences [True]
    minwild=X       : Minimum number of consecutive wildcard positions to allow [0]
    maxwild=X       : Maximum number of consecutive wildcard positions to allow [2]
    slimlen=X       : Maximum length of SLiMs to return (no. non-wildcard positions) [5]
    minocc=X        : Minimum number of unrelated occurrences for returned SLiMs. (Proportion of UP if < 1) [0.05]
    absmin=X        : Used if minocc<1 to define absolute min. UP occ [3]
    alphahelix=T/F  : Special i, i+3/4, i+7 motif discovery [False]

    ### SLiMBuild Options IV: Ambiguity ###
    ambiguity=T/F   : (preamb=T/F) Whether to search for ambiguous motifs during motif discovery [True]
    ambocc=X        : Min. UP occurrence for subvariants of ambiguous motifs (minocc if 0 or > minocc) [0.05]
    absminamb=X     : Used if ambocc<1 to define absolute min. UP occ [2]
    equiv=LIST      : List (or file) of TEIRESIAS-style ambiguities to use [AGS,ILMVF,FYW,FYH,KRH,DE,ST]
    wildvar=T/F     : Whether to allow variable length wildcards [True]
    combamb=T/F     : Whether to search for combined amino acid degeneracy and variable wildcards [False]

    ### SLiMBuild Options V: Advanced Motif Filtering ###
    musthave=LIST   : Returned motifs must contain one or more of the AAs in LIST (reduces search space) []
    focus=FILE      : FILE containing focal groups for SLiM return (see Manual for details) [None]
    focusocc=X      : Motif must appear in X+ focus groups (0 = all) [0]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMChance Options ###
    cloudfix=T/F    : Restrict output to clouds with 1+ fixed motif (recommended) [False]
    slimchance=T/F  : Execute main QSLiMFinder probability method and outputs [True]
    sigprime=T/F    : Calculate more precise (but more computationally intensive) statistical model [False]
    sigv=T/F        : Use the more precise (but more computationally intensive) fix to mean UPC probability [False]
    qexact=T/F      : Calculate exact Query motif space (True) or over-estimate from dimers (False) (quicker) [True]
    probcut=X       : Probability cut-off for returned motifs [0.1]
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [False]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) (!!!Experimental!!!) [None]
    negatives=FILE  : Multiply raw probabilities by under-representation in FILE (!!!Experimental!!!) [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=T/F      : Whether to upweight for multiple occurrences in same sequence (heuristic) [False]
    probscore=X     : Score to be used for probability cut-off and ranking (Prob/Sig) [Sig]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Advanced Output Options I: Output data ###
    clouds=X        : Identifies motif "clouds" which overlap at 2+ positions in X+ sequences (0=minocc / -1=off) [2]
    runid=X         : Run ID for resfile (allows multiple runs on same data) [DATE:TIME]
    logmask=T/F     : Whether to log the masking of individual sequences [True]
    slimcheck=FILE  : Motif file/list to add to resfile output [] 

    ### Advanced Output Options II: Output formats ###
    teiresias=T/F   : Replace TEIRESIAS, making *.out and *.mask.fasta files [False]
    slimdisc=T/F    : Emulate SLiMDisc output format (*.rank & *.dat.rank + TEIRESIAS *.out & *.fasta) [False]
    extras=X        : Whether to generate additional output files (alignments etc.) [1]
                        --1 = No output beyond main results file
                        - 0 = Generate occurrence file and cloud file
                        - 1 = Generate occurrence file, alignments and cloud file
                        - 2 = Generate all additional QSLiMFinder outputs
                        - 3 = Generate SLiMDisc emulation too (equiv extras=2 slimdisc=T)
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle
                        - 2 = Delete all bar *.upc (pickle added to tar)
                        - 3 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)

    ### Advanced Output Options III: Additional Motif Filtering ### 
    topranks=X      : Will only output top X motifs meeting probcut [1000]
    minic=X         : Minimum information content for returned motifs [2.1]
    allsig=T/F      : Whether to also output all SLiMChance combinations (Sig/SigV/SigPrime/SigPrimeV) [False]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    

### ~~~ Module slimbench_V1 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/legacy/slimbench_V1.py] ~~~ ###

Module:       SLiMBench
Description:  Short Linear Motif prediction Benchmarking
Version:      1.9
Last Edit:    06/08/13
Copyright (C) 2011  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMBench has two primary functions:

    1. Generating SLiM prediction benchmarking datasets from ELM (or other data in a similar format). This includes
    options for generating random and/or simulated datasets for ROC analysis etc.

    2. Assessing the results of SLiM predictions against a Benchmark. This program is designed to work with SLiMFinder
    and QSLiMFinder output, so some prior results parsing may be needed for other methods.

    Documentation for SLiMBench is currently under development. Please contact the author for more details.

Commandline:
    ### ~ INPUT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    sourcepath=PATH/    : Will look in this directory for input files if not found ['SourceData/']
    elmclass=FILE       : Download from ELM website of ELM classes ['elm_classes.tsv']
    elminstance=FILE    : Download from ELM website of ELM instances ['elm_instances.tsv']
    elmpfam=FILE        : Download from ELM website of ELM Pfam domain interactors ['elm_interaction_domains.tsv']
    uniprot=FILE        : File of downloaded UniProt entries (See rje_uniprot for more details) ['ELM.dat']

    ### ~ ELM BENCHMARK GENERATION OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    genpath=PATH        : Output path for datasets generated with SLiMBench file generator [./SLiMBenchDatasets/]
    integrity=T/F       : Whether to quit by default if input integrity is breached [True]
    generate=T/F        : Whether to generate SLiMBench datasets from ELM input [False]
    slimmaker=T/F       : Whether to use SLiMMaker to "reduce" ELMs to more findable SLiMs [True]
    minupc=X            : Minimum number of UPC for ELM dataset [True]
    minic=X             : Min information content for a motif (1 fixed position = 1.0) [2.0]
    queries=T/F         : Whether to generate datasets with specific Query proteins [True]
    flankmask=LIST      : List of flanking mask options [none,win300,win100,flank5,site]
    searchini=LIST      : List of INI files containing search options (should have runid setting) []

    ### ~ ELM PPI/3DID BENCHMARK GENERATION OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    elmpfam=FILE        : Download from ELM website of ELM Pfam domain interactors ['elm_interaction_domains.tsv']
    pfamdata=FILE       : File mapping PFam domains onto genes/proteins (BioMart or HMM search) []
    xrefdata=FILE       : File of gene identifier cross-reference data from rje_genemap []
    3didsql=PATH        : Path to 3DID sql data. Use rje_mysql sqldump to extract 3DID DMI data. []
    dmidata=FILE        : File of 3DID DMI data ['3did.DMI.csv']
    pdbdata=FILE        : File mapping PDB identifiers onto genes/proteins []

    ### ~ RANDOM/SIMULATION BENCHMARK GENERATION OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    simulate=T/F        : Whether to generate simulated datasets using reduced ELMs (if found) [False]
    randomise=T/F       : Whether to generate randomised datasets (part of simulation if simulate=T) [False]
    randreps=X          : Number of replicates for each random (or simulated) datasets [10]
    simratios=LIST      : List of simulated ELM:Random rations [1,4,9,19]
    simcount=LIST       : Number of "TPs" to have in dataset [5,10]
    randir=PATH         : Output path for creation of randomised datasets [./SLiMBenchDatasets/Random/]
    randbase=X          : Base for random dataset name if simulate=F [ran]
    randsource=FILE     : Source for new sequences for random datasets [None]
    masking=T/F         : Whether to use SLiMCore masking for query selection [True]

    ### ~ BENCHMARK ASSESSMENT OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    benchmark=T/F       : Whether to perfrom SLiMBench benchmarking assessment against motif file [False]
    datatype=X          : Type of data to be generated and/or benchmarked (elm/sim/simonly) [elm]
    resfiles=LIST       : List of (Q)SLiMFinder results files to use for benchmarking [*.csv]
    compdb=FILE         : Motif file to be used for benchmarking (default = reduced elmclass file) []
    benchbase=X         : Basefile for SLiMBench benchmarking output [slimbench]
    runid=LIST          : List of factors to split RunID column into (on '.') ['Program','Analysis']
    bycloud=X           : Whether to compress results into clouds prior to assessment (True/False/Both) [Both]
    sigcut=LIST         : Significance thresholds to use for assessment [0.05,0.01,0.001,0.0001]
    iccut=LIST          : Minimum IC for (Q)SLiMFinder results for benchmark assessment [2.0,2.1,3.0]
    slimlencut=LIST     : List of individual SLiM lengths to return results for (0=All) [0,3,4,5]
    noamb=T/F           : Filter out ambiguous patterns [False]
    # Add CompariMotif settings here for OT/TP etc.

    ### ~ GENERAL OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    force=T/F           : Whether to force regeneration of outputs (True) or assume existing outputs are right [False]
    backups=T/F         : Whether to (prompt if interactive and) generate backups before overwriting files [True]

See also rje.py generic commandline options.

### ~~~ Module slimdisc_V1.4 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/legacy/slimdisc_V1.4.py] ~~~ ###

################################################################################
import sys,os,re,sets,time,math,traceback,getopt,random,tempfile,string,copy,shutil
from threading import Thread

sys.path.append(os.path.join(os.path.dirname(sys.argv[0]),"RJE_modules"))

import rje_uniprot,rje_blast,rje_seq
try:
	from SurfaceAccessibilityPlotter import SurfaceAccessibilityPlotter
	from HomologyPlotter import HomologyPlotter
	from DomainPlotter import DomainPlotter
	from DistributionPlotter import DistributionPlotter
except:
	warning_PIL_not_installed = """
	####################################################################################\n
	The PIL library does not seem to be installed no images or graphs will be generated.
		     To create images associated with the analysis go to 

		           ---------------------------------------
		           http://www.pythonware.com/products/pil/
		           ---------------------------------------
	
			    and download and install PIL library

	#####################################################################################\n
	"""

	if '-Q0' not in sys.argv:
		print warning_PIL_not_installed
		time.sleep(0)
		
	pass


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class fileManipulation:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  fileManipulation

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   readFile(options)
	   @param options

	   createTEIRESIASinputfromProteinList(proteinList,options)
	   @param proteinList
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass 
	#--------------------------------------------------------------------------------------------------------------------#
	def readFileOLD(self,options,filename):
		proteinList = {}
		
		try:
			fileIn = open(filename,"r")	
		except:
			if os.path.isfile(options["filename"]):
				print "Error opening file"
			else:
				errorfile_not_found = "\nERROR:\n" + "+"*70 + "\nInput file does not exist.\nCheck the filepath and try again.\n" + "+"*70
				print errorfile_not_found

			sys.exit()

		sequence = ""
		name = ""
			
		proteinData = fileIn.read().strip()
		pattern1 = re.compile('^>')
		if len(pattern1.findall(proteinData)) == 0:
			print "Input not in a recognisable fasta format.\nIf input in UNIPROT format please use the -LT option"
			sys.exit()
			
		for lines in proteinData.split('\n'):
	
			if lines[0] == '>':
				if len(sequence) > 0:
					proteinList[name] = sequence.strip()
					sequence = ""
				try:
					name = lines.split("|")[1].split()[0]
				except:
					name = lines[1:].split()[0]
			else:
				sequence += lines.replace("\n","")

		proteinList[name] = sequence.strip()

		fileIn.close()
		return proteinList

	def readFile(self,options,filename):
		proteinList = {}
		
		if os.path.isfile(options["filename"]):
			pass
		else:
			errorfile_not_found = "\nERROR:\n" + "+"*70 + "\nInput file does not exist.\nCheck the filepath and try again.\n" + "+"*70
			print errorfile_not_found
			sys.exit()

		seqs = rje_seq.SeqList(log=None,cmd_list=['v=-1','i=-1'])
		seqs.loadSeqs(seqfile=options["filename"],nodup=True)

		for seq in seqs.seq:
			proteinList[seq.info['AccNum']] = seq.info['Sequence']

		return proteinList

	def createTEIRESIASinputfromProteinList(self,proteinList,options):
		outString = ""
		for proteins in proteinList.keys():
			outString += ">" + proteins + " 1\n" + proteinList[proteins] + "\n"

		open(options["TEIRESIAS_input"],"w").write(outString)
	#----------------------------------------------END OF CLASS----------------------------------------------------------#


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class fileManipulationFull:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  fileManipulation

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   readFile(options)
	   @param options

	   createTEIRESIASinputfromProteinList(proteinList,options)
	   @param proteinList
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass 
	#--------------------------------------------------------------------------------------------------------------------#
	def readFile(self,options):
		try:
			fileIn = open(options["filename"],"r")	
		except:
			errorfile_not_found = "\nERROR:\n" + "+"*70 + "\nInput file does not exist.\nCheck the filepath and try again.\n" + "+"*70
			print errorfile_not_found

			sys.exit()		

		proteinData = fileIn.read().strip()
		pattern1 = re.compile('^>')
		
		if len(pattern1.findall(proteinData)) != 0:
			print "Input not in a recognisable UNIPROT format.\nIf input in fasta format please use the -LF option"
			sys.exit()
			
		counter = 0
		mwcounter = 0
		features = {}
		proteinList = {}
		teiresiasInput = []
		offsets = []
		count = 0

		if options['memory_saver'] == 'F':
			try:
				plotter = DomainPlotter()
			except:
				pass
		else:
			pass
			#printLevel(1,options, "Skipping domain plotting")
		
		tempFile = open(options["output"].replace(".rank",".domains.html"),"w")
	
		tempFile.write("<html>")
		tempFile.write("<h2>Domains<br></h2>") 
		
		printLevel(1,options, options["Mask_path"])
		try:
			masksTemp = open(options["Mask_path"],"r").read().strip().split("\n")
			exmasks = []
			inmasks = []
			for mask in masksTemp:
				try:
					if mask.split()[1] == "e":
						exmasks.append(mask.split()[0])
					if mask.split()[1] == "i":
						inmasks.append(mask.split()[0])
				except:
					print mask
		except:
			print "Cannot find mask file"
			sys.exit()

		try:
			exception = open(os.path.join(os.path.dirname(options['Mask_path']),"exceptions.dat"),"r").read().split("\n")
		except:
			print "Cannot find exceptions file"
			open(os.path.join(os.path.dirname(options['Mask_path']),"exceptions.dat"),'w')
			sys.exit()

		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		printLevel(1,options,"Masking TEIRESIAS Input")
		printLevel(1,options,"Inclusive Masking:\t" + str(inmasks))
		printLevel(1,options,"Exclusive Masking:\t" + str(exmasks))

		printLevel(1,options,"exceptions: \t" + str(exception))
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")

		uniprotParser = rje_uniprot.UniProt(cmd_list=['v=-1'])
		uniprotParser.readUniProt(filename=options["filename"])
	
		try:
			maskingString = ''
			for entry in uniprotParser.list['Entry']:
				maskingString += '-'*80 + '\n'
				inclusiveMasking = 0
				entry_name = entry.info['Name']
				sequence = entry.obj['Sequence'].info['Sequence']
				unmasked = [0,len(sequence)]
				maskingString += 'Reading protein ' + entry_name + '\n'

				if entry_name not in proteinList:
					tempFile.write("<h3>" + entry_name + "</h3>\n")

					tempFile.write("<p><img src='" + os.path.abspath("./" + options["input_path"] + "/domains/" + entry_name + ".domain.gif'") + "></p><br>\n")
					proteinList[entry_name] = sequence

					features[entry_name] = {}

					for ft in entry.list['Feature']:

						ftlist = [ft['Start'],ft['End'],ft['Desc']]
						if ft['Type'] in features[entry_name]:
							features[entry_name][ft['Type']].append(list(ftlist))
						else:
							features[entry_name][ft['Type']] = [list(ftlist)]


					splits = []
					temp = sequence
					featureList = []
					
					tempFile.write("<FONT SIZE=2 face='courier'><table width='900' border='1' cellspacing='0' cellpadding='5'>" )
					
					proteinMaskInclusive = copy.deepcopy(list(sequence))
					proteinMaskExclusive = list(sequence)
					
					for x in range(0,len(sequence)):
						proteinMaskInclusive[x] = "x"

					
					maskingString += '\n'
					for inmask in inmasks:
						if inmask in features[entry_name]:
						

							maskingString += 'Inclusive masking ' + inmask + '\n'
							for iter in range(len(features[entry_name][inmask])):
								if features[entry_name][inmask][iter][2].replace('.','') in exception:
									maskingString += 'skipping ' + feature_data[2].replace('.','') + ' (Contained in exceptions file)' + '\n'

									pass
								else:	
									inclusiveMasking = 1
									maskingString +=  features[entry_name][inmask][iter][2].replace('.','') + ' from ' + str(features[entry_name][inmask][iter][0]) + ' to ' + str(features[entry_name][inmask][iter][1]) + '\n'
									for x in range(0,len(sequence)):
										if x >= features[entry_name][inmask][iter][0] -1  and x <= features[entry_name][inmask][iter][1] - 1:
											proteinMaskInclusive[x] = proteinList[entry_name][x]
											
					maskingString += '\n'	
					for featureType in exmasks:
						if featureType in features[entry_name]:
							maskingString += 'Exclusive masking ' + featureType + '\n'
							for feature_data in features[entry_name][featureType]:
								try:
									if feature_data[2].replace('.','') in exception:
										maskingString += 'skipping ' + feature_data[2].replace('.','') + ' (Contained in exceptions file)' + '\n'
	
										pass
									else:
										maskingString +=  feature_data[2].replace('.','') + ' from ' + str(feature_data[0]) + ' to ' + str(feature_data[1]) + '\n'
	
										featureList.append([featureType,feature_data[0],feature_data[1],feature_data[2]])
										tempFile.write("<tr><td align='left'><FONT SIZE=2 face='courier'>" + featureType + "</td><td align='left'><FONT SIZE=2 face='courier'>" + str(feature_data[0]) + "</font></td><td align='left'><FONT SIZE=2 face='courier'><FONT SIZE=2 face='courier'>" + str(feature_data[1]) + "</td><td align='left'><FONT SIZE=2 face='courier'>" + str(feature_data[2]) + "</td></tr>")
										
										
										for i in range(0,len(unmasked) - 1):
											if int(feature_data[0])-1 >= unmasked[i] and int(feature_data[1]) <= unmasked[i + 1]:
												unmasked.insert(i + 1,int(feature_data[0])-1) 
												unmasked.insert(i + 2,int(feature_data[1])) 
												for j in range(int(feature_data[0])-1,int(feature_data[1])):
													proteinMaskExclusive[j] = "x"
								except:
									print feature_data
					
					tempFile.write("</table><br>")
					
					if inclusiveMasking == 0:
						if options['strict_inclusive_masking'] == 'T':
							proteinMask = proteinMaskInclusive
						else:
							proteinMask = proteinMaskExclusive
					else:
						for iter in range(len(proteinMaskInclusive)):
							if proteinMaskExclusive[iter] == 'x':
								 proteinMaskInclusive[iter] = 'x'
							
						proteinMask = proteinMaskInclusive 
						
					maskedString = string.join(proteinMask,"") 
					pattern = re.compile("[^x]+")

					maskingString +=  "\n>" + entry_name + '\n'
					
					for i in range(0,len(maskedString)/60  +1): 
						tempFile.write(maskedString[60*i:60*(i+1)]  + "<br>")
						maskingString +=  maskedString[60*i:60*(i+1)] + '\n'

					iterMatch = pattern.finditer(maskedString)
					for match in iterMatch:
						offsets.append([entry_name,match.span()[0],count])
						teiresiasInput.append([entry_name,match.group()])
				
					if options['memory_saver'] == 'F':
						try:
							plotter.plotDomain(entry_name,featureList,len(sequence),options)
						except:
							maskingString +=  'Skipping plotting domains'
					else:
						pass
						#printLevel(1,options,  "Skipping domain plotting")
						
					count += 1
				else:
					record = iterator.next()
		except:
			errorfile_not_found = "\nERROR:\n" + "+"*70 + "\nFile Format Invalid.\n" + "+"*70
			print errorfile_not_found
			raise
			sys.exit()


		teiresiasInputFile = open(options["TEIRESIAS_input"],"w")

		for i in range(len(teiresiasInput)):
			teiresiasInputFile.write( ">" + teiresiasInput[i][0] + " " + str(i) + "\n") 
			teiresiasInputFile.write( teiresiasInput[i][1]  + "\n") 
		
		
		printLevel(1,options, maskingString)
		sys.stderr.write(maskingString)
		
		return offsets,proteinList



	def createTEIRESIASinputfromProteinList(self,proteinList,options):
		outString = ""
		for proteins in proteinList.keys():
			outString += ">" + proteins + " 1\n" + proteinList[proteins] + "\n"

		open(options["TEIRESIAS_input"],"w").write(outString)
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class RunBLAST:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  RunBLAST

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   runBLAST( proteinData,options)
	   @param  proteinData
	   @param  options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		sys.stderr.write("\n----------------------------------------------------------------\n")
		sys.stderr.write("########################CREATING CLUSTERS#######################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		pass
	#--------------------------------------------------------------------------------------------------------------------#
	def runBLAST(self, proteinData,options):
		blast = BLAST()
		blastParser = parseBLAST()
		
		if options["run_formatDB"] == "T":
			printLevel(1,options,"Formatting BLAST database")
			sys.stderr.write(time.strftime('%X') + ":\n" + "Formatting BLAST database\n")

			blast.createDatabase(options["filename"],proteinData)
				
		blastHits = {}
		blastGlobalHits = {}
		blastGlobalAlignments = {}


		for protein in proteinData:
			open(options["input_path"] + "/query/" + protein + ".seq","w").write(">" + protein + "\n" + proteinData[protein])
			
			if options["run_BLAST"] == "T":
				printLevel(1,options,"Running BLAST searches")
				sys.stderr.write(time.strftime('%X') + ":\n" + "Running BLAST searches\n")
				printLevel(1,options,"Blasting " + protein + " against Database\r",)
				sys.stderr.write(time.strftime('%X') + ":\t" + "Blasting " + protein + " against Database\n")
				blast.queryDatabase(options["filename"],protein,proteinData,options["eVal"])

			printLevel(1,options,options["BLAST_results"]  +protein + ".blastp")
			blastHits[protein] = blastParser.parseBLASTFile(options["BLAST_results"]  + protein + ".blastp", options)
			[blastGlobalHits[protein],blastGlobalAlignments[protein]] = blast.globalAlignment(protein,proteinData)

		
		printLevel(1,options,"Finished BLAST searches\t\t\t\n")
		sys.stderr.write("\n")
			

		sys.stderr.write(time.strftime('%X') + ":\n" + "Finished BLAST searches\n")

		return [blastHits,blastGlobalHits,blastGlobalAlignments]
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class BLAST:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  BLAST

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   createDatabase(filename)
	   @param filename

	   queryDatabase(filename,protein,proteinData,eValue)
	   @param filename
	   @param protein
	   @param proteinData
	   @param eValue

	   collapseList(listTemp)
	   @param listTemp

	   globalAlignment(protein)
	   @param protein

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass
	#--------------------------------------------------------------------------------------------------------------------#	
	def createDatabase(self,filename,proteinData):
		temp = open(filename + ".temp","w")

		for protein in proteinData:
			temp.write(">" + protein + "\n" + proteinData[protein] + "\n")

		temp.close()

		filesuffix = os.path.basename(options["filename"]).split(".")[0]

		args = ["formatdb","-p","T","-i",filename + ".temp","-n",options["input_path"] + "/database/" +filesuffix + "db"]
		blastDir = options["BLAST_path"] + "formatdb"


		try:
			os.spawnv(os.P_WAIT,blastDir,args)
		except:
			errorBLAST_not_found ="ERROR : formatDB cannot be found at: " +  options["BLAST_path"] + ".\n" + "-"*66
			print errorBLAST_not_found
			sys.exit()

		os.remove(filename + ".temp")
	#--------------------------------------------------------------------------------------------------------------------#	
	def queryDatabase(self,filename,protein,proteinData,eValue):
		#open(options["input_path"] + "/query/" + protein + ".seq","w").write(">" + protein + "\n" + proteinData[protein])
		filesuffix = os.path.basename(options["filename"]).split(".")[0]
		
		args = ["blastall","-p","blastp","-d",options["input_path"].replace("\\\\","/") + "/database/" + filesuffix + "db","-i",options["input_path"] + "/query/"  +protein + ".seq","-e",str(float(eValue)),"-m","0","-o",options["input_path"] + "/results/" + protein + ".blastp","-F","F"]
		
 		blastDir = options["BLAST_path"] + "blastall"
 		#print blastDir
		try:
			
			if options['overwrite'] == 'T':
				#print args
				os.spawnv(os.P_WAIT,blastDir,args)		
			else:
				if os.path.exists(options["input_path"] + "/results/" + protein + ".blastp"):
					printLevel(1,options,'Skipping ' + protein + ', blast results file already exists.')
					pass
				else:
					os.spawnv(os.P_WAIT,blastDir,args)
		except:
			errorBLAST_not_found ="ERROR blastall cannot be found at: " +  options["BLAST_path"] + ".\n" + "-"*66
			print errorBLAST_not_found
			sys.exit()

	#--------------------------------------------------------------------------------------------------------------------#
	def collapseList(self,listTemp):
			stringTemp = ""
			for values in listTemp:
				stringTemp += values

			return stringTemp
	#--------------------------------------------------------------------------------------------------------------------#
	def globalAlignment(self,protein,proteinData):
		
		htmlFile = open( options["input_path"] + "/"+"alignments/" + protein + ".alignment.html","w")
		htmlFile.write("<html><PRE>")
		htmlFile.write("<h2>Alignments for " + protein + "<br></h2><h3>Cut-off : \t" + str(options["eVal"]) + "</h3><br>") 

		blast = rje_blast.BLASTRun(cmd_list=['i=-1', 'v=0'])

		blastFile = options["input_path"] + "/results/" + protein + ".blastp"
		
		blast.readBLAST(resfile=blastFile,clear=True)

		
		tempFile = open(options["output"].replace(".rank",".alignments.html"),"a")
		sequence = open(options["input_path"] + "/query/" + protein + ".seq","r").read().split("\n")[1]

		tempString = ""
		globalHits = {}
		globalAlignments = {}
		printBlast = 1

		tempString += "<FONT SIZE=2 face='courier'>Query: " + protein + "<br></FONT><FONT SIZE=1 color='#ff0000' face='courier'>" 
		tempString +=  "<a href='" + "alignments/" +  protein + ".alignment.html" + "'>Graphical View</a></font><br>"
		count = 0

		
		for hit in blast.search[0].hit:
			
			tempFile.write("<FONT SIZE=1 face='courier'>")
			
			tempString += "<BR>Hit: %-20s"%(hit.info['Name'])[1:21] + "<br>"  + "%-5s"%str(hit.stat['E-Value']) + "<br>" + proteinData[protein]  + "<br>"
			htmlFile.write("<h3>" + hit.info['Name']+ "</h3>\n")
			
			check = 0
			count += 1

			matchList = list("x"*len(sequence))
			offsets = []

			for aln in hit.aln:
				bit_score = aln.stat['BitScore']
				queryStart = aln.stat['QryStart']
				subjectLen = len(aln.info['SbjSeq'])
				subject = aln.info['SbjSeq']
				subjectStart = aln.stat['SbjStart']
				match =  aln.info['AlnSeq']
				query = aln.info['QrySeq']
				

				htmlFile.write("<FONT SIZE=1 face='courier'>")
				htmlFile.write("subject : match<br>score  : %s"%bit_score + "<br>e-value: %s"%bit_score + "<BR>")
				htmlFile.write("Query location: %s to %s" % ( queryStart,queryStart + subjectLen) + "<BR>")
				htmlFile.write("Target location: %s to %s" % ( subjectStart,subjectStart + subjectLen) + "<BR>")
				htmlFile.write("\n" + subject.replace(" ",".") + "<BR>")
				htmlFile.write("\n" + match.replace(" ",".") + "<BR>" + query.replace(" ",".")  + "<BR><BR></FONT>")
			 
				offsets.append([[queryStart,queryStart + subjectLen],[subjectStart,subjectStart + subjectLen]])
										
				temp1 = "."*(queryStart - 1)
				temp2 = "."*(queryStart - 1)
				gap = 0
				
				for x in range(subjectLen):
					if query[x] == "-":
						gap += 1
					else:
						temp1 += match[x]
						if match[x] != " ":
							if matchList[x + queryStart - 1 - gap] == "x":
								matchList[x + queryStart - 1 - gap] = match[x]
						else:
							if matchList[x + queryStart - 1 - gap] == "x":
								matchList[x + queryStart - 1 - gap] = "."
						temp2 += query[x]

				

			data = [[protein,hit.info['Name']],[len(proteinData[protein]),len(proteinData[hit.info['Name']])],offsets]
	 		
			try:
				HomologyPlotter().plotHomology(data,options)
			except:
				#print 'Skipping homology plot'
				pass
			
			htmlFile.write("<p><img src='" + os.path.abspath("./" + options["input_path"] + "./alignments/"  + protein + "_" + hit.info['Name'] + ".alignment.gif'") + "></p><br>\n")
		
			alignments = self.collapseList(matchList)
			mismatch = self.collapseList(matchList).count("x")
			partialmatch = self.collapseList(matchList).count("+")
			fullmatch = (len(self.collapseList(matchList))  - (self.collapseList(matchList).count(".") + self.collapseList(matchList).count("x") + self.collapseList(matchList).count("+")))
			globalHits[hit.info['Name']] = float(fullmatch)/len(sequence)
			
			globalAlignments[hit.info['Name']] = self.collapseList(matchList).replace("x",".")

			tempString +=  self.collapseList(matchList) + "\n"

			tempFile.write(tempString + "<br>")
			tempString = ""
			
		del blast
		tempFile.write("<br><br>------------------------------------------------------------------------<br><br>")
		tempFile.close()
		htmlFile.write("</PRE></html>")
		
		return [globalHits,globalAlignments]
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class parseBLAST:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  parseBLAST

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   parseBLASTFile(filename,options)
	   @param filename
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass 
	#--------------------------------------------------------------------------------------------------------------------#
	def parseBLASTFile(self,filename,options):
		
		try:
			fileIn = open(filename,"r")	
		except:
			errorBLAST_file_not_found = "\n\nERROR:\n" + "+"*70 
			errorBLAST_file_not_found += "\nBLAST output file does not exist.\n"
			errorBLAST_file_not_found +="Try running AERPIMP again using the -BT and -DT options.\n" + "+"*70
			print errorBLAST_file_not_found
			sys.exit()

		data = fileIn.read()

		pattern = re.compile("Sequences producing significant alignments:[^>]*")

		hitDict = {}
		try:
			for lines in pattern.findall(data)[0].split("\n")[2:-2]:
				hitDetails =lines.split()

				if hitDetails[2][0] == "e":
					eValue = "1" + hitDetails[2]
				else:
					eValue = hitDetails[2]

				if float(eValue) < options["eVal"]:
					hitDict[hitDetails[0]] = [hitDetails[1],float(eValue)]
			
		except Exception,e:
		
			errorBLAST_file_Long = "\n\nERROR:\n" + "+"*70 
			errorBLAST_file_Long += "\nBLAST output file does not contain any data.\n"
			if options["long_format"] == 'F':
				errorBLAST_file_Long +="Try running SLimdisc again using the -LT option.\n" + "+"*70
				
			print errorBLAST_file_Long
			sys.exit()
	
		
		del data
		del hitDetails
		fileIn.close()
		return hitDict
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Teiresias:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Teiresias

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   teiresias(options)
	   @param options
	#################################################################################################
	"""
	#################################################################################################	

	def __init__(self):
		sys.stderr.write("\n----------------------------------------------------------------\n")
		sys.stderr.write("########################CREATING PATTERNS#######################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		pass
	#--------------------------------------------------------------------------------------------------------------------#
	def teiresias(self,options):
		wallTime =  options["TEIRESIAS_walltime"] * 60
		sleepTime = 0.5

		if options["TEIRESIAS_walltime"] == 0:
			self.teiresiasNonThread(options)
		else:
			print time.ctime()
			print 'walltime == ' + str(wallTime) + ' sec'
			startTime = time.time()
			
			teiresiasThread = TeiresiasThread(options)
			teiresiasThread.start()
			
			
			while teiresiasThread.isAlive():
				print 'TEIRESIAS run time : %-1.3f'%(time.time() - startTime) + '\t(Walltime==' + str(options["TEIRESIAS_walltime"] * 60) +  ')\r',
				sys.stdout.flush()
				
				if (time.time() - startTime) >  wallTime:
					print '\n********WALLTIME of ' + str(options["TEIRESIAS_walltime"])  + ' minutes reached ********'
					
					print 'Killing TEIRESIAS ' + str(teiresiasThread.pid)
					
					if sys.platform[0:3] == 'win':
						os.system('taskkill /F /PID ' + str(teiresiasThread.pid))
					else:
						os.system('kill -KILL ' + str(teiresiasThread.pid))
					
					print '**' + '*' * len('******WALLTIME of ' + str(options["TEIRESIAS_walltime"])  + ' minutes reached ********')
					sys.exit()
					
				time.sleep(sleepTime)
		
	#--------------------------------------------------------------------------------------------------------------------#		
	def teiresiasNonThread(self,options):
		if options["TEIRESIAS_equiv"] == 'F':
			equiv = '-p'
		elif options["TEIRESIAS_equiv"] == 'T':
			if os.path.exists(options["calling_folder"] + "/Teiresias/equiv.txt"):
				
				equiv = "-b" + options["calling_folder"] + "/Teiresias/equiv.txt"
				
				printLevel(1,options, "%-49s"%"Teiresias/equiv.txt" + "\t" + "present")
			else:
				print os.environ 
				equiv = '-p'
				print "#"*50 +'\nTeiresias equiv file path does not exist\n' + "Teiresias/equiv.txt" + '\nRunning without ambiguity\n' + "#"*50
				
		elif len(options["TEIRESIAS_equiv"]) > 1:
			
			if os.path.exists(options["TEIRESIAS_equiv"]):
				equiv = '-b' + options["TEIRESIAS_equiv"] 
				printLevel(1,options, "%-49s"%equiv + "\t" + "present")
			else:
				equiv = '-p'
				print "#"*50 +'\nTeiresias equiv file path does not exist\n' + options["TEIRESIAS_equiv"]+ '\nRunning without ambiguity\n' + "#"*50
			
		tempFile = tempfile.mktemp(".txt") 
		

		if options["TEIRESIAS_local(IBM_path_length_bug_fix)"] == 'T':
			args = [options["TEIRESIAS_path"] + 'teiresias_char',"-v","-r","-i" + options["TEIRESIAS_input"],"-n2","-o"+tempFile,"-l"+str(options["TEIRESIAS_fixedPositions"]),"-w"+str(options["TEIRESIAS_patternLength"]),"-c1","-k"+str(options["TEIRESIAS_supportString"]),"-p",equiv]
		else:
			args = [options["TEIRESIAS_path"] + 'teiresias_char',"-v","-r","-i" + options["TEIRESIAS_input"],"-n2","-o"+options["TEIRESIAS_output"],"-l"+str(options["TEIRESIAS_fixedPositions"]),"-w"+str(options["TEIRESIAS_patternLength"]),"-c1","-k"+str(options["TEIRESIAS_supportString"]),"-p",equiv]
		
		try:
			if options['overwrite'] == 'T':
				printLevel(1,options, "Running TEIRESIAS")
				sys.stderr.write("Running TEIRESIAS\n")
				#print options["TEIRESIAS_path"] + "teiresias_char " + string.join(args[1:],' ')
				#info = os.popen(options["TEIRESIAS_path"] + "teiresias_char " + string.join(args[1:],' '))
				processId = os.spawnv(os.P_NOWAIT,args[0],args)
				self.pid  = processId
				#print self.pid
				#print args
				os.waitpid(processId,0)
				
				
				if options['TEIRESIAS_local(IBM_path_length_bug_fix)']  == 'T':
					try:
						os.remove(options["TEIRESIAS_output"])
					except:
						pass
					
					try:
						shutil.copyfile(tempFile,options["TEIRESIAS_output"])
						os.remove(tempFile)
					except:
						print 'Unable to find temporary TEIRESIAS files'
	
			else:
				if os.path.exists(options["TEIRESIAS_output"]):
					print 'TEIRESIAS output already exists'
					pass
				else:
					printLevel(1,options, "Overwriting TEIRESIAS output")
					sys.stderr.write("Running TEIRESIAS\n")
					processId = os.spawnv(os.P_NOWAIT,args[0],args)
					self.pid  = processId
					os.waitpid(processId,0)
	
					print options["TEIRESIAS_path"] + "teiresias_char " + string.join(args[1:],' ')
	
					if options['TEIRESIAS_local(IBM_path_length_bug_fix)']  == 'T':
						print tempFile,options["TEIRESIAS_output"]
					
						try:
							os.remove(options["TEIRESIAS_output"])
						except:
							pass
							
						try:
							shutil.copyfile(tempFile,options["TEIRESIAS_output"])
							os.remove(tempFile)
						except:
							print 'Unable to find temporary TEIRESIAS files'
	
			#os.spawnv(os.P_WAIT,options["TEIRESIAS_path"] + "teiresias_char",args)
		except:
			errorBLAST_not_found ="ERROR : while running TEIRESIAS "
			print errorBLAST_not_found
	#----------------------------------------------END OF CLASS----------------------------------------------------------#


class TeiresiasThread(Thread):
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Teiresias

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   teiresias(options)
	   @param options
	#################################################################################################
	"""
	#################################################################################################	

	def __init__ (self,option):
		Thread.__init__(self)
		self.status = -1
     		self.pid = 0
     		self.options = option
     		
	#--------------------------------------------------------------------------------------------------------------------#
	def run(self):
		self.time = 20
		self.teiresias()
	#--------------------------------------------------------------------------------------------------------------------#
	def teiresias(self):
	
		if options["TEIRESIAS_equiv"] == 'F':
			equiv = '-p'
		elif options["TEIRESIAS_equiv"] == 'T':
			if os.path.exists("Teiresias/equiv.txt"):
				equiv = "-bTeiresias/equiv.txt"
				printLevel(1,options, "%-49s"%"Teiresias/equiv.txt" + "\t" + "present")
			else:
				equiv = '-p'
				print "#"*50 +'\nTeiresias equiv file path does not exist\n' + "Teiresias/equiv.txt" + '\nRunning without ambiguity\n' + "#"*50
			
		elif len(options["TEIRESIAS_equiv"]) > 1:
			
			if os.path.exists("Teiresias/equiv.txt"):
				equiv = '-b' + options["TEIRESIAS_equiv"] 
				printLevel(1,options, "%-49s"%equiv + "\t" + "present")
			else:
				equiv = '-p'
				print "#"*50 +'\nTeiresias equiv file path does not exist\n' + options["TEIRESIAS_equiv"] + '\nRunning without ambiguity\n' + "#"*50
			
		tempFile = tempfile.mktemp(".txt") 
		

		if options["TEIRESIAS_local(IBM_path_length_bug_fix)"] == 'T':
			args = [options["TEIRESIAS_path"] + 'teiresias_char',"-v","-r","-i" + options["TEIRESIAS_input"],"-n2","-o"+tempFile,"-l"+str(options["TEIRESIAS_fixedPositions"]),"-w"+str(options["TEIRESIAS_patternLength"]),"-c1","-k"+str(options["TEIRESIAS_supportString"]),"-p",equiv]
		else:
			args = [options["TEIRESIAS_path"] + 'teiresias_char',"-v","-r","-i" + options["TEIRESIAS_input"],"-n2","-o"+options["TEIRESIAS_output"],"-l"+str(options["TEIRESIAS_fixedPositions"]),"-w"+str(options["TEIRESIAS_patternLength"]),"-c1","-k"+str(options["TEIRESIAS_supportString"]),"-p",equiv]
		
		try:
			if self.options['overwrite'] == 'T':
				printLevel(1,self.options, "Running TEIRESIAS")
				sys.stderr.write("Running TEIRESIAS\n")
				print self.options["TEIRESIAS_path"] + "teiresias_char " + string.join(args[1:],' ')
				#info = os.popen(self.options["TEIRESIAS_path"] + "teiresias_char " + string.join(args[1:],' '))
				processId = os.spawnv(os.P_NOWAIT,args[0],args)
				self.pid  = processId
				#print self.pid
				#print args
				os.waitpid(processId,0)
				
				
				if self.options['TEIRESIAS_local(IBM_path_length_bug_fix)']  == 'T':
					try:
						os.remove(self.options["TEIRESIAS_output"])
					except:
						pass
					
					try:
						shutil.copyfile(tempFile,self.options["TEIRESIAS_output"])
						os.remove(tempFile)
					except:
						print 'Unable to find temporary TEIRESIAS files'
	
			else:
				if os.path.exists(self.options["TEIRESIAS_output"]):
					print 'TEIRESIAS output already exists'
					pass
				else:
					printLevel(1,self.options, "Overwriting TEIRESIAS output")
					sys.stderr.write("Running TEIRESIAS\n")
					processId = os.spawnv(os.P_NOWAIT,args[0],args)
					self.pid  = processId
					os.waitpid(processId,0)
	
					print self.options["TEIRESIAS_path"] + "teiresias_char " + string.join(args[1:],' ')
	
					if self.options['TEIRESIAS_local(IBM_path_length_bug_fix)']  == 'T':
						print tempFile,self.options["TEIRESIAS_output"]
					
						try:
							os.remove(self.options["TEIRESIAS_output"])
						except:
							pass
							
						try:
							shutil.copyfile(tempFile,self.options["TEIRESIAS_output"])
							os.remove(tempFile)
						except:
							print 'Unable to find temporary TEIRESIAS files'
	
			#os.spawnv(os.P_WAIT,self.options["TEIRESIAS_path"] + "teiresias_char",args)
		except:
			errorBLAST_not_found ="ERROR : while running TEIRESIAS "
			print errorBLAST_not_found
	
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class ParseTeiresias:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  ParseTeiresias

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   parse(filename,options)
	   @param filename
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		sys.stderr.write("\n----------------------------------------------------------------\n")
		sys.stderr.write("########################PARSING PATTERNS########################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		self.dictPatterns = {}
		self.dictOccurances = {}
	#--------------------------------------------------------------------------------------------------------------------#
	def parse(self,filename,options):
		self.dictPatterns.clear()
		start = time.time()	
		pattern2 = re.compile('\[[^\]]*\]|.')

		scorer = Score()
		complexity = Complexity()

		aminoAcidOcc = scorer.loadaminoAcidOccurances(options["aminoAcidOccuranceFile"])

		try:
			input = open(filename,'r')
		except:
			errorTEIRESIAS_file_not_found = "\nERROR:\n" + "+"*70 + "\nTEIRESIAS output file does not exist.\n"
			
			if options["run_TEIRESIAS"] == 'F':
				errorTEIRESIAS_file_not_found += "Try running SLiMDisc again using the -TT option.\n" 
				
			errorTEIRESIAS_file_not_found += "+"*70
			
			print errorTEIRESIAS_file_not_found
			sys.exit()
			
		pattern1 = re.compile('\S+')
		count = 0
		counter = 0
		
		input.seek(0,2)
		length = input.tell()
		input.seek(0)

		printLevel(1,options,"\nProcessing TEIRESIAS output")
		printLevel(1,options,"%-10s"%"Input Size" + "\t%-10s"%"Finished" )
			
		while input.tell() < length:
			data = input.readlines(10000)
		
			for lines in data:
				counter += 1
				#print lines
				if counter%1000 == 0:	
					if options['verbosity'] > 1:
						print "%-10s"%(str(length/1000) + "kb") + "\t%-10s"%str(str(input.tell()/1000) + "kb") + "\r", 
					
				
				dictTemp = {}
				
				if lines[0] != "#":
					data = pattern1.findall(lines)
					
					number_of_times = data.pop(0)
					number_of_unique_times = data.pop(0)

					pattern = data.pop(0)

					self.dictOccurances[pattern] = number_of_times
		

					while len(data) > 1:
						seq = data.pop()
						offset = data.pop() 

						if offset in dictTemp:
							dictTemp[offset].append(seq)
						else:
							dictTemp[offset] = [seq]



						
					information_content = scorer.calculateScore(pattern,aminoAcidOcc)		
	
					patternParts = pattern2.findall(pattern)
#


					if options["use_filtering"] == "T":
						if information_content > float(options["information_cutOff"]) and len(patternParts) <= int(options["TEIRESIAS_patternLength"]) and len(patternParts) > 2 and complexity.complexity(pattern) > float(options["complexity_cutOff"]):
							count += 1
							self.dictPatterns[pattern] = [dictTemp,information_content]
					else:
						count += 1
						self.dictPatterns[pattern] = [dictTemp,information_content]
					
		if options['verbosity'] > 1:
			print "%-10s"%(str(length/1000) + "kb") + "\t%-10s"%str(str(input.tell()/1000) + "kb") + "\r",
					
		try:		
			outString = "\n" 
	
			outString +=  "Patterns processed            \t:" + str(counter - 5) + "\n"
			outString +=  "Patterns kept                 \t:" + str(count) + "\n"
			outString +=  "Percentage processed          \t:" "%2.2f"%(float(count)/(counter -5)) + "%\n"
			outString +=  "Time			     \t:" + "%2.2f seconds"%float(time.time() - start) + "\n"
			
			printLevel(1,options, "\n\nInput Statistics" + outString)
			sys.stderr.write("Input Statistics" + outString  + "\n")
		except:
			print 'Problem with parsing of TEIRESIAS input'
			
		
		del data
		input.close()
		return [self.dictPatterns,self.dictOccurances]
	#----------------------------------------------END OF CLASS----------------------------------------------------------#

class ParseTeiresiasFull:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  ParseTeiresias

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   parse(filename,options)
	   @param filename
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		sys.stderr.write("\n----------------------------------------------------------------\n")
		sys.stderr.write("########################PARSING PATTERNS########################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		self.dictPatterns = {}
		self.dictOccurances = {}
	#--------------------------------------------------------------------------------------------------------------------#
	def parse(self,filename,offsets,options):
		self.dictPatterns.clear()
		start = time.time()	
		pattern2 = re.compile('\[[^\]]*\]|.')

		scorer = Score()
		complexity = Complexity()

		aminoAcidOcc = scorer.loadaminoAcidOccurances(options["aminoAcidOccuranceFile"])

		try:
			input = open(filename,'r')
		except:
			errorTEIRESIAS_file_not_found = "\nERROR:\n" + "+"*70 + "\nTEIRESIAS output file does not exist.\nTry running AERPIMP again using the -TT option.\n" + "+"*70
			print errorTEIRESIAS_file_not_found
			sys.exit()

		pattern1 = re.compile('\S+')
		count = 0
		counter = 0
		
		input.seek(0,2)
		length = input.tell()
		input.seek(0)

		printLevel(1,options,"\nProcessing TEIRESIAS output")
		printLevel(1,options,"%-10s"%"Input Size" + "\t%-10s"%"Finished" )
			
		while input.tell() < length:
			data = input.readlines(10000)
		
			for lines in data:
				counter += 1

				if counter%1000 == 0:
					if options['verbosity'] > 1:
						print "%-10s"%(str(length/1000) + "kb") + "\t%-10s"%str(str(input.tell()/1000) + "kb") + "\r",
					
				
				dictTemp = {}
				
				if lines[0] != "#":
					
					data = pattern1.findall(lines)
					
					number_of_times = data.pop(0)
					number_of_unique_times = data.pop(0)

					pattern = data.pop(0)

					self.dictOccurances[pattern] = number_of_times
		

					while len(data) > 1:
						
						seq = data.pop()
						offset = data.pop() 
					
						if offsets[int(offset)][2] in dictTemp:
							dictTemp[offsets[int(offset)][2]].append(int(seq) + int(offsets[int(offset)][1]))
						else:
							dictTemp[offsets[int(offset)][2]] = [int(seq) + int(offsets[int(offset)][1])]

				
					information_content = scorer.calculateScore(pattern,aminoAcidOcc)		
	
					patternParts = pattern2.findall(pattern)

					if options["use_filtering"] == "T":
				
						if information_content > float(options["information_cutOff"]) and len(patternParts) <= int(options["TEIRESIAS_patternLength"]) and len(patternParts) > 2 and complexity.complexity(pattern) >= float(options["complexity_cutOff"]):
							count += 1
						
							self.dictPatterns[pattern] = [dictTemp,information_content]
					else:
						count += 1
						self.dictPatterns[pattern] = [dictTemp,information_content]
					

		printLevel(1,options,"%-10s"%(str(length/1000) + "kb") + "\t%-10s"%str(str(input.tell()/1000) + "kb") + "\r",)
					
			
		outString = "\n" 

		outString +=  "Patterns processed            \t:" + str(counter - 5) + "\n"
		outString +=  "Patterns kept                 \t:" + str(count) + "\n"
	
		try:	
			outString +=  "Percentage processed          \t:" "%2.2f"%(float(count)/(counter -5)) + "%\n"
		except:
			pass
			
		outString +=  "Time			     \t:" + "%2.2f seconds"%float(time.time() - start) + "\n"
			
		printLevel(1,options,"\n\nInput Statistics" + outString)
		sys.stderr.write("Input Statistics" + outString  + "\n")
		
		del data
		input.close()
		return [self.dictPatterns,self.dictOccurances]
	#----------------------------------------------END OF CLASS----------------------------------------------------------#


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class aaOccurance:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	   

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		self.aaDict = {'A':0,'R':0,'N':0,'D':0,'C':0,'E':0,'G':0,'H':0,'I':0,'L':0,'K':0,'M':0,'F':0,'P':0,'S':0,'T':0,'W':0,'Y':0,'V':0,'Q':0,'X':0,'B':0,'Z':0}
	#--------------------------------------------------------------------------------------------------------------------#

	def resetDict(self):
		self.aaDict = {'A':0,'R':0,'N':0,'D':0,'C':0,'E':0,'G':0,'H':0,'I':0,'L':0,'K':0,'M':0,'F':0,'P':0,'S':0,'T':0,'W':0,'Y':0,'V':0,'Q':0,'X':0,'B':0,'Z':0}
	#--------------------------------------------------------------------------------------------------------------------#
	def countAAbyAcc(self,options,proteinData):
		self.resetDict()
		count = 0

		for protein in proteinData.keys():
			sequence = proteinData[protein]
			count += len(sequence)

			try:
				for aminoAcids in sequence:
					self.aaDict[aminoAcids] += 1
			except:
				pass
		return count
	#--------------------------------------------------------------------------------------------------------------------#
	def createAAOccuranceFilefromProteinList(self,options,proteinData):
		count = self.countAAbyAcc(options,proteinData)
		self.printOutput(options,count)
	#--------------------------------------------------------------------------------------------------------------------#
	def printOutput(self,options,count):

		output = open(options["aminoAcidOccuranceFile"],'w')
		
		for aminoAcids in self.aaDict:
			try:
				output.write(aminoAcids + " %1.3f\n"%(float(self.aaDict[aminoAcids])/count))
			except:
				print 'Error calculating amino acid probabilities'
				sys.exit()
				
		output.close()
	#----------------------------------------------END OF CLASS----------------------------------------------------------#
		
		
#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Filereader:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Filereader

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   createTeiresiasIndexList(filename)
	   @param filename

	   readTeiresiasData(options)
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass
	#--------------------------------------------------------------------------------------------------------------------#
	def createTeiresiasIndexList(self,filename):
		indexList = {}
		input = open(filename,'r')

		count = 0
		for lines in input.readlines():
			if(lines[0] == ">"):
				
				acc = lines.split(" ")
				acc[0].replace("\r","")
				indexList[count] = acc[0][1:]
				count += 1

		input.close()
		return indexList
	#--------------------------------------------------------------------------------------------------------------------#
	def readTeiresiasData(self,offset,options):
		printLevel(1,options,"Loading TEIRESIAS file\r",)
		start = time.time()
		sys.stderr.write(time.strftime('%X') + ":\n" + "Loading TEIRESIAS file\n")
		
		teiresiasIndex = {}
			
		x = 1

		if options["long_format"] == "T":	
			teiresiasParser =ParseTeiresiasFull()
			[dictPatterns,dictOccurances] = teiresiasParser.parse(options["TEIRESIAS_output"],offset,options)
			for subSequence in offset:
				teiresiasIndex[subSequence[2]] = subSequence[0]
				
		else:
			teiresiasParser =ParseTeiresias()
			[dictPatterns,dictOccurances] = teiresiasParser.parse(options["TEIRESIAS_output"],options)
			teiresiasIndex = self.createTeiresiasIndexList(options["TEIRESIAS_input"])

		teiresiasData = {}

		printLevel(1,options,"Formatting Pattern Data")
		length = len(dictPatterns.keys())
		count = 0

		printLevel(1,options, "%-10s"%"Patterns" + "\t%-10s"%"Complete" + "\r",)

		for pattern in dictPatterns.keys():
			if count%1000 == 0: 
				if options['verbosity'] > 1:
						print "%-10s"%str(length) + "\t"  +"%-10s"%str(count) + "\r",
			
			count += 1
			
			proteinList = []
			offsetList = []


			for occurances in dictPatterns[pattern][0]:
				proteinList.append(teiresiasIndex[int(occurances)])
				offsetList.append(dictPatterns[pattern][0][occurances])

		
			if len(options["query_protein"]) > 1:
				if options["query_protein"] in proteinList:
					teiresiasData[pattern] = [proteinList,offsetList,dictPatterns[pattern][1]]
				else:
					pass

			else:
				teiresiasData[pattern] = [proteinList,offsetList,dictPatterns[pattern][1]]

			del dictPatterns[pattern]

		printLevel(1,options,"%-10s"%str(count) + "\t"  +"%-10s"%str(length))
		outString = "\n" 

		outString +=  "Patterns processed            \t:" + str(count) + "\n"
		outString +=  "Time			     \t:" + "%2.2f seconds"%float(time.time() - start) + "\n"
		
		printLevel(1,options,"\n\nFormatting Statistics" + outString  +"\n")
		sys.stderr.write("Formatting Statistics" + outString  + "\n")

		printLevel(1,options, "Finished loading TEIRESIAS file\n")
		sys.stderr.write(time.strftime('%X') + ":\n" + "Finished loading TEIRESIAS file\n")


		del teiresiasIndex
		del teiresiasParser
		del dictOccurances
		return teiresiasData
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class SuffixTree:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  SuffixTree

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   add_pattern(tree,pattern,proteinData,pos,count)
	   @param tree
	   @param pattern
	   @param proteinData
	   @param pos
	   @param count

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass
	#--------------------------------------------------------------------------------------------------------------------#
	def add_pattern(tree,pattern,proteinData,pos,count):
		if pattern[count] in tree[pos] and count != len(pattern) - 1:
			temptree = tree[pos]

		
			if pattern[count + 1] in tree[pos][pattern[count]]:
				pass
			else:
				tree[pos][pattern[count]][pattern[count + 1]] = {}

			tree[pos] = add_pattern(temptree,pattern,proteinData,pattern[count],count + 1)
		else:
			
			pass

		if count == len(pattern) - 1:
			if "index" in tree[pos][pattern[-1]]:
				for offsets in proteinData:	
					tree[pos][pattern[-1]]["index"][offsets]= proteinData[offsets]
			else:
				tree[pos][pattern[-1]]["index"] = proteinData
				

		return tree
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Score:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Score

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   collapseList(listTemp)
	   @param listTemp

	   calculateScore(pattern,aaOcc)
	   @param pattern
	   @param aaOcc

	   loadaminoAcidOccurances(aminoAcidOccuranceFile)
	   @param aminoAcidOccuranceFile

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		self.part1 = 0
		self.average = 0

		pass
	#--------------------------------------------------------------------------------------------------------------------#
	def collapseList(self,listTemp):
			stringTemp = ""
			for values in listTemp:
				stringTemp += values

			return stringTemp
	#--------------------------------------------------------------------------------------------------------------------#
	def calculateInformationContent(self,pattern,aaOcc):

		count = 0	
		p1 = re.compile('\[[^\]]*\]')
		p2 = re.compile('(?<=])[^\[]*')

		data = p1.findall(pattern)
		patternTemp = "]" + pattern
		informationContent = 0
		for values in data:
			#print values
			count += 1
			sum = 0.0
			denominator = 0
			for AA in values[1:-1]:
				denominator += aaOcc[AA]
			for AA in values[1:-1]:
				if sum == 0.0:	
					sum = -aaOcc[AA]/denominator*math.log((aaOcc[AA]/denominator),2)
				else:
					sum += -aaOcc[AA]/denominator*math.log((aaOcc[AA]/denominator),2)
				
			informationContent +=  (-sum - self.part1)

		count += pattern.count(".")
		data = p2.findall(patternTemp)
		
		stringTmp = self.collapseList(data)
		stringTmp = stringTmp.replace(".","")

		for AA in stringTmp:	
			try:
				informationContent +=  -aaOcc[AA]/aaOcc[AA]*math.log((aaOcc[AA]/aaOcc[AA]),2) - self.part1 
			except:
				print pattern

		return  informationContent - options["gap_weight"]*pattern.count(".")
	#--------------------------------------------------------------------------------------------------------------------#
	def loadaminoAcidOccurances(self,aminoAcidOccuranceFile):
		printLevel(1,options,"\nLoading Peptide Occurances\r",)
		sys.stderr.write("Load Peptide Occurances\n")
		aaOcc = open(aminoAcidOccuranceFile,"r")
		
		aminoAcidOccurance = {}

		for AA in aaOcc.readlines():
			AA = AA.strip()
			data = AA.split(" ")
			aminoAcidOccurance[data[0]] = float(data[1])

		printLevel(1,options,"Peptide Occurances Loaded\t")
		aaOcc.close()

		for aa in aminoAcidOccurance:
			if aminoAcidOccurance[aa] > 0:
				self.part1 +=  aminoAcidOccurance[aa]*math.log((aminoAcidOccurance[aa]),2)
			
		return aminoAcidOccurance
	#--------------------------------------------------------------------------------------------------------------------#
	def calculateScore(self,pattern,aaOcc):
		count = 0	
		p1 = re.compile('\[[^\]]*\]')
		p2 = re.compile('(?<=])[^\[]*')

		data = p1.findall(pattern)
		patternTemp = "]" + pattern
		informationContent = 0
		for values in data:
			count += 1
			sum = 0.0
			denominator = 0
			for AA in values[1:-1]:
				denominator += aaOcc[AA]
			for AA in values[1:-1]:
				if sum == 0.0:	
					sum = -aaOcc[AA]/denominator*math.log((aaOcc[AA]/denominator),2)
				else:
					sum += -aaOcc[AA]/denominator*math.log((aaOcc[AA]/denominator),2)
				
			informationContent +=  (-sum - self.part1)/count

		count += pattern.count(".")
		data = p2.findall(patternTemp)
		
		stringTmp = self.collapseList(data)
		stringTmp = stringTmp.replace(".","")

		for AA in stringTmp:	
			try:
				informationContent +=  -aaOcc[AA]/aaOcc[AA]*math.log(1 + aaOcc[AA] - 0.05,2) - self.part1 
			except:
				pass

		return  (informationContent - options["gap_weight"]*pattern.count("."))
	#--------------------------------------------------------------------------------------------------------------------#
	def loadaminoAcidOccurances(self,aminoAcidOccuranceFile):
		printLevel(1,options, "\nLoading Peptide Occurances\r",)
		sys.stderr.write("Load Peptide Occurances\n")
		aaOcc = open(aminoAcidOccuranceFile,"r")
		
		aminoAcidOccurance = {}

		for AA in aaOcc.readlines():
			AA = AA.strip()
			data = AA.split(" ")
			aminoAcidOccurance[data[0]] = float(data[1])

		printLevel(1,options,"Peptide Occurances Loaded\t")
		aaOcc.close()

		for aa in aminoAcidOccurance:
			if aminoAcidOccurance[aa] > 0:
				self.part1 +=  aminoAcidOccurance[aa]*math.log((aminoAcidOccurance[aa]),2)
		
		self.average = sum(aminoAcidOccurance.values())/20

		return aminoAcidOccurance
	#----------------------------------------------END OF CLASS----------------------------------------------------------#


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class SurfaceProbability:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  SurfaceProbability

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	
	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		self.surface = ""
		self.janin = {'Z':0.00,'B':0.00,'X':0.00,'A':0.49,'R':0.95,'N':0.81,'D':0.78,'C':0.26,'Q':0.84,'E':0.84,'G':0.48,'H':0.66,'I':0.34,'L':0.40,'K':0.97,'M':0.48,'F':0.42,'P':0.75,'S':0.65,'T':0.70,'W':0.51,'Y':0.76,'V':0.36}
		self.average = sum(self.janin.values())/len(self.janin)
		
	#--------------------------------------------------------------------------------------------------------------------#
	def eminiMethod(self,sequence,pattern,portionPos,portionEnd,options):
		self.surface = ""
		scores = []
		count = 0
		countGreater = 0
		average = 0.0
		position = 0

		for i in range(portionPos,portionEnd):
			if pattern[i - portionPos] == '.':
				pass
			else:
				count += 1
				product = 0.0
				divisor = 0
				for j in range(1,7):
					if i + 4 - j < len(sequence) and i + 4 - j > 0:
						divisor += 1
						if product == 0.0:
							product = self.janin[sequence[i + 4 - j]]
						else:
							try:
								product *= self.janin[sequence[i + 4 - j]]*1.61
							except:
								print sequence
				
				if divisor != 6:
					score = product*self.average*pow(1.61,6 - (divisor))
				else:
					score = product

				scores.append(score)

				if  score > options["surface_cutOff"]:
					self.surface += sequence[i]
					countGreater += 1
				else:
					self.surface += "X"

				average += score
			
			position += 1
		
		percentageSurface = float(countGreater)/count
		averageSurfaceProb = average/count

		surfaceString = self.surface

		return {'averageSurfaceProb':averageSurfaceProb,'percentageSurface':percentageSurface,'surfaceString':surfaceString,"scores":scores}
	#--------------------------------------------------------------------------------------------------------------------#
	def checkPattern(self,sequence,pattern,offset,options):
		printLevel(1,options,sequence)
		for offsets in offset:
			eminiResult = self.eminiMethod(sequence,pattern,int(offsets),int(offsets) + len(pattern),options)
			
		eminiResult['pattern'] = pattern
			
	
		return [eminiResult["scores"],eminiResult['percentageSurface']]
	#--------------------------------------------------------------------------------------------------------------------#
	def checkSequence(self,sequence,options,protein):
		self.surface = ""
		scores = []
		count = 0
		countGreater = 0
		average = 0.0
		position = 0

		for i in range(0,len(sequence)):
				count += 1
				product = 0.0
				divisor = 0
				for j in range(1,7):
					if i + 4 - j < len(sequence) and i + 4 - j > 0:
						divisor += 1
						try:
							if product == 0.0:
								product = self.janin[sequence[i + 4 - j]]
							else:
								
								product *= self.janin[sequence[i + 4 - j]]*1.61
						except:
							print sequence,'<br>----',sequence[i + 4 - j]
							sys.exit()
				
				if divisor != 6:
					score = product*self.average*pow(1.61,6 - (divisor - 1))
				else:
					score = product

				scores.append(score)

				if  score > options["surface_cutOff"]:
					self.surface += sequence[i]
					countGreater += 1
				else:
					self.surface += "X"

				average += product
		
		try:
			SurfaceAccessibilityPlotter().plotSurfaceAccessibility(protein,scores,options)
		except:
			#print 'Skipping surface accessability plot'
			pass
			
		return self.surface
	#--------------------------------------------------------------------------------------------------------------------#
	def checkPosition(self,sequence,position,options):
		self.surface = ""
		i = position - 1

		for j in range(1,7):
			if i + 4 - j < len(sequence) and i + 4 - j > 0:
				if j == 1:
					product = self.janin[sequence[i + 4 - j]]
				else:
					product *= self.janin[sequence[i + 4 - j]]
					
		if  product*17.6 > options["surface_cutOff"]:
			self.surface += sequence[i]
		else:
			self.surface += "X"

		printLevel(1,options,self.surface)
		printLevel(1,options,sum(self.janin.values()))
	#--------------------------------------------------------------------------------------------------------------------#
	def checkPatterns(self,sequences,pattern,proteins,options):
		
		pattern1 = re.compile('\[[A-Z.]*\]|[A-Z.]')
		pattern =  pattern1.findall(pattern)

		results = {}
		for protein in proteins.keys():
				
			results[protein] = self.checkPattern(sequences[protein],pattern,proteins[protein],options)	
					
		length = len(pattern) - pattern.count(".")

		#######################
		#TODO ambiguous
		#######################

		accessibility = {}
		for i in range(0,length):
			if pattern[i] == ".":
				x =1
			else:
				accessibility[i] = []
				for protein in proteins.keys():
						if results[protein][0][i] > options["surface_cutOff"]:
							accessibility[i].append(1)
						else: 
							accessibility[i].append(0)


		accessibilityPercentage = []
		sumAverage = 0
		for vals in accessibility.keys():
			average = float(sum(accessibility[vals]))/len(accessibility[vals])
			accessibilityPercentage.append(average%1)
			sumAverage += average
		
		percentage_residues_accesible = float(sumAverage)/len(accessibility)

		correlation = float(accessibilityPercentage.count(0))/len(accessibility)
		return [percentage_residues_accesible,sum(accessibility[vals])]#correlation

	#----------------------------------------------END OF CLASS----------------------------------------------------------#


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Complexity:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Complexity

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   countChar(inp)
	   @param inp

	   collapseList(listTemp)
	   @param listTemp

	   factorial(inp)
	   @param inp

	   complexity(input)
	   @param input

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		results = {}
	#--------------------------------------------------------------------------------------------------------------------#
	def countChar(self,inp):
		result = {'A': 0,'R': 0,'N': 0,'D': 0,'C': 0,'Q': 0,'E': 0,'G': 0,'H': 0,'I': 0,'L': 0,'K': 0,'M': 0,'F': 0,'P': 0,'S': 0,'T': 0,'W': 0,'Y': 0,'V': 0, 'B': 0,'Z': 0,'length' : 0}

		for i in range(0,len(inp)):
			try:
				if inp[i] != "." or inp[i] != "[" or inp[i] != "]":
					result[inp[i]] += 1
					result["length"] += 1
			except:
				x = 0
	
		temp = len(inp)
	
		for i in result.keys():
			if i  == "length":
				x = 0
			else:
				result[i] = int((100*(float(result[i])/result["length"])))
		return result
	#--------------------------------------------------------------------------------------------------------------------#
	def collapseList(self,listTemp):
		stringTemp = ""
		for values in listTemp:
			stringTemp += values

		return stringTemp
	#--------------------------------------------------------------------------------------------------------------------#
	def factorial(self,inp):
		out = 1

		for i in range(1,inp + 1):
			out = i*out

		return out
	#--------------------------------------------------------------------------------------------------------------------#
	def complexity(self,input):
		try:
			p = re.compile('(?<=])[^\[]*')
			data = p.findall("]" + input)
			input = self.collapseList(data)

			acc = 1
			counter = []
			result = self.countChar(input)

			for i in result.keys():
				if i  == "length":
					x = 0
				else:
					acc = self.factorial(result[i]) * acc
					
			stringLength = 100

			x = self.factorial(stringLength)
			return 1/float(stringLength)*((math.log(float(x)/acc)/math.log(22)))
		except:
			return 0
	#----------------------------------------------END OF CLASS----------------------------------------------------------#


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Prim:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Prim

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   findEdges(proteinList,protein)
	   @param proteinList
	   @param protein

	   sortMinQueue(min_queue,highScoreList)
	   @param min_queue
	   @param highScoreList

	   prim(positions,blastData,proteinList)	   
	   @param positions
	   @param blastData
	   @param proteinList

	   scorePrim(blastData,proteinList)
	   @param blastData
	   @param proteinList

	#################################################################################################
	"""
	#################################################################################################
	
	#--------------------------------------------------------------------------------------------------------------------#
	def findEdges(self,proteinList, protein1):
		edgeList = []
		for protein2 in proteinList:
			if protein2 != protein1:
				edgeList.insert(0,protein2)
		return edgeList
	#--------------------------------------------------------------------------------------------------------------------#
	def sortMinQueue(self,min_queue,highScoreList):
		queueLength = len(min_queue)
		for i in range(0,queueLength):
			for j in range(0,queueLength):
				if highScoreList[min_queue[i]] < highScoreList[min_queue[j]]:
					tempProtein = min_queue[i]
					min_queue[i] = min_queue[j]
					min_queue[j] = tempProtein
		return min_queue
	#--------------------------------------------------------------------------------------------------------------------#
	def prim(self,positions,blastData,proteinList):
		INFINITY = pow(2,16)
		
		minimumSpanningTree =["empty"]*len(positions)

		highScoreList = {}
		for values in proteinList:
			highScoreList[values] = INFINITY

		min_queue=[]
		for protein in proteinList:
			min_queue.append(protein)

		highScoreList[proteinList[0]] = 0
		self.sortMinQueue(min_queue,highScoreList)    
		
		while len(min_queue) > 0:
			protein1 = min_queue[0]
			min_queue.remove(min_queue[0])
		
			edges = self.findEdges(positions, protein1)
			for protein2 in edges:
				try:
					
					dissimilarity =  math.pow(1 - max(blastData[protein1][protein2],blastData[protein2][protein1]),float(options['normalisation_value']))
				except:
					dissimilarity = 1

				if protein2 in min_queue and dissimilarity < highScoreList[protein2]:
					minimumSpanningTree[positions[protein2]] = protein1
					highScoreList[protein2] = dissimilarity
					min_queue = self.sortMinQueue(min_queue,highScoreList)


		return [minimumSpanningTree,highScoreList]
	#--------------------------------------------------------------------------------------------------------------------#
	def scorePrimBlast(self,blastData,proteinList):
		positions = {}
		count = 0
		for proteins in proteinList:
			positions[proteins] = count
			count += 1
			
		[minimumSpanningTree,key]  = self.prim(positions, blastData, proteinList)
		return 1 + sum(key.values())
	#--------------------------------------------------------------------------------------------------------------------#
	
	def scorePrimBinary(self,binaryDataIn,proteinList):
		positions = {}
		count = 0

		binaryData = copy.deepcopy(binaryDataIn)
		
		for key1 in binaryDataIn:
			for key2 in binaryDataIn:
				if key1 == key2:
					binaryData[key1][key2] = 1.0
				if binaryData[key1][key2] == 0:
					del binaryData[key1][key2]
				if binaryDataIn[key1][key2] == 1 or binaryDataIn[key2][key1] == 1:
					binaryData[key1][key2] = 1
					binaryData[key2][key1] = 1
					
		for proteins in proteinList:
			positions[proteins] = count
			count += 1
			
		[minimumSpanningTree,key]  = self.prim(positions, binaryData, proteinList)

		return 1 + sum(key.values())
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#

  
#----------------------------------------------------------------------------------------------------------------------------#
class rje_Score:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  rje_Score

	  Author:    
	  Rich Edwards

	  Created:  

	  Description:  
	  --
	  
	  Requirements:

	  Fixes:

	  Functions:   

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass

	def expect(self,pattern,aafreq,aanum,seqnum):    	
		### Returns the expected number of occurrences for that pattern
		'''
		Returns the expected number of occurrences for given pattern. Xs and .s both count as wildcards.
		>> aafreq:dictionary of AA frequencies {aa:freq}
		>> aanum:int = sum total of positions in dataset
		>> seqnum:int = number of different sequence fragments searched
		<< expdict:dictionary of {mismatch:expectation}
		'''
		### Setup ###
		terminal_constraint = False
		expvar = string.replace(pattern,')',') ')   # String to analyses
		expvar = string.replace(expvar,'[A-Z]','X')
		expvar = string.replace(expvar,'.','X')
		expvar = string.replace(expvar,']','] ')
		if expvar[0] == '^' or expvar[-1] == '$':
			terminal_constraint = True
		patlen = 0          # Length of pattern for calculating possible number of positions
		aafreq['X'] = 1.0   # Wildcards do not affect expectation (prob=1)
		prob_per_site = 1.0 # Probability of the pattern occurring at any given site.
		
		### Calculate prob_per_site ###    
		while expvar:       # Still some pattern to look at
		## Deal with spacers inserted for ease of pattern matching ##
			if expvar[0] in [' ','^','$']:
				expvar = expvar[1:]
				continue
		## Wildcard ##
			if expvar[:1] == 'X':   
				expvar = expvar[1:]
				patlen += 1
				continue
		## Update probability per site ##
			if expvar[0] == '[' and expvar.find(']') > 0:   # Choices
				csum = 0.0      # Summed frequency over choices
				for c in expvar[1:expvar.find(']')]:    # Region between []
					csum += aafreq[c]   # Prob = sum of all choices
				if csum < 1.0:
					prob_per_site *= csum
				expvar = expvar[expvar.find(']')+1:]
				patlen += 1
			elif expvar[0] == '(' and expvar.find(')') > 0: # Complex choice!
		    		csum = 0.0      # Sum prob of whole choice
		    		cvar = string.split(expvar[1:expvar.find(')')],'|')     # List of different options
		    		for cv in cvar:
					cvexp = 1.0 # Probability of just one portion of choice
					while rje.matchExp('(\[(\S+)\])',cv):
			   		 	msum = 0.0  # sum for choice within portion!
			   			cvm = rje.matchExp('(\[(\S+)\])',cv)
			   			cv = string.replace(cv,cvm[0],'',maxsplit=1)
			    			for m in cvm[1]:
							msum += aafreq[m]   # Prob = sum of all choices
			    			cvexp *= msum
					cv = string.replace(cv,' ','')
					for c in cv:
			    			cvexp *= aafreq[c]
					csum += cvexp
		   		if csum < 1.0:
					prob_per_site *= csum
		    		expvar = expvar[expvar.find(')')+1:]
		   		patlen += float(len(string.join(cvar,'')))/len(cvar)    # Add mean length of options
			elif expvar[0] not in aafreq.keys():    # Problem!
		    		print '! aafreq missing <%s> for pattern %s!' % (expvar[0],pattern)
		    		expvar = expvar[1:]
			else:   # Simple
		    		prob_per_site *= aafreq[expvar[0]]
		    		expvar = expvar[1:]
	
	   	 ### Convert to Expectation ###
		if terminal_constraint:
			num_sites = seqnum
		else:
			num_sites = aanum - (seqnum * (patlen - 1))
			
		return prob_per_site * num_sites
	#----------------------------------------------------------------------------------------------------------------------------#
	def occProb(self,observed,expected):     ### Returns the poisson probability of observed+ occurrences, given expected
		'''Returns the poisson probability of observed+ occurrences, given expected.'''
		prob = 0
		for x in range(0,observed):
			try:        #!# Fudge for OverflowError: long int too large to convert to float
				prob += (math.exp(-expected) * pow(expected,x) / self.factorial(x))
			except:
				break
		return 1 - prob
	    
	#----------------------------------------------------------------------------------------------------------------------------#
	def rje_Probability(self,observed,pattern,aminoAcidOcc,proteinData):
		expected = 0
		for protein in proteinData:
			aanum = len(proteinData[protein])
			seqnum = 1
			expectedProtein = self.expect(pattern,aminoAcidOcc,aanum,seqnum)
			expected += expectedProtein
	
		probScore =  self.occProb(observed,expected)
		probabilityScores = [expected,probScore]
		return probabilityScores
	#----------------------------------------------------------------------------------------------------------------------------#
	def factorial(self,m): ### Returns the factorial of the number m
		'''Returns the factorial of the number m.'''
		value = 1
		if m != 0:
			while m != 1:
				value = value*m
				m = m - 1
		return value
	#----------------------------------------------END OF CLASS----------------------------------------------------------#
	


#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Scorer:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Scorer

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   checkScore(scoreDict,score,data,options)
	   @param scoreDict
	   @param score
	   @param data
	   @param options

	   printGlobalMatrix(proteinData,blastHits,blastGlobalHits)
	   @param proteinData
	   @param blastHits
	   @param blastGlobalHits
	   printMatrix(proteinData,blastHits)
	   @param proteinData
	   @param blastHits

	   normalise_Occurances(pattern,teiresiasData,blastGlobalHits)
	   @param pattern
	   @param teiresiasData
	   @param blastGlobalHits

	   scorePatterns(teiresiasOutputData,topRanking,proteinData,options,blastData)
	   @param teiresiasOutputData
	   @param topRanking
	   @param proteinData
	   @param options
	   @param blastData

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		sys.stderr.write("\n----------------------------------------------------------------\n")
		sys.stderr.write("########################RANKING PATTERNS########################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		self.setList = {}
		self.distribution = {}

	#--------------------------------------------------------------------------------------------------------------------#
	def checkScore(self,scoreDict,score,data,fileHandle,options):
		if (int(score)/5)*5 in self.distribution:
			self.distribution[(int(score)/5)*5] += 1
		else:
			self.distribution[(int(score)/5)*5] = 1

		try:
			fileHandle.write(str(data[2]) + "\t" + str(data[3]) + "\n")
			
		except:
			pass

		
		if score > min(scoreDict.keys()):
			if len(scoreDict) > options["no_of_outputs"] - 1:
				del scoreDict[min(scoreDict.keys())]
			if score in scoreDict:
				scoreDict[score + float(random.randint(1,1000))/100000] = data
			else:
				scoreDict[score] = data

		return scoreDict
	#--------------------------------------------------------------------------------------------------------------------#
	def printGlobalMatrix(self,proteinData,blastHits,blastGlobalHits):
		outString = "\n"

		for protein1 in proteinData:
			outString += "%-10s"%protein1[0:10] + "\t"
			for protein2 in proteinData:
				if protein2 in blastHits[protein1].keys():
					outString += "  %2.2f"%float(blastGlobalHits[protein1][protein2]) + "\t"
				else:
					try:
						outString += " *%2.2f"%float(blastGlobalHits[protein1][protein2]) + "\t"
					except:
						outString += "  0.0\t" 
			outString += "\t\n"


		sys.stderr.write("\n" + outString)

		sys.stderr.write("\n")
	
	#--------------------------------------------------------------------------------------------------------------------#
	def printMatrix(self,proteinData,blastHits):

		outString = "\n"

		for protein1 in proteinData:
			outString += "%-10s"%protein1[0:10] + "\t"
			for protein2 in proteinData:
				if protein2 in blastHits[protein1].keys():
					outString += "   X\t"
				else:
					outString += "   -\t"
			outString += "\n"

		sys.stderr.write("\n" + outString)

	#--------------------------------------------------------------------------------------------------------------------#
	def normalise_Occurances(self,pattern,teiresiasData,blastGlobalHits,options):
		normalised_occurrance = 0

		primAlgorithm = Prim()
		#print blastGlobalHits
		#print teiresiasData
		normalised_occurrance = primAlgorithm.scorePrimBlast(blastGlobalHits,teiresiasData)
		del primAlgorithm	
		
		return normalised_occurrance
		
	#--------------------------------------------------------------------------------------------------------------------#
	def clusterBinary(self,clusterDict,tempDictIn):	
		tempDict = copy.deepcopy(tempDictIn)
		
		cluster = []
		count = 0
		
		for p1 in clusterDict.values():
			cluster.append(sets.Set(p1))
			clusterDict[p1[0]] = count
			count += 1
			
		for p1 in tempDict.keys():
			for p2 in tempDict.keys():
				if p1 != p2:
					if tempDict[p1][p2] == 1:
						temp = []
						temp = cluster[clusterDict[p1]].union(cluster[clusterDict[p2]])
						
						if temp not in cluster:
							cluster.append(temp)
							for protein in temp:
								clusterDict[protein] = len(cluster) - 1
 				else:
 					pass
 				
		countList = []

		for i in clusterDict.values():
			if i not in countList:
				countList.append(i)
		
		return len(countList)
	#--------------------------------------------------------------------------------------------------------------------#
		
	def normalise_Occurances_Nondistance(self,pattern,teiresiasData,blastGlobalAlignments,options):
		pattern1 = re.compile('\[[^\]]*\]|.')

		patternParts = pattern1.findall(pattern)
		
		tempDomainDict = {}
		clusterDomainDict = {}
		tempHomologyDict = {}
		clusterHomologyDict = {}
		offsetDict = {}

		querySet = sets.Set(teiresiasData[pattern][0])

		for i in range(len(teiresiasData[pattern][0])):
			offsetDict[teiresiasData[pattern][0][i]] = teiresiasData[pattern][1][i]

		count = 0
			
		if options['self_hit'] == "T":
			for proteins1 in offsetDict.keys():
				for x in range(len(offsetDict[proteins1])):
					tempDomainDict[proteins1  + str(x)] = {}
					tempHomologyDict[proteins1  + str(x)] = {}

					clusterDomainDict[proteins1 + str(x)] = [proteins1 + str(x)] 
					clusterHomologyDict[proteins1 + str(x)] = [proteins1 + str(x)] 
					

					for proteins2 in offsetDict.keys():
						for y in range(len(offsetDict[proteins2])):
							
							try:
								tempHomologyDict[proteins1 + str(x)][proteins2 + str(y)] = 1
								count = 0
								match = 1

								matched = 0
								mismatched = 0

								for values in patternParts:
									if len(values) == 1:
										if values == blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] or values == "."  or blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] == "+":
											matched += 1
											pass
										else:
											mismatched += 1	
							
									else:
										if blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] in values or blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] == "+":
											matched += 1
											pass
										else:
											mismatched += 1
										
									count += 1
									
								if float(mismatched)/matched > options["match/mismatch"]:	
									tempDomainDict[proteins1 + str(x)][proteins2 + str(y)] = 0
								else:
									tempDomainDict[proteins1 + str(x)][proteins2 + str(y)] = 1

								if proteins1 == proteins2:
									tempDomainDict[proteins1 + str(x)][proteins2 + str(y)] = 0
									tempHomologyDict[proteins1 + str(x)][proteins2 + str(y)] = 0
								
							except:
								tempDomainDict[proteins1 + str(x)][proteins2 + str(y)] = 0
								tempHomologyDict[proteins1 + str(x)][proteins2 + str(y)] = 0
				

		elif options['self_hit'] == "F":
			for proteins1 in offsetDict.keys():
				for x in range(len(offsetDict[proteins1])):
					tempDomainDict[proteins1 ] = {}
					tempHomologyDict[proteins1 ] = {}

					clusterDomainDict[proteins1] = [proteins1] 
					clusterHomologyDict[proteins1] = [proteins1] 
					
					for proteins2 in offsetDict.keys():
						for y in range(len(offsetDict[proteins2])):
							
							
							if proteins2 in blastGlobalAlignments[proteins1].keys(): 
								tempHomologyDict[proteins1][proteins2] = 1
								count = 0
								match = 1

								matched = 0
								mismatched = 0
								for values in patternParts:
									if  values != ".":
										if len(values) == 1:
											if  blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] != "+" or blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] != ".":
												if values == blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count]:
														matched += 1
														pass
												else:														
													mismatched += 1
											else:
												mismatched += 1	
								
										else:
											if blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] in values or blastGlobalAlignments[proteins1][proteins2][(int(offsetDict[proteins1][x])) + count] == "+":
												matched += 1
												pass
											else:
												mismatched += 1
											
									count += 1
								
								
								if matched > 0:
									if float(mismatched)/matched + 0.00000001 >  options["match/mismatch"]:	
										tempDomainDict[proteins1][proteins2] = 0
										break
									else:
										tempDomainDict[proteins1][proteins2] = 1
								else:
									tempDomainDict[proteins1][proteins2] = 0
									
								if proteins1 == proteins2:
									
									tempDomainDict[proteins1][proteins2] = 0
									tempHomologyDict[proteins1][proteins2] = 0
								
							else:
								tempDomainDict[proteins1][proteins2] = 0
								tempHomologyDict[proteins1][proteins2] = 0

			
		primAlgorithm = Prim()

		domainSupport = self.clusterBinary(clusterDomainDict,tempDomainDict)
		homologySupport = self.clusterBinary(clusterHomologyDict,tempHomologyDict)

		'''if 1:
			print '-h'*30
			for key1 in tempHomologyDict:
				print key1,'\t',
				for key2 in tempHomologyDict[key1]:
					print tempHomologyDict[key1][key2],
				print 
				
			print float(homologySupport)
			print '-'*30'''
		return [domainSupport,homologySupport]
	#--------------------------------------------------------------------------------------------------------------------#

	def normalise_Occurances_SlidingWindow_MST(self,pattern,teiresiasData,blastGlobalAlignments,options):
		pattern1 = re.compile('\[[^\]]*\]|.')
		
		patternParts = pattern1.findall(pattern)
		
		tempDict = {}
		offsetDict = {}
		clusterDict = {}
		
		querySet = sets.Set(teiresiasData[pattern][0])
		for i in range(len(teiresiasData[pattern][0])):
			offsetDict[teiresiasData[pattern][0][i]] = teiresiasData[pattern][1][i]

		print offsetDict
		windowSize = 30
		for proteins1 in offsetDict.keys():
			
			tempDict[proteins1] = {}
			clusterDict[proteins1] = [proteins1] 
			for proteins2 in offsetDict.keys():
				count = 0
				for offset in offsetDict[proteins1]:
					count += 1
					print proteins1 + '_' + str(count),'\t', proteins2,'\t', 
					try:
						if proteins2 in blastGlobalAlignments[proteins1].keys():
							start = offset - windowSize/2
							end = offset + len(pattern) + windowSize/2
							
							print blastGlobalAlignments[proteins1][proteins2][start:end],
							mismatch =  blastGlobalAlignments[proteins1][proteins2][start:start+windowSize/2].count('.') +  blastGlobalAlignments[proteins1][proteins2][end:end+windowSize/2].count('.')
							print start,end,
							print blastGlobalAlignments[proteins1][proteins2][start:start+windowSize/2],
							print blastGlobalAlignments[proteins1][proteins2][end:end+windowSize/2],
							print blastGlobalAlignments[proteins1][proteins2][start:start+windowSize/2].count('.'),
							print blastGlobalAlignments[proteins1][proteins2][end:end+windowSize/2].count('.'),
							#print len(blastGlobalAlignments[proteins1][proteins2][start:start+windowSize/2] + blastGlobalAlignments[proteins1][proteins2][end:end+windowSize/2])
							print "%-2.2f"%(float(mismatch)/len(blastGlobalAlignments[proteins1][proteins2][start:start+windowSize/2] + blastGlobalAlignments[proteins1][proteins2][end:end+windowSize/2]))
						else:
							print "No hit"
							
					except:
						print "failed"
					
	#--------------------------------------------------------------------------------------------------------------------#
	def scorePatterns(self,teiresiasOutputData,topRanking,proteinData,options,blastData):
		printLevel(1,options,"Ranking Patterns")
		
		[blastHits,blastGlobalHits,blastGlobalAlignments] = blastData

		tempString = ''
		for protein1 in blastGlobalAlignments:
			for protein2 in blastGlobalAlignments[protein1]:
				tempString += protein1 + "\t" + protein2 + "\t" + "%-3.2f"%(blastGlobalHits[protein1][protein2]*100) + "\t" + blastGlobalAlignments[protein1][protein2] + "\n"
		
		open(options["output"].replace(".rank", ".homology"),"w").write(tempString)
		
		[percentage_residues_accessible,count] = [0,0]
		
		surfaceProbabilityCheck = SurfaceProbability()
		scorer = Score()
		aminoAcidOcc = scorer.loadaminoAcidOccurances(options["aminoAcidOccuranceFile"])
		del scorer
	
		tempFile = open(options["output"].replace(".rank", ".surface.html"),"w")
		tempFile.write("<html>")
		tempFile.write("<h2>Surface Accessibility<br></h2><h3>Cut-off : \t" + str(options["surface_cutOff"]) + "</h3><br>") 
		
		for protein in proteinData.keys():
			tempFile.write("<h3>" + protein + "</h3>\n")
			tempFile.write("<p><img src='" +  os.path.abspath("./" + options["input_path"] + "/surface/"  + protein + ".gif'") + "></p><br>\n")
			stringTemp = surfaceProbabilityCheck.checkSequence(proteinData[protein],options,protein)

			tempFile.write("<FONT SIZE=1 color=#ff0000 face='courier'>")
			

			for i in range(0,(len(stringTemp)/100) + 1):
				if 60*(i+1) < len(stringTemp):
					stringOut = stringTemp[100*i:100*(i+1)].replace("X","<FONT color=#000000 >X</FONT>")
				
					tempFile.write(stringOut  + "<br>\n")
				else:				
					stringOut = stringTemp[100*i:100*(i+1)].replace("X","<FONT color=#000000 >X</FONT>")
				
					tempFile.write(stringOut  + "<br>\n")
					
			
			tempFile.write("</FONT>")
		
		tempFile.write("</html>")
		tempFile.close()

		fileHnd = open(options["output"].replace("rank","distribution"),"w")
		fileHnd.write("norm\tIC\n")

		sys.stderr.write(time.strftime('%X') + ":\n" + "Ranking Patterns\n\n")
		
		start =  time.time()
		counter = 0
		countOut = 0

		length = len(teiresiasOutputData.keys())

		if options["view_matrix"] == "T":
			self.printMatrix(proteinData,blastHits)
			self.printGlobalMatrix(proteinData,blastHits,blastGlobalHits)

		rje_Scorer = rje_Score()
		fileReader = fileManipulation()
		teiresiasInputProteins =  fileReader.readFile(options,options["TEIRESIAS_input"])
		del fileReader			
		
		if int(options["flavour"]) == 1 or int(options["flavour"]) == 2 or int(options["flavour"]) == 3:
	
			
			for pattern in teiresiasOutputData.keys():
				#self.normalise_Occurances_SlidingWindow_MST(pattern,teiresiasOutputData,blastGlobalAlignments,options)
				#sys.exit()
				"""pos1 = 0
				windowSize = 20
				
				for protein1 in teiresiasOutputData[pattern][0]:
					pos2 = 0
					for protein2 in teiresiasOutputData[pattern][0]:
						print protein1, '-', protein2,
						if protein2 in blastGlobalAlignments[protein1].keys():
							start1 = teiresiasOutputData[pattern][1][pos1][0] - windowSize/2
							start2 = teiresiasOutputData[pattern][1][pos2][0] - windowSize/2
							
							end1 = start1 + len(pattern) + windowSize/2
							end2 = start2 + len(pattern) + windowSize/2
							
							print '?'
							print blastGlobalAlignments[protein1][protein2]
							print blastGlobalAlignments[protein1][protein2][start1:end1],
							print blastGlobalAlignments[protein1][protein2][start2:end2]
						else:
							print 0
						pos2 += 1
					pos1 += 1
					
				sys.exit()"""
				
				if countOut%1000 == 0:
					if options['verbosity'] > 1:
						print "%-10s"%str(length) + "\t%-10s"%str(counter) + "\r",

				counter += 1
				similarity_Normalised_Occurances =  self.normalise_Occurances(pattern,teiresiasOutputData[pattern][0],blastGlobalHits,options)
				[domain_Normalised_Occurances,homology_Normalised_Occurances] = self.normalise_Occurances_Nondistance(pattern,teiresiasOutputData,blastGlobalAlignments,options)
				
				normalised_Supports = [similarity_Normalised_Occurances,domain_Normalised_Occurances,homology_Normalised_Occurances]
				
				if int(options["flavour"]) == 1:
					normalised_occurances = similarity_Normalised_Occurances 
				elif int(options["flavour"]) == 2:
					normalised_occurances = domain_Normalised_Occurances 
				elif int(options["flavour"]) == 3:
					normalised_occurances = homology_Normalised_Occurances
				else:
					print "Error occured with normalised_occurances"
					print int(options["flavour"])

					
				if homology_Normalised_Occurances > 1:
					countOut += 1
					observed = len(teiresiasOutputData[pattern][0])
					
					probabilityScores = rje_Scorer.rje_Probability(observed,pattern,aminoAcidOcc,teiresiasInputProteins)
					[expected,probScore] = probabilityScores
					
					information_content = teiresiasOutputData[pattern][2]
					
					if options["use_evolution"] == "T":	
						
						if int(options["ranking"]) == 1:
							score = information_content*(normalised_occurances)
						elif int(options["ranking"]) == 2:
							score = information_content*(normalised_occurances)*(observed/expected)
						elif int(options["ranking"]) == 10:
							score = information_content*observed
						else:
							errorfile_not_found = "\nERROR:\n" + "+"*70 + "\nOption ranking = " + str(options["ranking"]) + " does not exist.\nPlease chosse {1-2] try again.\n" + "+"*70
							print errorfile_not_found
							sys.exit()
					else:
						
						normalised_occurances = len(teiresiasOutputData[pattern][0])
						score = information_content*normalised_occurances

					offsetDict = {}
					
					for i in range(len(teiresiasOutputData[pattern][0])):
						offsetDict[teiresiasOutputData[pattern][0][i]] = teiresiasOutputData[pattern][1][i]
										
					patternInfo = [pattern,teiresiasOutputData[pattern],normalised_occurances,information_content,percentage_residues_accessible,normalised_Supports,probabilityScores]#,correlation]
					topRanking = self.checkScore(topRanking,score,patternInfo,fileHnd,options)

				del teiresiasOutputData[pattern]		
		else:
			errorfile_not_found = "\nERROR:\n" + "+"*70 + "\nOption flavour = " + str(options["flavour"]) + " does not exist.\nPlease chosse {1-3] try again.\n" + "+"*70
			print errorfile_not_found

			sys.exit()

		printLevel(1,options,"%-10s"%str(length) + "\t%-10s"%str(counter))
				
		printLevel(1,options,"Finished Scoring Clusters")

		outString = "\n" 
		outString +=  "Patterns                      \t:" + str(counter) + "\n"
		outString +=  "Patterns in unrelated proteins\t:" + str(countOut) + "\n"
		
		try:
			outString +=  "Percentage processed          \t:" "%2.2f"%(float(countOut)/counter) + "%\n"
		except:
			pass
			
		outString +=  "Time			     \t:" + "%2.2f seconds"%float(time.time() - start) + "\n"
		
		printLevel(1,options,outString)
		sys.stderr.write("Search Statistics" + outString  + "\n")
		sys.stderr.write(time.strftime('%X') + ":\n" + "Finished Ranking Patterns\n")
		
		try:
			sortList = self.distribution.keys()
			numPatterns = sum(self.distribution.values())
			#print numPatterns
			sortList.sort()

			printLevel(1,options, "------------------------------------------------------------------")
			printLevel(1,options, "Score Distribution")
			for i in range(min(sortList),max(sortList) + 5,5):
				try:
					printLevel(1,options, i ,"\t",self.distribution[i],"\t","%-2.5f"%(float(self.distribution[i])/numPatterns),(self.distribution[i]/(numPatterns/100)+1)*"*")
				except:
					printLevel(1,options, 0,"\t","0.00000")
					pass
		except:
			pass

	
		return topRanking
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class FileWriter:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  FileWriter
	  
	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   printOutput(topRanking,options)
	   @param topRanking
	   @param options

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		sys.stderr.write("\n----------------------------------------------------------------\n")
		sys.stderr.write("########################CREATING OUTPUT#########################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		printLevel(1,options,"-----------------------------" + time.strftime('%X') + "-----------------------------")
		pass

	#--------------------------------------------------------------------------------------------------------------------#
	def htmlOutput(self,topRanking,	teiresiasData,options):
		'''HTML Output method.'''
		printLevel(1,options,os.path.abspath(options["filename"]))
		printLevel(1,options, os.path.abspath(options["aminoAcidOccuranceFile"]))

		if options['memory_saver'] == 'F':
			header = "<h2 align='left'>Data</h2><table width='900' border='1' cellspacing='0' cellpadding='5'>" 
			header += "<tr><td align='left'><a href='" + os.path.abspath(options["filename"]) + "'>Input File</a></td>"
			header += "<td align='left'><a href='" + os.path.abspath(options["output"]) + "'>Tab Delimited Output</a></td></tr>"
			header += "<tr><td align='left'><a href='" + os.path.abspath(options["logfile"])+ "'>Logfile</a></td>"
			header += "<td align='left'><a href='" + os.path.abspath(options["aminoAcidOccuranceFile"]) + "'>Amino Acid Occurance Probabilities</a></td></tr>"
			header += "<tr><td align='left'><a href='" +	os.path.abspath(options["TEIRESIAS_output"]) + "'>TEIRESIAS output</a></td>"
			header += "<td align='left'><a href='" + os.path.abspath(options["output"].replace(".rank",".alignments.html"))+ "'>Global BLAST Generated Alignments</a></td></tr>"
			header += "<tr><td align='left'><a href='" + os.path.abspath(options["output"].replace(".rank",".surface.html")) + "'>Emini Generate Surface Maps</a></td>"
			if options["long_format"] == "T":
				header += "<td align='left'><a href='" + os.path.abspath(options["output"].replace(".rank",".domains.html")) + "'>Domain View</a></td></tr>"
			header += "</table><br>"
		else:
			header = ""
		
		tableHeader = """<h2 align='left'>Results</h2>
		<table width="900" border="1" cellspacing="0" cellpadding="5">
		<align="left">
		<tr>
		<td align="left">Rank</td>
		<td align="left">Pattern</td>
		<td align="left">Score</td>
		<td align="left">Occ</td>
		<td align="left">Proteins</td>
		<td align="left">IC</td>
		<td align="left">N Occ</td>		
		<td align="left">Sim</td>
		</tr>
		</align>"""


		tableFooter = """<br><b>IC = Information Content<br>
		Occ = Occurances<br>
		N Occ = Normalised Occurances<br>
		Sim = Similarity<br>
		</html>"""

		row = """<tr>
		<td align="center"><b>RANK</b></td>
		<td align="left">MOTIF</td>
		<td align="left">SCORE</td>
		<td align="left">OCCURANCES</td>
		<td align="left">PROTEINS</td>
		<td align="left">IC</td>
		<td align="left">NORMALISED</td>
		<td align="left">SIMILARITY</td>

		</tr>
		"""

		html = open(options["filename"]+ ".output.html","w")
		html.write("<html><h1 align='center'>" + options["filename"] + " Results</h1>")
		
		html.write(header)
		html.write(tableHeader)
		temp = ""

		sort = topRanking.keys()
		sort.sort()

		sort.reverse()
		counter = 1
		
#		0 pattern
#		1 proteins
#		2 normalised_occurances
#		3 information_content
#		4 minExpected
#		5 maxExpected
#		6 percentage_residues_accessible
#		7 correlation
		##############################print dat.rank####################################
		proteinDict = {}
		motifDict = {}
		motifList = []
		
		if 0 in sort:
			sort.remove(0)
			
		for scores in sort:
			motifList.append(topRanking[scores][0])
			motifDict[topRanking[scores][0]] = {}
			for protein in topRanking[scores][1][0]:
				if protein in proteinDict:
					pass
				else:
					proteinDict[protein] = len(proteinDict)
					
			for i in range(0,len(topRanking[scores][1][0])):
				motifDict[topRanking[scores][0]][topRanking[scores][1][0][i]] = topRanking[scores][1][1][i]
		
		compressedOutput = ''
		
		compressedOutput += '#Proteins\n'
		for i in range(0,len(proteinDict)):
			for protein in proteinDict:
				if i == proteinDict[protein]:
					compressedOutput += str(proteinDict[protein]) + '\t' + protein  +'\n'
		
		compressedOutput += '#Motif\n'
		count = 0
		for motif in motifList:
			count += 1
			compressedOutput += str(count) + '\t' + motif + '\t'
			tempDict = {}
			for protein in motifDict[motif]:
				tempDict[proteinDict[protein]] = []
				for offset in motifDict[motif][protein]:
					tempDict[proteinDict[protein]].append(str(offset))
					#compressedOutput += str(proteinDict[protein]) + ':' + str(offset)  + ' '
			
			sortTemp = tempDict.keys()
			sortTemp.sort()
			
			for key in sortTemp:
				for offset in tempDict[key]:
					compressedOutput += str(key) + ':' + str(int(offset)  + 1)  + ' '

			compressedOutput += '\n'
		

		open(options['output'].replace('rank','dat.rank'),'w').write(compressedOutput)
		

		for scores in sort:
			#print scores
			try:	
				#print topRanking[scores][1][0]
				proteinString = "<TABLE border='0' table width='100%' cellspacing='0' cellpadding='2'>"
				for i in range(0,len(topRanking[scores][1][0])):
					offsetsTemp = topRanking[scores][1][1][i]
					offsetsTemp.sort()
					offsetString = ""

					for offset in offsetsTemp :
						
						offsetString += str(int(str(offset).replace("'",""))) + "-" +  str(int(str(offset).replace("'","")) + len(topRanking[scores][0]))  + " , "
					
					offsetString = offsetString
					proteinString += "<tr><td width='50%'><a href='http://us.expasy.org/cgi-bin/niceprot.pl?" + str(topRanking[scores][1][0][i]) + "'>" + str(topRanking[scores][1][0][i]) +  "</a></td><td>" + offsetString[:-2] + "<br></td></tr>"
				proteinString += "</TABLE>"

				temp = row.replace("RANK",str(counter))
				temp = temp.replace("MOTIF",str(topRanking[scores][0]))
				temp = temp.replace("SCORE","%-2.3f"%(scores))
				temp = temp.replace("OCCURANCES",str(len(topRanking[scores][1][0])))
				temp = temp.replace("PROTEINS",proteinString)
				temp = temp.replace("IC","%-2.3f"%(topRanking[scores][3]))
				temp = temp.replace("NORMALISED",str(topRanking[scores][2]))
				temp = temp.replace("SIMILARITY",str(topRanking[scores][2]/len(topRanking[scores][1][0])))
				html.write(temp)
				counter += 1
			except Exception,e:
				print e
				pass
		html.write("</table><br>")
		html.write(tableFooter)
		html.close()

	#--------------------------------------------------------------------------------------------------------------------#
	def printOutput(self,topRanking,stats,options):
		printLevel(1,options,"Formatting output")
		
		 
		outHnd = open(options["output"],"w")
		sort = topRanking.keys()
		sort.sort()
		sort.reverse()
		
		counter = 0
		printLevel(1,options,'')
		inputStatsString =  "------------------------------------------------------------------\n"

		sys.stderr.write(time.strftime('%X') + ":\n" + "Scores\n")
		
		outstring = "Rank\tScore\t%-20s"%"Pattern" + "\tOcc\t"  + "IC\t"+ "Norm\t" + "Sim\t" +"\tMST\tUHS\tUP\t\tExpect\tProb\n"
		outHnd.write( '#' + (stats['ProteinData'] + '\n' +'No. of motifs: '  + str(stats['NumMotifs']) ).replace('\n','\n#\t') + '\n' +"#------------------------------------------------------------------\n" + outstring)
		
		
		sys.stderr.write(outstring)
		printLevel(1,options,outstring)
		
		for scores in sort:	
			try:
				counter += 1
				
				if scores > 1000:
					eTemp = ("%-1.2e"%float(scores)).split("+")
					outstring = "(" + str(counter) + ")\t" + eTemp[0] + "+" + str(int(eTemp[1]))
				else:
					outstring = "(" + str(counter) + ")\t%2.3f"%scores 

				if len(str(topRanking[scores][0])) > 35:
					outstring += "SEQUENCE " 
				else:
					outstring += "SEQUENCE "

				outstring += "%-2s\t"%str(len(topRanking[scores][1][0])) 
				outstring += "%-2.3f\t"%float(topRanking[scores][3])
				outstring += "%-2.3f\t"%float(topRanking[scores][2])
				outstring += "%-2.3f\t"%(float(topRanking[scores][2])/len(topRanking[scores][1][0]))
				outstring += "\t"
				outstring += "%-2.3f\t"%float(topRanking[scores][-2][0])
				outstring += "%-2.3f\t"%float(topRanking[scores][-2][1])
				outstring += "%-2.3f\t"%float(topRanking[scores][-2][2])
				outstring += "\t"		
				outstring += "%-2.3f\t"%float(topRanking[scores][-1][0])
				outstring += "%-2.2g\t"%float(topRanking[scores][-1][1])
				
				outstring +="\n"
		
					
				outHnd.write(outstring.replace("SEQUENCE","\t%-20s "%str(topRanking[scores][0])) )
				sys.stderr.write(outstring.replace("SEQUENCE","\t%-20s "%str(topRanking[scores][0])) )
				
				if len(str(topRanking[scores][0])) > 20:
					printLevel(1,options, outstring.replace("SEQUENCE","\t%-20s"%(str(topRanking[scores][0])[0:17])  + "...  ")[:-1])
				else:
					printLevel(1,options, outstring.replace("SEQUENCE","\t%-20s"%str(topRanking[scores][0]))[:-1])
			
			except Exception,e:
				#print e
				#printLevel(1,options, scores)
				pass
				
		outHnd.close()
	#----------------------------------------------END OF CLASS----------------------------------------------------------#



#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class CommandLine:	
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  CommandLine

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   
	   isInt(i) 
	   @param i

	   isFloat(i) 
	   @param i

	   printOptions()

	   usage()

	   parseCommandLine()

	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		
		self.optionsVariables = {
				"filename":"-i",
				"output":"-o",
				"logfile":"-l",
				"batch":"-b",
				"run_TEIRESIAS":"-T",
				"run_formatDB":"-D",
				"run_BLAST":"-B",
				"BLAST_results":"-r",
				"self_hit":"-H",
				"aminoAcidOccuranceFile" : "-a",
				"no_of_outputs": "-n",
				"information_cutOff":"-y",
				"complexity_cutOff":"-z",
				"surface_cutOff":"-s",
				"ranking":'-K',
				"filetype":'-t',
				"view_matrix": "-v",
				"use_evolution":"-E",
				"use_filtering":"-A",	
				"long_format":"-L",
				"query_protein":"-q",
				"gap_weight":"J",
				"eVal" : "-e",
				"flavour":"-f",
				"match/mismatch":"-p",
				'overwrite':'-O',
				"TEIRESIAS_output":"",
				"TEIRESIAS_input":"-I",
				"TEIRESIAS_patternLength":"-P",
				"TEIRESIAS_fixedPositions": "-F",
				"TEIRESIAS_supportString":"-S",
				"TEIRESIAS_equiv":"-R",
				"TEIRESIAS_walltime":"-W",
				'normalisation_value':"-C",
				'TEIRESIAS_local(IBM_path_length_bug_fix)':"-G",
				"Mask_path":"-m",
				"BLAST_path":"",
				"TEIRESIAS_path":"",
				"input_path":"",
				'memory_saver':'-X',
				'ini_file':'-d',
				'verbosity':'-Q',
				'strict_inclusive_masking':'-M'
				}

		self.options = {
				"filename":"",
				"output":"",
				"logfile":"",
				"batch":"F",
				"run_TEIRESIAS":"F",
				"run_formatDB":"T",
				"run_BLAST":"T",
				"self_hit":"F",
				"BLAST_results":"./results/",
				"strict_inclusive_masking":"F",
				"aminoAcidOccuranceFile" : "PeptideOccurances.txt",
				"no_of_outputs": 100,
				"information_cutOff":8,
				"complexity_cutOff":-1,
				"surface_cutOff":0.0,
				"view_matrix": "F",
				"use_evolution":"T",
				"use_filtering":"T",
				"long_format":"F",
				"query_protein":"",
				"gap_weight":0.5,
				"eVal" : 0.001,
				"flavour":"1",
				"filetype":'',
				"ranking":'1',
				'overwrite':'T',
				'TEIRESIAS_local(IBM_path_length_bug_fix)':'F',
				"match/mismatch":0.2,
				"TEIRESIAS_output":"",
				"TEIRESIAS_input":"",
				"TEIRESIAS_patternLength":8,
				"TEIRESIAS_fixedPositions": 2,
				"TEIRESIAS_supportString":3,
				"TEIRESIAS_equiv":"F",
				"TEIRESIAS_walltime":0,
				"BLAST_path":"",
				'verbosity':'0',
				"Mask_path":"mask.dat",
				"TEIRESIAS_path":"",
				'normalisation_value':1,
				'memory_saver':'F',
				'ini_file':'slimdisc.ini',
				"input_path":""
				}
	#--------------------------------------------------------------------------------------------------------------------#
	def isInt(self,i): 
		try: 
			int(i) 
			return 1 
		except: 
			return 
	#--------------------------------------------------------------------------------------------------------------------#
	def isFloat(self,i): 
		try:
			float(i) 
			return 1 
		except: 
			return 
	#--------------------------------------------------------------------------------------------------------------------#
	def printOptions(self):
		outString = ""
		printLevel(1,self.options, "-----------------------------" + time.strftime('%X') + "-----------------------------")
		sys.stderr.write("----------------------------------------------------------------\n")
		sys.stderr.write("########################OPTIONS#################################\n")
		sys.stderr.write("----------------------------------------------------------------\n")
		
		sort = self.options.keys()
		sort.sort()
		for option in sort:
			outString += "%-30s"%option + "\t" + str(self.options[option]) + "\n"

		sys.stderr.write(outString)
		printLevel(1,self.options,outString)
	#--------------------------------------------------------------------------------------------------------------------#
	def usage(self):
		printLevel(1,self.options,"%-30s"%str("Options")  + "\t" + "%-10s"%"Prefix" + "\t" + "%-20s"%str("Defaults"))
		variable = "-a"
		printLevel(1,self.options, "------------------------------------------------------------------------------------------")
		sort = self.options.keys()
		sort.sort()
		for option in sort:
			printLevel(1,self.options,"%-30s"%option  + "\t" + "%-10s"%self.optionsVariables[option]  + "\t" + "%-20s"%str(self.options[option]))
		
		printLevel(1,self.options, "------------------------------------------------------------------------------------------")

 	#--------------------------------------------------------------------------------------------------------------------#
	def loadINI(self,dir):
		try:
			#print os.path.join(dir,self.options['ini_file'])
			
			iniFile = open(os.path.join(dir,self.options['ini_file']),"r").read()
			data = re.sub('\n+','\n',iniFile).split("\n")
			
			for value in data:
				if len(value) > 2:
					iniValues = value.split("=")

					if self.isInt(self.options[iniValues[0].strip()]):
						if iniValues[1].find(".") == -1:
							self.options[iniValues[0].strip()] = int(iniValues[1].strip())
						else:
							self.options[iniValues[0].strip()] = float(iniValues[1].strip())
					else:
						self.options[iniValues[0].strip()] = str(iniValues[1].strip())	
			
		except:
			if os.path.exists(self.options['ini_file']):
				error_iniFileError = '#'*40 + "\nINI file " + self.options['ini_file'] + " corrupt using default settings\n" + '#'*40 
			else:
				error_iniFileError = '#'*40 + "\nINI file " + self.options['ini_file'] + " missing using default setting\nCheck path\n" + '#'*40 
			print error_iniFileError
			time.sleep(2)

 	#--------------------------------------------------------------------------------------------------------------------#
	def parseCommandLine(self):
		if len(sys.argv) == 1:
			print '-'*23 + '\nPlease enter arguements\n' + '-'*23
			self.usage()
			sys.exit(2)
		else:
			if os.path.isfile('slimdisc.ini'):
				self.loadINI('')
			elif os.path.isfile(os.path.join(os.path.dirname(sys.argv[0]),'slimdisc.ini')): 
				self.loadINI(os.path.dirname(sys.argv[0]))

			try:
				sort = self.optionsVariables.values()
				sort.sort()
				opts, args = getopt.getopt(sys.argv[1:],"A:B:C:D:E:F:G:H:I:J:K:L:O:M:N:P:R:S:T:X:Q:W:a:b:c:d:e:f:i:l:m:n:o:p:q:r:s:t:v:w:x:y:z:h", self.options.keys())

			except getopt.GetoptError,e:
				print e
				sys.exit(2)
			
			output = None
		
			for o, a in opts:
				for option in self.optionsVariables.keys():
					if o ==self.optionsVariables[option]:
						if self.isInt(self.options[option]):
							if a.find(".") == -1:
								self.options[option] = int(a)
							else:
								self.options[option] = float(a)
						else:
							self.options[option] = a	
				
				if o in ("-h", "--help"):
					self.usage()
					
					print __doc__
					sys.exit(2)

			if self.options['ini_file'] != 'slimdisc.ini':
				self.loadINI('')
			
			self.options["calling_folder"] = os.path.dirname(sys.argv[0])
		
			return self.options
	#----------------------------------------------END OF CLASS----------------------------------------------------------#
		
		
#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class Expectation:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  Expectation

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to run Teiresias Algorithm

	  Requirements:

	  Fixes:

	  Functions:   


	#################################################################################################
	"""
	#################################################################################################

	def __init__(self,blastData,proteinData,aaOcc):
		self.totalLength = 0
		self.proteinData = proteinData
		self.aaOcc = aaOcc

		for proteins in proteinData.keys():	
			self.totalLength += len(proteinData[proteins])

	#--------------------------------------------------------------------------------------------------------------------#
	def collapseList(self,listTemp):
		stringTemp = ""
		for values in listTemp:
			stringTemp += values

		return stringTemp
	#--------------------------------------------------------------------------------------------------------------------#
	def calculateExpectationRange(self,pattern):
		maxProb = (((self.totalLength/len(self.proteinData.keys()) - (self.patternLength(pattern) + 1)))*len(self.proteinData.keys()))*(self.calculateScore(pattern))
		minProb = maxProb * (len(self.proteinData.keys())/self.normalised_occurrance)
		return [maxProb,minProb]
	#--------------------------------------------------------------------------------------------------------------------#
	def calculateScore(self,pattern):
		aaOcc = {"A":0.066,"C":0.017,"B":0.000,"E":0.062,"D":0.056,"G":0.089,"F":0.030,"I":0.049,"H":0.020,"K":0.049,"M":0.018,"L":0.107,"N":0.039,"Q":0.049,"P":0.056,"S":0.062,"R":0.063,"T":0.060,"W":0.013,"V":0.058,"Y":0.037,"X":0.000,"Z":0.000}
		
		count = 0	
		p1 = re.compile('\[[^\]]*\]')
		p2 = re.compile('(?<=])[^\[]*')

		data = p1.findall(pattern)
		pattern = "]" + pattern
		probability = 1.0
		for values in data:
			count += 1
			sum = 0.0
			for AA in values[1:-1]:
				if sum == 0.0:	
					sum = aaOcc[AA]
				else:
					sum += aaOcc[AA]
			
			probability *= sum

		count += pattern.count(".")
		data = p2.findall(pattern)
		
		stringTmp = self.collapseList(data)
		stringTmp = stringTmp.replace(".","")
		
		for AA in stringTmp:
			count += 1
			xx = -aaOcc[AA]*math.log(aaOcc[AA])
			probability *= aaOcc[AA]
		
		return probability
	#--------------------------------------------------------------------------------------------------------------------#
	def patternLength(self,pattern):
		p = re.compile('(?<=])[^\[]*')
		
		data = p.findall("]" + pattern)
		length = pattern.count("[") + len(self.collapseList(data))
		return length
	#----------------------------------------------END OF CLASS----------------------------------------------------------#
	
#------------------------------------------------------CLASS-----------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
class FileChecker:
	#################################################################################################
	"""
	#################################################################################################

	  Class:
	  FileChecker

	  Author:    
	  Norman Davey 

	  Created:       
	  08/03/2005

	  Description:  
	  Class used to check if all files are present

	  Requirements:

	  Fixes:

	  Functions:   


	#################################################################################################
	"""
	#################################################################################################

	def __init__(self):
		pass

	#--------------------------------------------------------------------------------------------------------------------#
	def checkFiles(self,options):
		printLevel(1,options, "-----------------------------" + time.strftime('%X') + "-----------------------------")
		printLevel(1,options, "Checking presence of Files")
	
		if os.path.exists(options["filename"]):
			printLevel(1,options, "%-49s"%options["filename"] + "\t" + "present")
		else:
			print "#"*50 +'\nInput file path does not exist\n' + options["filename"] + '\n' + "#"*50
			sys.exit()

		if os.path.exists(os.path.join(options["BLAST_path"],'blastall.exe')) or os.path.exists(os.path.join(options["BLAST_path"],'blastall')):
			printLevel(1,options,"%-49s"%"blastall" + "\t" + "present")
		else:
			print "#"*20 +"\nERROR: blastall not present at '" + options["BLAST_path"] + "' .Please check path\n"+ "#"*97  +"\n" 
			sys.exit()
		
		if os.path.exists(os.path.join(options["TEIRESIAS_path"],"teiresias_char.exe")) or os.path.exists(os.path.join(options["TEIRESIAS_path"],"teiresias_char")):
			printLevel(1,options, "%-49s"%"TEIRESIAS" + "\t" + "present")
		else:
			print "#"*97 +"\nERROR: TEIRESIAS not present at " + options["TEIRESIAS_path"]  +". Please check path\n"+ "#"*97  +"\n" 
			sys.exit()
			
	#----------------------------------------------END OF CLASS------------------


#------------------------------------------------------MAIN------------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
def printLevel(level,options,string):
	if int(options['verbosity']) < int(level):
		pass
	else:
		print string

def runMain(options):

	topRanking = {0:[]}	
	offsets = {}
	stats = {}
	
	x = 1

	if options["long_format"] == "T":
		fileObject = fileManipulationFull()
		offsets,proteinData = fileObject.readFile(options)
		if len(proteinData.keys()) < 3:
			print "Dataset contains only " + str(len(proteinData.keys())) + " proteins. Too small for analysis"
			sys.exit()
			
		aaOcc = aaOccurance()
		aaOcc.createAAOccuranceFilefromProteinList(options,proteinData)

	else:
		fileObject = fileManipulation()
		proteinData = fileObject.readFile(options,options["filename"])
		if len(proteinData.keys()) < 3:
			print "Dataset contains only " + str(len(proteinData.keys())) + " proteins. Too small for analysis"
			sys.exit()
			
		aaOcc = aaOccurance()
		aaOcc.createAAOccuranceFilefromProteinList(options,proteinData)
		fileObject.createTEIRESIASinputfromProteinList(proteinData,options)

	
	if float(options["TEIRESIAS_supportString"]) < 1:
		if int(len(proteinData)*float(options["TEIRESIAS_supportString"])) > 3:
			options["TEIRESIAS_supportString"] = int(len(proteinData)*float(options["TEIRESIAS_supportString"]))
		else:
			options["TEIRESIAS_supportString"] = 3

	
	if options["run_TEIRESIAS"] == "T":
		teiresiasRunner = Teiresias()
		teiresiasRunner.teiresias(options)
		del teiresiasRunner

	filereader = Filereader()	
	teiresiasData = filereader.readTeiresiasData(offsets,options)

	del filereader

	run = RunBLAST()
	blastOutput = run.runBLAST(proteinData,options)
	#print blastOutput
	del run

	primAlgorithm = Prim()
	normalised_occurrance = primAlgorithm.scorePrimBlast(blastOutput[1],proteinData.keys())

	stats['overall_MST'] = normalised_occurrance
	stats['NumMotifs'] = len(teiresiasData)
	
	inputStatsString =  "---------------------------Input stats----------------------------\n"
	inputStatsString += 'Input dataset:   ' + options['filename'] + '\n'
	inputStatsString += 'No. of proteins: ' + str(len(proteinData)) + '\n'
	inputStatsString += 'Overall MST:     ' + "%-2.5f"%stats['overall_MST'] + '\n\n'
	
	for protein in proteinData:
		inputStatsString += '-%-20s'%protein + '\t' + '%5d'%(len(proteinData[protein])) + ' a.a' + '\n'

	inputStatsString += '%-20s'%' ' + '\t' + '-----' + '\n'
	inputStatsString += '%-20s'%'No. of residues: ' + '\t' + '%5d'%(len(''.join(proteinData.values()))) + '\n'
	
	stats['ProteinData'] = inputStatsString
	
	printLevel(1,options, inputStatsString)
	sys.stderr.write(inputStatsString)
	
	stats['ProteinData'] = inputStatsString
	
	scorer = Scorer()
	
	topRanking = scorer.scorePatterns(teiresiasData,topRanking,proteinData,options,blastOutput)
	del scorer
	
	fileWriter = FileWriter()
	fileWriter.htmlOutput(topRanking,teiresiasData,options)
		
	fileWriter.printOutput(topRanking,stats,options)
	del fileWriter

	if options['memory_saver'] == 'F':
		try:
			distPlot = DistributionPlotter(options)
			distPlot.plotDistribution(options,10)
			del distPlot
		except:
			pass
			#printLevel(1,options, 'Skipping distribution plotting')
	else:
		pass
		#printLevel(1,options, 'Skipping distribution plotting')

	sys.stderr.write("----------------------------------------------------------------\n")
	printLevel(1,options, "-----------------------------" + time.strftime('%X') + "-----------------------------")


def setupFiles(options):
	
		#print "-----------------------------" + time.strftime('%X') + "-----------------------------"
		
		fileChecker = FileChecker()
		fileChecker.checkFiles(options)
		
		options["filename"] = os.path.abspath(options["filename"])
	
		if os.path.basename(options["filename"]).split(".")[0] in os.listdir(os.path.dirname(options["filename"])):
			pass
		else:
			printLevel(1,options,os.path.abspath(os.path.dirname(options["filename"]) + "/" + os.path.split(options["filename"])[-1].split(".")[0]))
			os.mkdir(os.path.abspath(os.path.dirname(options["filename"]) + "/" + os.path.split(options["filename"])[-1].split(".")[0]))
		
		
		path = os.path.dirname(options["filename"]) + "/" + os.path.basename(options["filename"]).split(".")[0]
		
		options["input_path"] = os.path.abspath(path)

		path = options["input_path"]

		if "logs" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/logs/")

		if "alignments" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/alignments/")

		if "domains" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/domains/")

		if "surface" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/surface/")

		if "query" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/query/")

		if "results" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/results/")

		if "database" in os.listdir(path):
			pass
		else:
			os.mkdir(path + "/database/")

		options["output"] = path + "/" + os.path.basename(options["filename"]).split(".")[0] + ".rank"
		options["logfile"]= path + "/logs/" + time.strftime('%d%b%Y_%X_').replace(":","-") + os.path.basename(options["filename"]).split(".")[0] +  '.log'
		options["TEIRESIAS_output"] = path + "/" + os.path.basename(options["filename"]).split(".")[0] + ".fasta.out"
		options["TEIRESIAS_input"] = path + "/" + os.path.basename(options["filename"]).split(".")[0] + ".fasta"
		options["aminoAcidOccuranceFile"] = path + "/" + "PeptideOccurances.txt"
		options["BLAST_results"] = path + "/results/"
		options["filename"] = os.path.abspath(options["filename"])

		if options['Mask_path'] == 'mask.dat':
			options['Mask_path'] = os.path.join(os.path.dirname(sys.argv[0]),'mask.dat')
	
def removeFiles(options):
	if options['memory_saver'] == 'T':
		for file in os.listdir(options["input_path"]):
			if os.path.isfile(os.path.join(options["input_path"],file)):
				if file.split('.')[-1] not in ['fasta','rank','out']:
					os.remove(os.path.join(options["input_path"],file))
				else:
					pass
					
			elif os.path.isdir(os.path.join(options["input_path"],file)):
				try:
					shutil.rmtree(os.path.join(options["input_path"],file))
				except:
					pass
			else:
				pass
#----------------------------------------------------RUN---------------------------------------------------------------------#
#
#----------------------------------------------------------------------------------------------------------------------------#
if __name__ == "__main__":
	'''
	Main
	'''
	try:
		__version__ = "2.0"
		
		start = time.time()
		commandline = CommandLine()
		options = commandline.parseCommandLine()	
	
		if options["batch"] == 'F':
			setupFiles(options)
			
			logfile = open(options["logfile"],"w")
			sys.stderr = logfile	
			
			commandline.printOptions()
			del commandline
			
			runMain(options)
			
			logfile.close()
			removeFiles(options)
		elif  options["batch"] == 'T':
			
			try:
				os.listdir(options["filename"])
			except:
				print 'Input directory path does not exist'
				sys.exit()
				
			fileList = os.listdir(options["filename"])
			file_path = copy.deepcopy(options["filename"])
			for file in fileList:
				print '\n\n'
				print '+'*15 +'Running ' + file + '+'*15
				
				if file.split('.')[-1] != options['filetype']:
					pass
				else:
					options["filename"] = file_path + '/' + file
					setupFiles(options)
					
					logfile = open(options["logfile"],"w")
					sys.stderr = logfile
					
					try:
						runMain(options)
					except Exception,e:
						print 'Error running ' + file
						print e
					logfile.close()
					removeFiles(options)
	
			del commandline

	
		printLevel(1,options,"Total Time Elapsed\n%2.4f"%(time.time() - start))
		printLevel(1,options,"------------------------------------------------------------------\n")

		sys.stderr.write("End Time            : " + str(time.strftime('%X')) + "\n")
		sys.stderr.write("Total Time Elapsed  : %2.4f"%(time.time() - start)  + "\n")
		sys.stderr.write("----------------------------------------------------------------\n")
	

	except SystemExit:
		pass

	except Exception,e:
		error_type = sys.exc_info()[0]
		error_value = sys.exc_info()[1]
		error_traceback = traceback.extract_tb(sys.exc_info()[2])
	
		sys.stderr.write("\n")
		sys.stderr.write('Error in routine\n')
		sys.stderr.write('Error Type       : ' + str(error_type) + '\n')
		sys.stderr.write('Error Value      : ' + str(error_value) + '\n')
		sys.stderr.write('File             : ' + str(error_traceback[-1][0]) + '\n')	
		sys.stderr.write('Method           : ' + str(error_traceback[-1][2]) + '\n')	
		sys.stderr.write('Line             : ' + str(error_traceback[-1][1]) + '\n')		
		sys.stderr.write('Error            : ' + str(error_traceback[-1][3]) + '\n')	
		
		try:
			logfile.close()
			printLevel(1,options, open(options["logfile"],"r").read(-1))
		except:
			pass
	except:
		raise
#----------------------------------------------------------------------------------------------------------------------------#

### ~~~ Module slimfinder_V4.9 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/legacy/slimfinder_V4.9.py] ~~~ ###

Program:      SLiMFinder
Description:  Short Linear Motif Finder
Version:      4.9
Last Edit:    11/07/14
Citation:     Edwards, Davey & Shields (2007), PLoS ONE 2(10): e967. [PMID: 17912346]
ConsMask Citation: Davey NE, Shields DC & Edwards RJ (2009), Bioinformatics 25(4): 443-50. [PMID: 19136552]
SigV/SigPrime Citation: Davey NE, Edwards RJ & Shields DC (2010), BMC Bioinformatics 11: 14. [PMID: 20055997]
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    Short linear motifs (SLiMs) in proteins are functional microdomains of fundamental importance in many biological
    systems. SLiMs typically consist of a 3 to 10 amino acid stretch of the primary protein sequence, of which as few
    as two sites may be important for activity, making identification of novel SLiMs extremely difficult. In particular,
    it can be very difficult to distinguish a randomly recurring "motif" from a truly over-represented one. Incorporating
    ambiguous amino acid positions and/or variable-length wildcard spacers between defined residues further complicates
    the matter.

    SLiMFinder is an integrated SLiM discovery program building on the principles of the SLiMDisc software for accounting
    for evolutionary relationships [Davey NE, Shields DC & Edwards RJ (2006): Nucleic Acids Res. 34(12):3546-54].
    SLiMFinder is comprised of two algorithms:

    SLiMBuild identifies convergently evolved, short motifs in a dataset. Motifs with fixed amino acid positions are
    identified and then combined to incorporate amino acid ambiguity and variable-length wildcard spacers. Unlike
    programs such as TEIRESIAS, which return all shared patterns, SLiMBuild accelerates the process and reduces returned
    motifs by explicitly screening out motifs that do not occur in enough unrelated proteins. For this, SLiMBuild uses
    the "Unrelated Proteins" (UP) algorithm of SLiMDisc in which BLAST is used to identify pairwise relationships.
    Proteins are then clustered according to these relationships into "Unrelated Protein Clusters" (UPCs), which are
    defined such that no protein in a UPC has a BLAST-detectable relationship with a protein in another UPC.  If desired,
    SLiMBuild can be used as a replacement for TEIRESIAS in other software (teiresias=T slimchance=F).

    SLiMChance estimates the probability of these motifs arising by chance, correcting for the size and composition of
    the dataset, and assigns a significance value to each motif. Motif occurrence probabilites are calculated
    independently for each UPC, adjusted the size of a UPC using the Minimum Spanning Tree algorithm from SLiMDisc. These
    individual occurrence probabilities are then converted into the total probability of the seeing the observed motifs
    the observed number of (unrelated) times. These probabilities assume that the motif is known before the search. In
    reality, only over-represented motifs from the dataset are looked at, so these probabilities are adjusted for the
    size of motif-space searched to give a significance value. This is an estimate of the probability of seeing that
    motif, or another one like it. These values are calculated separately for each length of motif. Where pre-known
    motifs are also of interest, these can be given with the slimcheck=MOTIFS option and will be added to the output.
    SLiMFinder version 4.0 introduced a more precise (but more computationally intensive) statistical model, which can
    be switched on using sigprime=T. Likewise, the more precise (but more computationally intensive) correction to the
    mean UPC probability heuristic can be switched on using sigv=T. (Note that the other SLiMChance options may not
    work with either of these options.) The allsig=T option will output all four scores. In this case, SigPrimeV will be
    used for ranking etc. unless probscore=X is used.

    Where significant motifs are returned, SLiMFinder will group them into Motif "Clouds", which consist of physically
    overlapping motifs (2+ non-wildcard positions are the same in the same sequence). This provides an easy indication
    of which motifs may actually be variants of a larger SLiM and should therefore be considered together.

    Additional Motif Occurrence Statistics, such as motif conservation, are handled by the rje_slimlist module. Please
    see the documentation for this module for a full list of commandline options. These options are currently under
    development for SLiMFinder and are not fully supported. See the SLiMFinder Manual for further details. Note that the
    OccFilter *does* affect the motifs returned by SLiMBuild and thus the TEIRESIAS output (as does min. IC and min.
    Support) but the overall Motif StatFilter *only* affects SLiMFinder output following SLiMChance calculations.

Secondary Functions:
    The "MotifSeq" option will output fasta files for a list of X:Y, where X is a motif pattern and Y is the output file.

    The "Randomise" function will take a set of input datasets (as in Batch Mode) and regenerate a set of new datasets
    by shuffling the UPC among datasets. Note that, at this stage, this is quite crude and may result in the final
    datasets having fewer UPC due to common sequences and/or relationships between UPC clusters in different datasets.

Basic Input/Output Options: 
    seqin=FILE      : Sequence file to search [None]
    batch=LIST      : List of files to search, wildcards allowed. (Over-ruled by seqin=FILE.) [*.dat,*.fas]
    maxseq=X        : Maximum number of sequences to process [500]
    maxupc=X        : Maximum UPC size of dataset to process [0]
    sizesort=X      : Sorts batch files by size prior to running (+1 small->big; -1 big->small; 0 none) [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main SLiMFinder results table [slimfinder.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [SLiMFinder/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMFinder/]
    force=T/F       : Force re-running of BLAST, UPC generation and SLiMBuild [False]
    pickup=T/F      : Pick-up from aborted batch run by identifying datasets in resfile [False]
    pickid=T/F      : Whether to use RunID to identify run datasets when using pickup [True]
    pickall=T/F     : Whether to skip aborted runs (True) or only those datasets that ran to completion (False) [True]
    dna=T/F         : Whether the sequences files are DNA rather than protein [False]

SLiMBuild Options:
    ### SLiMBuild Options I: Evolutionary Filtering  ###
    efilter=T/F     : Whether to use evolutionary filter [True]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    homcut=X        : Max number of homologues to allow (to reduce large multi-domain families) [0]
    newupc=PATH     : Look for alternative UPC file and calculate Significance using new clusters [None]

    ### SLiMBuild Options II: Input Masking ###
    masking=T/F     : Master control switch to turn off all masking if False [True]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out [EM]
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M (can be useful if termini=T) [True]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []
    qregion=X,Y     : Mask all but the region of the query from (and including) residue X to residue Y [0,-1]

    ### SLiMBuild Options III: Basic Motif Construction ###
    termini=T/F     : Whether to add termini characters (^ & $) to search sequences [True]
    minwild=X       : Minimum number of consecutive wildcard positions to allow [0]
    maxwild=X       : Maximum number of consecutive wildcard positions to allow [2]
    slimlen=X       : Maximum length of SLiMs to return (no. non-wildcard positions) [5]
    minocc=X        : Minimum number of unrelated occurrences for returned SLiMs. (Proportion of UP if < 1) [0.05]
    absmin=X        : Used if minocc<1 to define absolute min. UP occ [3]
    alphahelix=T/F  : Special i, i+3/4, i+7 motif discovery [False]
    fixlen=T/F      : If true, will use maxwild and slimlen to define a fixed total motif length [False]
    palindrome=T/F  : Special DNA mode that will search for palindromic sequences only [False]

    ### SLiMBuild Options IV: Ambiguity ###
    ambiguity=T/F   : (preamb=T/F) Whether to search for ambiguous motifs during motif discovery [True]
    ambocc=X        : Min. UP occurrence for subvariants of ambiguous motifs (minocc if 0 or > minocc) [0.05]
    absminamb=X     : Used if ambocc<1 to define absolute min. UP occ [2]
    equiv=LIST      : List (or file) of TEIRESIAS-style ambiguities to use [AGS,ILMVF,FYW,FYH,KRH,DE,ST]
    wildvar=T/F     : Whether to allow variable length wildcards [True]
    combamb=T/F     : Whether to search for combined amino acid degeneracy and variable wildcards [False]

    ### SLiMBuild Options V: Advanced Motif Filtering ###
    altupc=PATH     : Look for alternative UPC file and filter based on minocc [None]
    musthave=LIST   : Returned motifs must contain one or more of the AAs in LIST (reduces search space) []
    query=LIST      : Return only SLiMs that occur in 1+ Query sequences (Name/AccNum) []
    focus=FILE      : FILE containing focal groups for SLiM return (see Manual for details) [None]
    focusocc=X      : Motif must appear in X+ focus groups (0 = all) [0]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    
SLiMChance Options:
    cloudfix=T/F    : Restrict output to clouds with 1+ fixed motif (recommended) [False]
    slimchance=T/F  : Execute main SLiMFinder probability method and outputs [True]
    sigprime=T/F    : Calculate more precise (but more computationally intensive) statistical model [False]
    sigv=T/F        : Use the more precise (but more computationally intensive) fix to mean UPC probability [False]
    dimfreq=T/F     : Whether to use dimer masking pattern to adjust number of possible sites for motif [True]
    probcut=X       : Probability cut-off for returned motifs (sigcut=X also recognised) [0.1]
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [True]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) (!!!Experimental!!!) [None]
    negatives=FILE  : Multiply raw probabilities by under-representation in FILE (!!!Experimental!!!) [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=T/F      : Whether to upweight for multiple occurrences in same sequence (heuristic) [False]
    probscore=X     : Score to be used for probability cut-off and ranking (Prob/Sig/S/R) [Sig]

Advanced Output Options:
    ### Advanced Output Options I: Output data ###
    clouds=X        : Identifies motif "clouds" which overlap at 2+ positions in X+ sequences (0=minocc / -1=off) [2]
    runid=X         : Run ID for resfile (allows multiple runs on same data) [DATE]
    logmask=T/F     : Whether to log the masking of individual sequences [True]
    slimcheck=FILE  : Motif file/list to add to resfile output [] 

    ### Advanced Output Options II: Output formats ###
    teiresias=T/F   : Replace TEIRESIAS, making *.out and *.mask.fasta files [False]
    slimdisc=T/F    : Emulate SLiMDisc output format (*.rank & *.dat.rank + TEIRESIAS *.out & *.fasta) [False]
    extras=X        : Whether to generate additional output files (alignments etc.) [1]
                        --1 = No output beyond main results file
                        - 0 = Generate occurrence file
                        - 1 = Generate occurrence file, alignments and cloud file
                        - 2 = Generate all additional SLiMFinder outputs
                        - 3 = Generate SLiMDisc emulation too (equiv extras=2 slimdisc=T)
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle
                        - 2 = Delete all bar *.upc (pickle added to tar)
                        - 3 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)

    ### Advanced Output Options III: Additional Motif Filtering ###
    topranks=X      : Will only output top X motifs meeting probcut [1000]
    oldscores=T/F   : Whether to also output old SLiMDisc score (S) and SLiMPickings score (R) [False]
    allsig=T/F      : Whether to also output all SLiMChance combinations (Sig/SigV/SigPrime/SigPrimeV) [False]
    minic=X         : Minimum information content for returned motifs [2.1]
    * See also rje_slimcalc options for occurrence-based calculations and filtering *

Additional Functions:
    ### Additional Functions I: MotifSeq ###
    motifseq=LIST   : Outputs fasta files for a list of X:Y, where X is the pattern and Y is the output file []
    slimbuild=T/F   : Whether to build motifs with SLiMBuild. (For combination with motifseq only.) [True]

    ### Additional Functions II: Randomised datasets ###
    randomise=T/F   : Randomise UPC within batch files and output new datasets [False]
    randir=PATH     : Output path for creation of randomised datasets [Random/]
    randbase=X      : Base for random dataset name [rand]

Uses general modules: copy, glob, math, os, string, sys, time
Uses RJE modules: rje, rje_blast, rje_slim, rje_slimlist, rje_slimcalc, rje_slimcore, rje_dismatrix_V2, rje_seq, rje_scoring
Other modules needed: None

### ~~~ Module slimprob_V1.4 ~ [/Users/redwards/ownCloud/projects/SLiMSuite-Jun14/code/slimsuite/legacy/slimprob_V1.4.py] ~~~ ###

Program:      SLiMProb
Description:  Short Linear Motif Probability tool
Version:      1.4
Last Edit:    11/07/14
Citation:     Davey, Haslam, Shields & Edwards (2010), Lecture Notes in Bioinformatics 6282: 50-61. 
Copyright (C) 2007  Richard J. Edwards - See source code for GNU License Notice

Function:
    SLiMProb is a tool for finding pre-defined SLiMs (Short Linear Motifs) in a protein sequence database. SLiMProb
    can make use of corrections for evolutionary relationships and a variation of the SLiMChance alogrithm from
    SLiMFinder to assess motifs for statistical over- and under-representation. SLiMProb is replace for the original
    SLiMSearch, which itself was a replacement for PRESTO. The basic architecture is the same but it was felt that having
    two different "SLiMSearch" servers was confusing. 

    Benefits of SLiMProb that make it more useful than a lot of existing tools include:
    * searching with mismatches rather than restricting hits to perfect matches.
    * optional equivalency files for searching with specific allowed mismatched (e.g. charge conservation)
    * generation or reading of alignment files from which to calculate conservation statistics for motif occurrences.
    * additional statistics, including protein disorder, surface accessibility and hydrophobicity predictions
    * recognition of "n of m" motif elements in the form <X:n:m>, where X is one or more amino acids that must occur n+
    times across which m positions. E.g. <IL:3:5> must have 3+ Is and/or Ls in a 5aa stretch.

    Main output for SLiMProb is a delimited file of motif/peptide occurrences but the motifaln=T and proteinaln=T also
    allow output of alignments of motifs and their occurrences. The primary outputs are named *.occ.csv for the occurrence
    data and *.csv for the summary data for each motif/dataset pair. (This is a change since SLiMSearch.)

Commandline: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Basic Input/Output Options ###
    motifs=FILE     : File of input motifs/peptides [None]
                      Single line per motif format = 'Name Sequence #Comments' (Comments are optional and ignored)
                      Alternative formats include fasta, SLiMDisc output and raw motif lists.
    seqin=FILE      : Sequence file to search [None]
    batch=LIST      : List of sequence files for batch input (wildcard * permitted) []
    maxseq=X        : Maximum number of sequences to process [0]
    maxsize=X       : Maximum dataset size to process in AA (or NT) [100,000]
    maxocc=X        : Filter out Motifs with more than maximum number of occurrences [0]
    walltime=X      : Time in hours before program will abort search and exit [1.0]
    resfile=FILE    : Main SLiMProb results table (*.csv and *.occ.csv) [slimprob.csv]
    resdir=PATH     : Redirect individual output files to specified directory (and look for intermediates) [SLiMProb/]
    buildpath=PATH  : Alternative path to look for existing intermediate files [SLiMProb/]
    force=T/F       : Force re-running of BLAST, UPC generation and search [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SearchDB Options I: Input Protein Sequence Masking ###
    masking=T/F     : Master control switch to turn off all masking if False [False]
    dismask=T/F     : Whether to mask ordered regions (see rje_disorder for options) [False]
    consmask=T/F    : Whether to use relative conservation masking [False]
    ftmask=LIST     : UniProt features to mask out [EM,DOMAIN,TRANSMEM]
    imask=LIST      : UniProt features to inversely ("inclusively") mask. (Seqs MUST have 1+ features) []
    compmask=X,Y    : Mask low complexity regions (same AA in X+ of Y consecutive aas) [5,8]
    casemask=X      : Mask Upper or Lower case [None]
    motifmask=X     : List (or file) of motifs to mask from input sequences []
    metmask=T/F     : Masks the N-terminal M [False]
    posmask=LIST    : Masks list of position-specific aas, where list = pos1:aas,pos2:aas  [2:A]
    aamask=LIST     : Masks list of AAs from all sequences (reduces alphabet) []

    ### SearchDB Options II: Evolutionary Filtering  ###
    efilter=T/F     : Whether to use evolutionary filter [False]
    blastf=T/F      : Use BLAST Complexity filter when determining relationships [True]
    blaste=X        : BLAST e-value threshold for determining relationships [1e=4]
    altdis=FILE     : Alternative all by all distance matrix for relationships [None]
    gablamdis=FILE  : Alternative GABLAM results file [None] (!!!Experimental feature!!!)
    occupc=T/F      : Whether to output the UPC ID number in the occurrence output file [False]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### SLiMChance Options ###
    maskfreq=T/F    : Whether to use masked AA Frequencies (True), or (False) mask after frequency calculations [True]
    aafreq=FILE     : Use FILE to replace individual sequence AAFreqs (FILE can be sequences or aafreq) [None]
    aadimerfreq=FILE: Use empirical dimer frequencies from FILE (fasta or *.aadimer.tdt) [None]
    negatives=FILE  : Multiply raw probabilities by under-representation in FILE [None]
    background=FILE : Use observed support in background file for over-representation calculations [None]
    smearfreq=T/F   : Whether to "smear" AA frequencies across UPC rather than keep separate AAFreqs [False]
    seqocc=X        : Restrict to sequences with X+ occurrences (adjust for high frequency SLiMs) [1]
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    ### Output Options ###
    extras=X        : Whether to generate additional output files (alignments etc.) [2]
                        - 0 = No output beyond main results file
                        - 1 = Saved masked input sequences [*.masked.fas]
                        - 2 = Generate additional outputs (alignments etc.)
    pickle=T/F      : Whether to save/use pickles [True]
    targz=T/F       : Whether to tar and zip dataset result files (UNIX only) [False]
    savespace=0     : Delete "unneccessary" files following run (best used with targz): [0]
                        - 0 = Delete no files
                        - 1 = Delete all bar *.upc and *.pickle files
                        - 2 = Delete all dataset-specific files including *.upc and *.pickle (not *.tar.gz)
    * See also rje_slimcalc options for occurrence-based calculations and filtering *
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

Uses general modules: copy, glob, os, string, sys, time
Uses RJE modules: rje
Other modules needed: None




